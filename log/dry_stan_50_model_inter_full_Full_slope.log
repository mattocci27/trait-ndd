── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  dry season"
[1] "Use full"
[1] "Habitat = slope"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: 1 + all the traits"
[1] "sp number in seedling data: 69"
[1] "sp number in trait data: 69"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.4 as a scaling parameter for the distance effect"
[1] "n_sp = J =69"
[1] "n_para = K = 11"
[1] "n_plot = S = 93"
[1] "n_census = T = 10"
[1] "n_tag = M = 1993"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.006156 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 61.56 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.007028 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 70.28 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.008631 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 86.31 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 3: 
Chain 3: Gradient evaluation took 0.007067 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 70.67 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Exception: validate transformed params: y is not positive definite.  (in 'model96a32d5c30e1_model_inter' at line 59)

Chain 2: Exception: validate transformed params: y is not positive definite.  (in 'model96a32d5c30e1_model_inter' at line 59)

Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 67523.9 seconds (Warm-up)
Chain 4:                11633.2 seconds (Sampling)
Chain 4:                79157.1 seconds (Total)
Chain 4: 
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 21468.7 seconds (Warm-up)
Chain 1:                58472.8 seconds (Sampling)
Chain 1:                79941.5 seconds (Total)
Chain 1: 
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 29112.2 seconds (Warm-up)
Chain 3:                55949.5 seconds (Sampling)
Chain 3:                85061.7 seconds (Total)
Chain 3: 
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 23670.7 seconds (Warm-up)
Chain 2:                90189.6 seconds (Sampling)
Chain 2:                113860 seconds (Total)
Chain 2: 
Warning message:
In validityMethod(object) :
  The following variables have undefined values:  Omega[1,1],The following variables have undefined values:  Omega[2,1],The following variables have undefined values:  Omega[3,1],The following variables have undefined values:  Omega[4,1],The following variables have undefined values:  Omega[5,1],The following variables have undefined values:  Omega[6,1],The following variables have undefined values:  Omega[7,1],The following variables have undefined values:  Omega[8,1],The following variables have undefined values:  Omega[9,1],The following variables have undefined values:  Omega[10,1],The following variables have undefined values:  Omega[11,1],The following variables have undefined values:  Omega[1,2],The following variables have undefined values:  Omega[2,2],The following variables have undefined values:  Omega[3,2],The following variables have undefined values:  Omega[4,2],The following variables have undefined values:  Omega[5,2],The following variables have undefined values:  Omega[ [... truncated]
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean    sd     2.5%      25%      50%      75%
gamma[1,1]       2.71    0.08  5.07    -7.07    -0.68     2.70     6.04
gamma[1,2]       0.54    0.06  4.56    -8.36    -2.49     0.52     3.56
gamma[1,3]      -7.00    0.08  4.54   -15.67   -10.10    -7.04    -4.00
gamma[1,4]       7.33    0.06  3.13     1.41     5.24     7.25     9.36
gamma[1,5]       1.62    0.08  3.83    -5.87    -1.00     1.58     4.25
gamma[1,6]      -0.31    0.09  4.99   -10.05    -3.70    -0.30     3.08
gamma[1,7]      -3.69    0.06  3.98   -11.57    -6.36    -3.67    -1.04
gamma[1,8]       0.57    0.06  2.59    -4.65    -1.13     0.58     2.35
gamma[1,9]       2.66    0.04  1.66    -0.56     1.53     2.64     3.76
gamma[1,10]     -1.88    0.05  1.69    -5.48    -2.93    -1.79    -0.75
gamma[1,11]     12.67    0.05  2.95     7.02    10.71    12.59    14.62
gamma[2,1]      -3.73    0.08  4.64   -12.90    -6.84    -3.73    -0.57
gamma[2,2]       3.68    0.06  4.62    -5.70     0.59     3.74     6.83
gamma[2,3]      -0.07    0.08  4.25    -8.26    -2.92    -0.08     2.83
gamma[2,4]      -2.72    0.08  3.73    -9.80    -5.29    -2.79    -0.29
gamma[2,5]      -1.29    0.09  4.11    -9.59    -3.97    -1.22     1.46
gamma[2,6]      -2.49    0.07  3.20    -8.84    -4.60    -2.44    -0.36
gamma[2,7]      -0.16    0.07  4.12    -8.30    -2.94    -0.14     2.61
gamma[2,8]       0.54    0.07  2.96    -5.37    -1.42     0.65     2.56
gamma[2,9]      -2.29    0.06  2.32    -6.97    -3.83    -2.27    -0.66
gamma[2,10]      1.50    0.06  2.14    -2.65     0.04     1.46     2.92
gamma[2,11]      2.34    0.09  3.51    -4.49     0.00     2.35     4.66
gamma[3,1]      -1.04    0.06  4.64   -10.10    -4.14    -1.01     2.07
gamma[3,2]       1.10    0.06  4.46    -7.55    -1.94     1.07     4.09
gamma[3,3]       2.12    0.08  4.40    -6.53    -0.87     2.15     5.11
gamma[3,4]       1.16    0.08  3.92    -6.39    -1.52     1.16     3.84
gamma[3,5]      -0.34    0.08  4.19    -8.67    -3.10    -0.32     2.46
gamma[3,6]      -0.27    0.06  3.59    -7.22    -2.74    -0.30     2.22
gamma[3,7]       0.18    0.06  4.20    -8.22    -2.66     0.20     3.03
gamma[3,8]      -1.29    0.06  3.27    -7.65    -3.52    -1.32     0.93
gamma[3,9]       1.41    0.08  3.13    -4.74    -0.67     1.39     3.49
gamma[3,10]     -0.70    0.08  3.22    -6.86    -2.88    -0.73     1.42
gamma[3,11]     -2.34    0.10  3.98    -9.95    -5.08    -2.35     0.32
gamma[4,1]      -0.92    0.07  4.65    -9.98    -4.03    -0.89     2.18
gamma[4,2]       1.43    0.06  4.61    -7.50    -1.70     1.45     4.51
gamma[4,3]      -0.74    0.07  4.43    -9.36    -3.77    -0.80     2.27
gamma[4,4]       2.42    0.08  3.99    -5.33    -0.28     2.41     5.14
gamma[4,5]      -2.99    0.09  4.25   -11.38    -5.80    -2.95    -0.16
gamma[4,6]      -2.61    0.06  3.51    -9.40    -4.96    -2.64    -0.29
gamma[4,7]       0.65    0.06  4.33    -7.79    -2.27     0.68     3.57
gamma[4,8]      -1.46    0.07  3.34    -7.80    -3.75    -1.50     0.71
gamma[4,9]       1.28    0.07  2.84    -4.45    -0.61     1.29     3.19
gamma[4,10]     -0.80    0.08  2.96    -6.69    -2.78    -0.76     1.19
gamma[4,11]      3.17    0.08  3.87    -4.24     0.50     3.10     5.82
gamma[5,1]      -0.59    0.08  4.52    -9.45    -3.62    -0.58     2.37
gamma[5,2]       0.40    0.06  4.49    -8.42    -2.56     0.44     3.44
gamma[5,3]      -0.21    0.08  4.29    -8.69    -3.14    -0.20     2.69
gamma[5,4]       1.03    0.08  3.40    -5.64    -1.30     1.05     3.35
gamma[5,5]      -3.00    0.08  3.90   -10.76    -5.64    -2.95    -0.30
gamma[5,6]      -0.49    0.07  3.07    -6.58    -2.53    -0.47     1.59
gamma[5,7]       1.02    0.06  3.94    -6.73    -1.63     1.07     3.69
gamma[5,8]      -2.33    0.07  2.81    -7.95    -4.17    -2.32    -0.44
gamma[5,9]       0.37    0.07  2.54    -4.64    -1.35     0.40     2.08
gamma[5,10]      1.99    0.05  2.25    -2.30     0.44     1.94     3.48
gamma[5,11]      0.36    0.07  3.38    -6.29    -1.92     0.35     2.70
gamma[6,1]       0.45    0.08  4.49    -8.37    -2.55     0.48     3.45
gamma[6,2]      -4.50    0.06  4.38   -13.09    -7.41    -4.51    -1.49
gamma[6,3]       2.77    0.09  4.38    -5.71    -0.20     2.81     5.77
gamma[6,4]       0.42    0.08  3.15    -5.93    -1.60     0.52     2.52
gamma[6,5]      -4.50    0.07  4.04   -12.39    -7.23    -4.53    -1.76
gamma[6,6]      -0.23    0.06  2.70    -5.54    -2.05    -0.17     1.60
gamma[6,7]      -0.29    0.06  3.76    -7.63    -2.82    -0.29     2.17
gamma[6,8]      -0.74    0.07  2.42    -5.53    -2.34    -0.74     0.86
gamma[6,9]      -0.48    0.04  1.64    -3.68    -1.56    -0.47     0.61
gamma[6,10]     -0.52    0.05  1.85    -4.15    -1.74    -0.52     0.70
gamma[6,11]     -4.42    0.05  2.96   -10.33    -6.44    -4.34    -2.42
gamma[7,1]      -2.64    0.07  4.43   -11.28    -5.64    -2.67     0.34
gamma[7,2]       0.20    0.06  4.67    -8.94    -3.05     0.21     3.43
gamma[7,3]      -1.19    0.08  4.35    -9.73    -4.11    -1.25     1.73
gamma[7,4]      -0.79    0.08  3.53    -7.61    -3.22    -0.83     1.61
gamma[7,5]       1.97    0.08  4.07    -6.04    -0.79     1.97     4.77
gamma[7,6]       4.23    0.07  3.39    -2.54     1.99     4.24     6.50
gamma[7,7]      -1.27    0.06  4.29    -9.59    -4.27    -1.31     1.65
gamma[7,8]       1.03    0.07  3.00    -4.73    -0.99     1.02     3.04
gamma[7,9]      -0.02    0.05  2.18    -4.34    -1.44    -0.01     1.41
gamma[7,10]      1.32    0.05  2.26    -3.01    -0.21     1.32     2.77
gamma[7,11]     -7.82    0.05  3.31   -14.35    -9.98    -7.83    -5.61
gamma[8,1]       0.56    0.06  4.58    -8.58    -2.42     0.58     3.60
gamma[8,2]      -0.10    0.05  4.65    -9.24    -3.23    -0.12     3.03
gamma[8,3]       1.07    0.07  4.47    -7.67    -1.95     1.09     4.11
gamma[8,4]       1.74    0.08  4.12    -6.43    -1.03     1.74     4.46
gamma[8,5]      -0.87    0.09  4.29    -9.12    -3.81    -0.84     2.02
gamma[8,6]       2.60    0.07  3.92    -5.09    -0.05     2.55     5.20
gamma[8,7]      -1.27    0.06  4.47   -10.07    -4.26    -1.31     1.75
gamma[8,8]      -0.16    0.07  3.57    -7.15    -2.58    -0.08     2.24
gamma[8,9]       0.59    0.07  3.24    -5.95    -1.56     0.63     2.73
gamma[8,10]     -1.76    0.07  3.16    -8.03    -3.89    -1.77     0.39
gamma[8,11]     -2.23    0.08  3.97   -10.20    -4.88    -2.22     0.39
gamma[9,1]      -2.19    0.06  4.56   -11.09    -5.22    -2.17     0.89
gamma[9,2]      -0.28    0.05  4.20    -8.36    -3.17    -0.24     2.52
gamma[9,3]       0.87    0.08  4.32    -7.79    -2.09     0.93     3.83
gamma[9,4]      -3.67    0.08  3.89   -11.18    -6.28    -3.69    -1.05
gamma[9,5]       0.62    0.09  4.11    -7.49    -2.14     0.62     3.36
gamma[9,6]       1.60    0.06  3.42    -5.07    -0.70     1.55     3.87
gamma[9,7]       1.21    0.05  4.00    -6.59    -1.47     1.21     3.91
gamma[9,8]       0.01    0.07  3.52    -6.80    -2.36    -0.05     2.39
gamma[9,9]       0.58    0.08  3.21    -6.02    -1.56     0.67     2.82
gamma[9,10]      0.33    0.08  3.22    -6.00    -1.84     0.30     2.55
gamma[9,11]     -0.96    0.07  3.68    -8.11    -3.49    -0.91     1.51
gamma[10,1]     -2.06    0.08  4.63   -11.21    -5.08    -2.14     1.05
gamma[10,2]      0.78    0.06  4.89    -8.86    -2.47     0.81     4.06
gamma[10,3]      3.23    0.07  4.51    -5.68     0.13     3.22     6.24
gamma[10,4]     -3.96    0.07  3.75   -11.18    -6.45    -4.07    -1.47
gamma[10,5]      0.30    0.10  4.13    -7.94    -2.39     0.33     3.05
gamma[10,6]      2.39    0.06  3.22    -3.75     0.19     2.41     4.52
gamma[10,7]     -1.63    0.07  4.56   -10.66    -4.70    -1.68     1.45
gamma[10,8]      0.81    0.06  2.62    -4.42    -0.92     0.86     2.56
gamma[10,9]     -2.57    0.05  2.49    -7.60    -4.20    -2.53    -0.93
gamma[10,10]    -4.03    0.06  2.36    -8.94    -5.57    -3.97    -2.41
gamma[10,11]    -0.29    0.07  3.53    -7.23    -2.64    -0.35     2.06
gamma[11,1]      2.52    0.08  4.48    -6.13    -0.59     2.49     5.58
gamma[11,2]     -0.68    0.07  4.71    -9.93    -3.88    -0.69     2.52
gamma[11,3]     -3.08    0.08  4.26   -11.26    -5.97    -3.16    -0.21
gamma[11,4]     -2.21    0.08  3.40    -9.05    -4.45    -2.20     0.10
gamma[11,5]      1.57    0.09  4.04    -6.15    -1.07     1.48     4.16
gamma[11,6]     -1.78    0.07  2.97    -7.59    -3.81    -1.81     0.25
gamma[11,7]     -1.20    0.07  4.20    -9.41    -4.03    -1.24     1.60
gamma[11,8]     -1.96    0.06  2.48    -6.91    -3.62    -1.94    -0.27
gamma[11,9]      2.79    0.05  1.94    -0.98     1.50     2.79     4.04
gamma[11,10]    -0.18    0.05  1.93    -3.89    -1.51    -0.23     1.11
gamma[11,11]     2.14    0.09  3.16    -3.95     0.02     2.11     4.24
gamma[12,1]      1.10    0.07  4.58    -8.01    -1.99     1.07     4.25
gamma[12,2]      1.10    0.06  4.49    -7.59    -1.84     1.11     4.13
gamma[12,3]      0.35    0.08  4.36    -8.25    -2.62     0.32     3.35
gamma[12,4]      2.03    0.09  3.82    -5.83    -0.44     2.13     4.54
gamma[12,5]     -0.05    0.08  4.09    -8.27    -2.81     0.05     2.74
gamma[12,6]      0.03    0.06  3.09    -6.20    -2.04     0.05     2.10
gamma[12,7]      0.62    0.06  4.21    -7.56    -2.28     0.64     3.45
gamma[12,8]      0.95    0.06  3.01    -5.09    -1.06     0.96     2.93
gamma[12,9]      1.61    0.07  2.77    -3.94    -0.22     1.62     3.48
gamma[12,10]     0.21    0.07  2.32    -4.33    -1.37     0.20     1.76
gamma[12,11]    -3.31    0.08  3.81   -10.63    -5.89    -3.33    -0.74
gamma[13,1]      2.31    0.07  4.58    -6.73    -0.75     2.33     5.34
gamma[13,2]     -1.50    0.06  4.15    -9.68    -4.30    -1.44     1.24
gamma[13,3]      0.98    0.09  4.16    -7.14    -1.91     1.02     3.83
gamma[13,4]     -1.77    0.07  3.34    -8.46    -4.00    -1.78     0.52
gamma[13,5]      3.39    0.08  3.74    -4.09     0.88     3.41     5.92
gamma[13,6]      2.66    0.06  2.67    -2.65     0.89     2.70     4.47
gamma[13,7]      0.34    0.06  3.63    -6.58    -2.15     0.33     2.78
gamma[13,8]      3.26    0.06  2.64    -2.09     1.51     3.30     5.05
gamma[13,9]     -0.15    0.06  2.24    -4.52    -1.62    -0.17     1.33
gamma[13,10]    -0.16    0.05  1.98    -3.95    -1.48    -0.20     1.13
gamma[13,11]    -3.61    0.07  3.41   -10.19    -5.97    -3.65    -1.38
sig[1]          23.23    0.21  6.26    12.68    18.79    22.60    27.02
sig[2]          97.82    0.85 33.75    49.24    74.17    92.34   115.87
sig[3]          93.48    0.62 18.12    62.12    80.68    92.21   104.62
lp__         -1688.68    1.07 44.26 -1779.28 -1717.96 -1688.20 -1658.40
                97.5% n_eff Rhat
gamma[1,1]      12.74  4541 1.00
gamma[1,2]       9.63  5037 1.00
gamma[1,3]       2.19  2865 1.00
gamma[1,4]      13.68  3176 1.00
gamma[1,5]       9.05  2326 1.00
gamma[1,6]       9.52  3003 1.00
gamma[1,7]       4.09  4399 1.00
gamma[1,8]       5.49  1663 1.00
gamma[1,9]       6.00  1432 1.00
gamma[1,10]      1.28  1395 1.00
gamma[1,11]     18.65  4084 1.00
gamma[2,1]       5.36  3659 1.00
gamma[2,2]      12.55  6558 1.00
gamma[2,3]       8.19  2834 1.00
gamma[2,4]       4.88  2162 1.00
gamma[2,5]       6.72  1904 1.00
gamma[2,6]       3.74  2316 1.00
gamma[2,7]       7.83  3671 1.00
gamma[2,8]       6.15  2005 1.00
gamma[2,9]       2.14  1389 1.00
gamma[2,10]      5.77  1230 1.00
gamma[2,11]      9.27  1665 1.00
gamma[3,1]       8.05  6014 1.00
gamma[3,2]      10.07  5782 1.00
gamma[3,3]      10.62  3258 1.00
gamma[3,4]       8.92  2137 1.00
gamma[3,5]       7.90  2615 1.00
gamma[3,6]       6.72  3477 1.00
gamma[3,7]       8.27  5799 1.00
gamma[3,8]       5.06  2678 1.00
gamma[3,9]       7.77  1604 1.00
gamma[3,10]      5.77  1650 1.00
gamma[3,11]      5.48  1701 1.00
gamma[4,1]       8.13  5047 1.00
gamma[4,2]      10.64  6561 1.00
gamma[4,3]       8.06  3544 1.00
gamma[4,4]      10.18  2828 1.00
gamma[4,5]       5.42  2355 1.00
gamma[4,6]       4.42  3281 1.00
gamma[4,7]       9.12  5373 1.00
gamma[4,8]       5.22  2527 1.00
gamma[4,9]       6.75  1513 1.00
gamma[4,10]      4.90  1396 1.00
gamma[4,11]     10.76  2482 1.00
gamma[5,1]       8.41  3385 1.00
gamma[5,2]       9.01  5177 1.00
gamma[5,3]       8.10  2927 1.00
gamma[5,4]       7.66  2003 1.00
gamma[5,5]       4.65  2406 1.00
gamma[5,6]       5.38  1848 1.00
gamma[5,7]       8.68  3794 1.00
gamma[5,8]       3.14  1476 1.00
gamma[5,9]       5.27  1322 1.00
gamma[5,10]      6.50  1850 1.00
gamma[5,11]      6.92  2089 1.00
gamma[6,1]       9.09  3414 1.00
gamma[6,2]       4.03  4987 1.00
gamma[6,3]      11.13  2352 1.00
gamma[6,4]       6.40  1478 1.00
gamma[6,5]       3.46  3099 1.00
gamma[6,6]       5.05  1755 1.00
gamma[6,7]       7.05  3625 1.00
gamma[6,8]       4.02  1374 1.00
gamma[6,9]       2.74  1786 1.00
gamma[6,10]      3.02  1340 1.00
gamma[6,11]      1.40  3551 1.00
gamma[7,1]       6.17  3609 1.00
gamma[7,2]       9.35  6106 1.00
gamma[7,3]       7.40  2641 1.00
gamma[7,4]       6.09  1931 1.00
gamma[7,5]       9.92  2412 1.00
gamma[7,6]      11.03  2401 1.00
gamma[7,7]       7.20  5405 1.00
gamma[7,8]       6.90  1875 1.00
gamma[7,9]       4.26  1659 1.00
gamma[7,10]      5.92  1880 1.00
gamma[7,11]     -1.37  3640 1.00
gamma[8,1]       9.62  5030 1.00
gamma[8,2]       9.00  7361 1.00
gamma[8,3]       9.86  4077 1.00
gamma[8,4]       9.97  2586 1.00
gamma[8,5]       7.77  2506 1.00
gamma[8,6]      10.37  3407 1.00
gamma[8,7]       7.53  5929 1.00
gamma[8,8]       6.63  2617 1.00
gamma[8,9]       6.93  1932 1.00
gamma[8,10]      4.41  2034 1.00
gamma[8,11]      5.59  2678 1.00
gamma[9,1]       6.83  5343 1.00
gamma[9,2]       8.07  5862 1.00
gamma[9,3]       9.34  2819 1.00
gamma[9,4]       4.17  2236 1.00
gamma[9,5]       8.74  2332 1.00
gamma[9,6]       8.36  2930 1.00
gamma[9,7]       9.10  5301 1.00
gamma[9,8]       6.99  2367 1.00
gamma[9,9]       6.60  1679 1.00
gamma[9,10]      6.55  1588 1.00
gamma[9,11]      6.37  2474 1.00
gamma[10,1]      7.04  3535 1.00
gamma[10,2]     10.15  6576 1.00
gamma[10,3]     12.03  4005 1.00
gamma[10,4]      3.69  2735 1.00
gamma[10,5]      8.37  1856 1.00
gamma[10,6]      8.80  2535 1.00
gamma[10,7]      7.35  4571 1.00
gamma[10,8]      5.93  2202 1.00
gamma[10,9]      2.37  2148 1.00
gamma[10,10]     0.43  1749 1.00
gamma[10,11]     6.69  2379 1.00
gamma[11,1]     11.22  2962 1.00
gamma[11,2]      8.53  4531 1.00
gamma[11,3]      5.20  2878 1.00
gamma[11,4]      4.51  1908 1.00
gamma[11,5]      9.65  2025 1.00
gamma[11,6]      4.13  1753 1.00
gamma[11,7]      7.06  3773 1.00
gamma[11,8]      2.94  1565 1.00
gamma[11,9]      6.63  1766 1.00
gamma[11,10]     3.67  1504 1.00
gamma[11,11]     8.37  1276 1.00
gamma[12,1]      9.97  4101 1.00
gamma[12,2]      9.91  6400 1.00
gamma[12,3]      8.87  2780 1.00
gamma[12,4]      9.41  1833 1.00
gamma[12,5]      7.75  2392 1.00
gamma[12,6]      5.97  2829 1.00
gamma[12,7]      8.82  4796 1.00
gamma[12,8]      6.84  2226 1.00
gamma[12,9]      7.02  1672 1.00
gamma[12,10]     4.75  1234 1.00
gamma[12,11]     4.31  2316 1.00
gamma[13,1]     11.39  4112 1.00
gamma[13,2]      6.67  5005 1.00
gamma[13,3]      9.07  2192 1.00
gamma[13,4]      4.78  2332 1.00
gamma[13,5]     10.54  2190 1.00
gamma[13,6]      7.76  2358 1.00
gamma[13,7]      7.52  3522 1.00
gamma[13,8]      8.37  2044 1.00
gamma[13,9]      4.18  1550 1.00
gamma[13,10]     3.87  1348 1.00
gamma[13,11]     3.26  2166 1.00
sig[1]          37.16   909 1.01
sig[2]         177.57  1558 1.00
sig[3]         132.76   860 1.00
lp__         -1603.84  1719 1.00

Samples were drawn using NUTS(diag_e) at Fri Aug 13 21:12:03 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/dry_spab_50_model_inter_full_Full_slope.rda"
[1] "MCMC done!!"
