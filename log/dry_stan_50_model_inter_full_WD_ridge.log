── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  dry season"
[1] "Use full"
[1] "Habitat = ridge"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: except for WD"
[1] "sp number in seedling data: 66"
[1] "sp number in trait data: 66"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.18 as a scaling parameter for the distance effect"
[1] "n_sp = J =66"
[1] "n_para = K = 11"
[1] "n_plot = S = 111"
[1] "n_census = T = 10"
[1] "n_tag = M = 2934"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.003358 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 33.58 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.003791 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 37.91 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 3: 
Chain 3: Gradient evaluation took 0.003893 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 38.93 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.009259 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 92.59 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 31393.1 seconds (Warm-up)
Chain 2:                23246 seconds (Sampling)
Chain 2:                54639.2 seconds (Total)
Chain 2: 
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 31180.5 seconds (Warm-up)
Chain 1:                25702.6 seconds (Sampling)
Chain 1:                56883.1 seconds (Total)
Chain 1: 
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 34706 seconds (Warm-up)
Chain 3:                43500.6 seconds (Sampling)
Chain 3:                78206.6 seconds (Total)
Chain 3: 
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 48340.3 seconds (Warm-up)
Chain 4:                41396.6 seconds (Sampling)
Chain 4:                89736.9 seconds (Total)
Chain 4: 
Warning message:
Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean    sd     2.5%      25%      50%      75%
gamma[1,1]       2.67    0.02  1.24     0.29     1.81     2.67     3.50
gamma[1,2]      -2.85    0.04  1.97    -6.77    -4.17    -2.84    -1.52
gamma[1,3]      -0.96    0.01  0.62    -2.24    -1.37    -0.96    -0.55
gamma[1,4]       0.87    0.01  0.29     0.34     0.66     0.85     1.05
gamma[1,5]       0.02    0.01  0.34    -0.64    -0.21     0.02     0.24
gamma[1,6]       0.24    0.02  0.96    -1.64    -0.41     0.25     0.89
gamma[1,7]       0.80    0.03  1.72    -2.63    -0.35     0.81     1.97
gamma[1,8]      -0.22    0.01  0.37    -0.94    -0.47    -0.22     0.03
gamma[1,9]      -0.14    0.00  0.23    -0.59    -0.30    -0.14     0.01
gamma[1,10]      0.16    0.00  0.24    -0.33     0.01     0.17     0.33
gamma[1,11]      1.27    0.00  0.26     0.76     1.10     1.27     1.44
gamma[2,1]      -1.26    0.03  1.75    -4.73    -2.43    -1.26    -0.09
gamma[2,2]      -0.71    0.05  2.91    -6.40    -2.62    -0.72     1.24
gamma[2,3]      -0.64    0.02  0.87    -2.34    -1.23    -0.65    -0.06
gamma[2,4]       0.34    0.01  0.38    -0.45     0.10     0.35     0.59
gamma[2,5]       0.13    0.01  0.58    -1.00    -0.26     0.13     0.51
gamma[2,6]       0.25    0.03  1.53    -2.73    -0.76     0.25     1.27
gamma[2,7]       0.74    0.05  2.78    -4.67    -1.13     0.71     2.55
gamma[2,8]      -0.03    0.01  0.57    -1.13    -0.40    -0.04     0.34
gamma[2,9]       0.03    0.01  0.37    -0.69    -0.21     0.03     0.27
gamma[2,10]     -0.33    0.01  0.47    -1.26    -0.64    -0.33    -0.01
gamma[2,11]     -0.56    0.01  0.46    -1.51    -0.85    -0.54    -0.25
gamma[3,1]       3.26    0.02  1.24     0.78     2.43     3.25     4.09
gamma[3,2]       6.45    0.04  2.31     1.78     4.90     6.47     8.01
gamma[3,3]       0.60    0.01  0.66    -0.73     0.17     0.60     1.04
gamma[3,4]      -0.95    0.01  0.33    -1.64    -1.16    -0.93    -0.72
gamma[3,5]       0.83    0.01  0.42     0.02     0.55     0.83     1.11
gamma[3,6]       0.36    0.02  1.10    -1.80    -0.39     0.33     1.11
gamma[3,7]       0.39    0.04  2.16    -3.87    -1.08     0.36     1.81
gamma[3,8]      -0.02    0.01  0.40    -0.82    -0.28    -0.02     0.25
gamma[3,9]       0.29    0.00  0.27    -0.24     0.11     0.28     0.47
gamma[3,10]      0.22    0.00  0.31    -0.38     0.01     0.21     0.43
gamma[3,11]     -0.19    0.00  0.30    -0.79    -0.39    -0.20     0.00
gamma[4,1]      -1.19    0.03  1.75    -4.57    -2.36    -1.21    -0.07
gamma[4,2]      -4.38    0.05  2.78    -9.64    -6.28    -4.40    -2.48
gamma[4,3]      -0.17    0.02  0.81    -1.77    -0.72    -0.18     0.37
gamma[4,4]       0.13    0.01  0.34    -0.56    -0.09     0.14     0.36
gamma[4,5]      -0.28    0.01  0.55    -1.35    -0.64    -0.29     0.09
gamma[4,6]       0.56    0.03  1.53    -2.47    -0.47     0.55     1.58
gamma[4,7]       1.80    0.05  2.63    -3.28     0.01     1.83     3.58
gamma[4,8]      -0.07    0.01  0.54    -1.14    -0.43    -0.07     0.29
gamma[4,9]      -0.11    0.01  0.33    -0.75    -0.33    -0.10     0.11
gamma[4,10]      0.27    0.01  0.44    -0.60    -0.03     0.27     0.57
gamma[4,11]      0.36    0.01  0.38    -0.38     0.11     0.36     0.62
gamma[5,1]      -0.41    0.03  1.34    -3.06    -1.31    -0.39     0.49
gamma[5,2]      -0.54    0.04  2.35    -5.12    -2.11    -0.52     1.04
gamma[5,3]      -0.33    0.01  0.62    -1.56    -0.74    -0.33     0.09
gamma[5,4]      -0.04    0.00  0.27    -0.58    -0.21    -0.03     0.14
gamma[5,5]       0.20    0.01  0.42    -0.62    -0.09     0.19     0.48
gamma[5,6]       0.38    0.03  1.20    -1.93    -0.43     0.39     1.18
gamma[5,7]       2.31    0.04  2.22    -2.01     0.83     2.30     3.81
gamma[5,8]      -0.65    0.01  0.41    -1.47    -0.92    -0.64    -0.38
gamma[5,9]       0.03    0.00  0.25    -0.48    -0.14     0.03     0.19
gamma[5,10]     -0.02    0.01  0.32    -0.65    -0.23    -0.02     0.19
gamma[5,11]     -0.38    0.01  0.32    -1.05    -0.59    -0.37    -0.17
gamma[6,1]      -0.28    0.03  1.96    -4.14    -1.62    -0.28     1.02
gamma[6,2]      -2.46    0.05  3.28    -8.89    -4.71    -2.45    -0.20
gamma[6,3]      -0.19    0.01  0.85    -1.87    -0.74    -0.19     0.37
gamma[6,4]      -0.22    0.01  0.48    -1.12    -0.55    -0.25     0.08
gamma[6,5]      -0.93    0.02  0.76    -2.42    -1.44    -0.93    -0.42
gamma[6,6]      -0.35    0.03  1.74    -3.78    -1.48    -0.37     0.78
gamma[6,7]       0.93    0.05  3.08    -5.08    -1.14     0.93     3.04
gamma[6,8]      -0.03    0.01  0.58    -1.16    -0.42    -0.03     0.36
gamma[6,9]       0.14    0.01  0.44    -0.76    -0.14     0.14     0.43
gamma[6,10]     -0.09    0.01  0.60    -1.30    -0.50    -0.08     0.32
gamma[6,11]      1.56    0.01  0.56     0.48     1.19     1.55     1.92
gamma[7,1]       0.19    0.05  2.63    -4.91    -1.59     0.23     1.98
gamma[7,2]       1.91    0.04  3.45    -4.88    -0.38     1.90     4.23
gamma[7,3]       1.48    0.03  1.60    -1.64     0.40     1.48     2.57
gamma[7,4]       0.43    0.03  1.05    -1.77    -0.25     0.50     1.16
gamma[7,5]       0.44    0.03  1.27    -2.05    -0.41     0.45     1.31
gamma[7,6]       2.01    0.03  2.05    -1.95     0.58     2.04     3.41
gamma[7,7]       1.96    0.04  3.22    -4.27    -0.24     1.89     4.11
gamma[7,8]       0.74    0.02  1.07    -1.39     0.04     0.76     1.47
gamma[7,9]      -0.46    0.02  0.82    -2.05    -0.98    -0.48     0.06
gamma[7,10]      0.49    0.02  1.00    -1.42    -0.19     0.49     1.14
gamma[7,11]     -1.46    0.02  0.80    -3.01    -1.99    -1.47    -0.93
gamma[8,1]      -2.39    0.04  2.30    -6.85    -3.95    -2.41    -0.83
gamma[8,2]      -1.49    0.05  3.03    -7.31    -3.55    -1.52     0.54
gamma[8,3]       1.13    0.03  1.53    -1.84     0.10     1.13     2.15
gamma[8,4]       0.45    0.02  1.03    -1.72    -0.20     0.52     1.17
gamma[8,5]       0.59    0.03  1.36    -2.09    -0.33     0.60     1.48
gamma[8,6]      -0.20    0.03  1.83    -3.87    -1.43    -0.20     1.02
gamma[8,7]      -2.54    0.04  2.87    -8.15    -4.47    -2.47    -0.62
gamma[8,8]       0.51    0.02  1.05    -1.58    -0.17     0.52     1.21
gamma[8,9]      -0.77    0.02  0.83    -2.43    -1.30    -0.77    -0.25
gamma[8,10]      0.53    0.02  1.08    -1.53    -0.19     0.52     1.24
gamma[8,11]     -1.35    0.02  0.92    -3.13    -1.96    -1.37    -0.77
gamma[9,1]       1.62    0.03  1.56    -1.38     0.56     1.63     2.68
gamma[9,2]       3.50    0.05  2.88    -2.15     1.51     3.49     5.45
gamma[9,3]      -0.02    0.01  0.61    -1.24    -0.42    -0.01     0.40
gamma[9,4]      -0.36    0.00  0.31    -0.96    -0.56    -0.37    -0.17
gamma[9,5]       0.12    0.01  0.46    -0.75    -0.20     0.11     0.42
gamma[9,6]      -0.25    0.02  1.39    -2.97    -1.17    -0.25     0.70
gamma[9,7]       1.58    0.04  2.66    -3.58    -0.19     1.59     3.39
gamma[9,8]      -0.37    0.01  0.39    -1.11    -0.63    -0.37    -0.11
gamma[9,9]      -0.22    0.00  0.33    -0.86    -0.44    -0.21    -0.01
gamma[9,10]      0.50    0.00  0.35    -0.20     0.27     0.50     0.73
gamma[9,11]     -0.09    0.01  0.33    -0.77    -0.30    -0.09     0.13
gamma[10,1]     -3.40    0.03  1.46    -6.26    -4.38    -3.37    -2.44
gamma[10,2]     -6.20    0.04  2.59   -11.38    -7.93    -6.17    -4.48
gamma[10,3]     -0.08    0.01  0.61    -1.27    -0.47    -0.08     0.32
gamma[10,4]      0.26    0.01  0.41    -0.60    -0.01     0.27     0.54
gamma[10,5]      0.05    0.01  0.59    -1.10    -0.35     0.05     0.46
gamma[10,6]     -1.46    0.02  1.32    -4.02    -2.34    -1.48    -0.57
gamma[10,7]     -2.96    0.04  2.45    -7.71    -4.63    -2.98    -1.32
gamma[10,8]      0.07    0.01  0.41    -0.72    -0.21     0.06     0.34
gamma[10,9]     -0.29    0.01  0.34    -0.98    -0.51    -0.29    -0.06
gamma[10,10]     0.23    0.01  0.47    -0.68    -0.09     0.23     0.55
gamma[10,11]    -0.32    0.01  0.37    -1.06    -0.57    -0.32    -0.07
gamma[11,1]      0.54    0.03  2.07    -3.45    -0.87     0.53     1.96
gamma[11,2]      2.90    0.05  3.50    -4.01     0.53     2.91     5.28
gamma[11,3]     -1.01    0.02  1.09    -3.19    -1.74    -1.00    -0.30
gamma[11,4]     -0.30    0.01  0.49    -1.27    -0.62    -0.30     0.02
gamma[11,5]      0.56    0.01  0.75    -0.86     0.05     0.54     1.05
gamma[11,6]     -1.35    0.03  1.80    -4.91    -2.57    -1.34    -0.15
gamma[11,7]     -2.20    0.05  3.27    -8.66    -4.36    -2.23     0.00
gamma[11,8]     -0.71    0.01  0.74    -2.20    -1.20    -0.70    -0.21
gamma[11,9]     -0.31    0.01  0.45    -1.23    -0.60    -0.30     0.00
gamma[11,10]     0.09    0.01  0.60    -1.05    -0.32     0.09     0.49
gamma[11,11]    -0.20    0.01  0.56    -1.33    -0.57    -0.19     0.17
gamma[12,1]      0.98    0.03  1.33    -1.64     0.10     1.00     1.86
gamma[12,2]      2.33    0.04  2.30    -2.28     0.79     2.35     3.88
gamma[12,3]     -0.12    0.01  0.74    -1.61    -0.60    -0.10     0.37
gamma[12,4]      0.12    0.01  0.32    -0.51    -0.08     0.12     0.32
gamma[12,5]      0.06    0.01  0.41    -0.76    -0.21     0.06     0.34
gamma[12,6]     -0.38    0.02  1.14    -2.66    -1.13    -0.37     0.40
gamma[12,7]     -0.64    0.04  2.09    -4.71    -2.06    -0.65     0.77
gamma[12,8]      0.00    0.01  0.46    -0.89    -0.31    -0.01     0.30
gamma[12,9]      0.02    0.00  0.28    -0.56    -0.16     0.02     0.21
gamma[12,10]    -0.34    0.01  0.33    -1.00    -0.56    -0.34    -0.12
gamma[12,11]    -0.17    0.01  0.33    -0.84    -0.39    -0.16     0.04
sig[1]           0.51    0.00  0.09     0.34     0.44     0.50     0.56
sig[2]           0.35    0.00  0.14     0.16     0.26     0.33     0.42
sig[3]           1.06    0.01  0.20     0.65     0.93     1.06     1.19
lp__         -4432.67    5.09 96.58 -4627.87 -4495.94 -4432.02 -4368.69
                97.5% n_eff Rhat
gamma[1,1]       5.13  2484 1.00
gamma[1,2]       0.96  3139 1.00
gamma[1,3]       0.27  2622 1.00
gamma[1,4]       1.49  3307 1.00
gamma[1,5]       0.71  3143 1.00
gamma[1,6]       2.09  2329 1.00
gamma[1,7]       4.11  2782 1.00
gamma[1,8]       0.50  3563 1.00
gamma[1,9]       0.33  4726 1.00
gamma[1,10]      0.63  4541 1.00
gamma[1,11]      1.77  4196 1.00
gamma[2,1]       2.15  2581 1.00
gamma[2,2]       5.05  3592 1.00
gamma[2,3]       1.07  2347 1.00
gamma[2,4]       1.07  2622 1.00
gamma[2,5]       1.30  2500 1.00
gamma[2,6]       3.25  2719 1.00
gamma[2,7]       6.25  3102 1.00
gamma[2,8]       1.11  3368 1.00
gamma[2,9]       0.76  3029 1.00
gamma[2,10]      0.60  2589 1.00
gamma[2,11]      0.29  3126 1.00
gamma[3,1]       5.72  3579 1.00
gamma[3,2]      11.02  4085 1.00
gamma[3,3]       1.87  2992 1.00
gamma[3,4]      -0.34  3176 1.00
gamma[3,5]       1.67  3905 1.00
gamma[3,6]       2.61  3415 1.00
gamma[3,7]       4.70  3582 1.00
gamma[3,8]       0.78  4291 1.00
gamma[3,9]       0.85  4591 1.00
gamma[3,10]      0.86  4840 1.00
gamma[3,11]      0.44  5456 1.00
gamma[4,1]       2.30  2619 1.00
gamma[4,2]       1.26  3543 1.00
gamma[4,3]       1.42  2825 1.00
gamma[4,4]       0.79  3468 1.00
gamma[4,5]       0.80  3448 1.00
gamma[4,6]       3.60  2091 1.00
gamma[4,7]       6.99  2874 1.00
gamma[4,8]       0.98  2559 1.00
gamma[4,9]       0.53  2877 1.00
gamma[4,10]      1.12  3806 1.00
gamma[4,11]      1.09  3686 1.00
gamma[5,1]       2.20  2690 1.00
gamma[5,2]       4.08  3027 1.00
gamma[5,3]       0.89  3434 1.00
gamma[5,4]       0.47  3605 1.00
gamma[5,5]       1.02  3267 1.00
gamma[5,6]       2.74  2215 1.00
gamma[5,7]       6.66  2610 1.00
gamma[5,8]       0.14  3323 1.00
gamma[5,9]       0.50  2983 1.00
gamma[5,10]      0.62  3888 1.00
gamma[5,11]      0.24  3270 1.00
gamma[6,1]       3.49  3544 1.00
gamma[6,2]       4.05  4445 1.00
gamma[6,3]       1.52  3460 1.00
gamma[6,4]       0.81  2164 1.00
gamma[6,5]       0.58  2129 1.00
gamma[6,6]       3.17  3210 1.00
gamma[6,7]       6.89  3364 1.00
gamma[6,8]       1.14  3305 1.00
gamma[6,9]       0.96  2784 1.00
gamma[6,10]      1.08  2536 1.00
gamma[6,11]      2.67  2788 1.00
gamma[7,1]       5.28  3326 1.00
gamma[7,2]       8.80  6120 1.00
gamma[7,3]       4.59  2548 1.00
gamma[7,4]       2.30  1764 1.00
gamma[7,5]       2.91  2045 1.00
gamma[7,6]       6.02  3539 1.00
gamma[7,7]       8.38  5452 1.00
gamma[7,8]       2.85  2504 1.00
gamma[7,9]       1.23  2401 1.00
gamma[7,10]      2.49  2059 1.00
gamma[7,11]      0.16  2711 1.00
gamma[8,1]       2.21  3090 1.00
gamma[8,2]       4.57  4388 1.00
gamma[8,3]       4.17  2539 1.00
gamma[8,4]       2.32  1737 1.00
gamma[8,5]       3.22  1807 1.00
gamma[8,6]       3.42  2748 1.00
gamma[8,7]       3.06  4120 1.00
gamma[8,8]       2.53  2192 1.00
gamma[8,9]       0.89  2172 1.00
gamma[8,10]      2.67  2033 1.00
gamma[8,11]      0.52  2527 1.00
gamma[9,1]       4.61  3818 1.00
gamma[9,2]       9.20  3926 1.00
gamma[9,3]       1.17  4148 1.00
gamma[9,4]       0.25  4495 1.00
gamma[9,5]       1.03  4449 1.00
gamma[9,6]       2.50  3829 1.00
gamma[9,7]       6.85  4429 1.00
gamma[9,8]       0.42  4291 1.00
gamma[9,9]       0.44  4427 1.00
gamma[9,10]      1.20  5396 1.00
gamma[9,11]      0.52  4232 1.00
gamma[10,1]     -0.50  3331 1.00
gamma[10,2]     -1.03  3926 1.00
gamma[10,3]      1.13  3052 1.00
gamma[10,4]      1.00  2084 1.00
gamma[10,5]      1.20  2363 1.00
gamma[10,6]      1.12  3301 1.00
gamma[10,7]      1.83  3641 1.00
gamma[10,8]      0.88  3717 1.00
gamma[10,9]      0.36  2859 1.00
gamma[10,10]     1.17  2458 1.00
gamma[10,11]     0.41  3122 1.00
gamma[11,1]      4.57  3812 1.00
gamma[11,2]      9.71  4996 1.00
gamma[11,3]      1.11  2975 1.00
gamma[11,4]      0.70  3009 1.00
gamma[11,5]      2.09  2633 1.00
gamma[11,6]      2.18  3119 1.00
gamma[11,7]      4.23  3958 1.00
gamma[11,8]      0.71  2691 1.00
gamma[11,9]      0.55  2748 1.00
gamma[11,10]     1.28  2726 1.00
gamma[11,11]     0.88  3100 1.00
gamma[12,1]      3.59  2469 1.00
gamma[12,2]      6.81  4060 1.00
gamma[12,3]      1.30  3027 1.00
gamma[12,4]      0.76  3827 1.00
gamma[12,5]      0.86  3855 1.00
gamma[12,6]      1.80  2916 1.00
gamma[12,7]      3.49  3184 1.00
gamma[12,8]      0.92  3994 1.00
gamma[12,9]      0.55  3904 1.00
gamma[12,10]     0.32  4028 1.00
gamma[12,11]     0.48  3419 1.00
sig[1]           0.70  1532 1.00
sig[2]           0.69  1928 1.00
sig[3]           1.44   356 1.01
lp__         -4243.40   360 1.01

Samples were drawn using NUTS(diag_e) at Mon Aug 23 06:19:56 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/dry_spab_50_model_inter_full_WD_ridge.rda"
[1] "MCMC done!!"
