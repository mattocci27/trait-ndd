── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  rainy season"
[1] "Use full"
[1] "Habitat = valley"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: except for WD"
[1] "sp number in seedling data: 69"
[1] "sp number in trait data: 69"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.34 as a scaling parameter for the distance effect"
[1] "n_sp = J =69"
[1] "n_para = K = 11"
[1] "n_plot = S = 180"
[1] "n_census = T = 10"
[1] "n_tag = M = 2978"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 2: 
Chain 2: Gradient evaluation took 0.003563 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 35.63 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 3: 
Chain 3: Gradient evaluation took 0.003952 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 39.52 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 1: 
Chain 1: Gradient evaluation took 0.004325 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 43.25 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.00862 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 86.2 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Exception: validate transformed params: y is not positive definite.  (in 'model117d050dfebf_model_inter' at line 59)

Chain 3: Exception: validate transformed params: y is not positive definite.  (in 'model117d050dfebf_model_inter' at line 59)

Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 81013.5 seconds (Warm-up)
Chain 4:                27055.8 seconds (Sampling)
Chain 4:                108069 seconds (Total)
Chain 4: 
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 74008.6 seconds (Warm-up)
Chain 2:                52163.7 seconds (Sampling)
Chain 2:                126172 seconds (Total)
Chain 2: 
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 74313.6 seconds (Warm-up)
Chain 3:                51920.5 seconds (Sampling)
Chain 3:                126234 seconds (Total)
Chain 3: 
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 83274.9 seconds (Warm-up)
Chain 1:                48436.4 seconds (Sampling)
Chain 1:                131711 seconds (Total)
Chain 1: 
Warning messages:
1: In validityMethod(object) :
  The following variables have undefined values:  Omega[1,1],The following variables have undefined values:  Omega[2,1],The following variables have undefined values:  Omega[3,1],The following variables have undefined values:  Omega[4,1],The following variables have undefined values:  Omega[5,1],The following variables have undefined values:  Omega[6,1],The following variables have undefined values:  Omega[7,1],The following variables have undefined values:  Omega[8,1],The following variables have undefined values:  Omega[9,1],The following variables have undefined values:  Omega[10,1],The following variables have undefined values:  Omega[11,1],The following variables have undefined values:  Omega[1,2],The following variables have undefined values:  Omega[2,2],The following variables have undefined values:  Omega[3,2],The following variables have undefined values:  Omega[4,2],The following variables have undefined values:  Omega[5,2],The following variables have undefined values:  Omega[ [... truncated]
2: There were 169 divergent transitions after warmup. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
3: Examine the pairs() plot to diagnose sampling problems
 
4: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
5: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean     sd     2.5%      25%      50%      75%
gamma[1,1]       0.69    0.05   4.97    -8.76    -2.68     0.62     4.05
gamma[1,2]       1.08    0.08   5.18    -9.28    -2.29     1.03     4.61
gamma[1,3]      -2.84    0.07   4.84   -12.43    -6.08    -2.85     0.38
gamma[1,4]       3.07    0.05   4.24    -5.29     0.26     3.07     5.89
gamma[1,5]       0.45    0.08   4.76    -8.74    -2.80     0.41     3.69
gamma[1,6]      -0.09    0.05   5.04   -10.05    -3.49    -0.11     3.29
gamma[1,7]      -3.06    0.07   4.71   -12.43    -6.10    -3.07     0.07
gamma[1,8]      -0.78    0.05   3.85    -8.33    -3.35    -0.76     1.72
gamma[1,9]      -4.11    0.06   4.31   -12.61    -7.00    -4.19    -1.30
gamma[1,10]      0.02    0.05   3.34    -6.60    -2.15     0.06     2.23
gamma[1,11]      6.20    0.21   6.03    -4.99     2.04     5.94    10.04
gamma[2,1]      -1.06    0.05   5.06   -11.07    -4.44    -1.07     2.39
gamma[2,2]       1.37    0.07   4.97    -8.79    -1.91     1.41     4.80
gamma[2,3]      -0.74    0.05   4.75   -10.06    -3.96    -0.68     2.55
gamma[2,4]       2.49    0.05   4.30    -6.15    -0.33     2.50     5.34
gamma[2,5]       0.04    0.06   4.58    -8.78    -3.05     0.04     3.11
gamma[2,6]       1.31    0.05   3.97    -6.52    -1.36     1.32     3.92
gamma[2,7]      -2.40    0.05   4.40   -10.99    -5.40    -2.37     0.64
gamma[2,8]      -0.19    0.05   3.90    -7.96    -2.80    -0.14     2.45
gamma[2,9]      -1.63    0.05   4.21   -10.13    -4.44    -1.56     1.28
gamma[2,10]     -1.07    0.05   3.62    -8.04    -3.46    -1.05     1.34
gamma[2,11]      0.90    0.06   4.89    -8.63    -2.37     0.94     4.21
gamma[3,1]      -0.29    0.05   4.85    -9.69    -3.55    -0.33     2.98
gamma[3,2]       0.78    0.05   5.15    -9.29    -2.65     0.79     4.22
gamma[3,3]      -0.49    0.05   4.93   -10.26    -3.85    -0.55     2.82
gamma[3,4]       0.71    0.05   4.50    -7.95    -2.39     0.67     3.81
gamma[3,5]      -1.71    0.06   4.81   -11.32    -4.89    -1.76     1.57
gamma[3,6]      -0.62    0.05   4.20    -8.85    -3.44    -0.68     2.17
gamma[3,7]      -1.05    0.05   4.66   -10.11    -4.19    -1.08     2.10
gamma[3,8]       0.33    0.06   4.25    -7.86    -2.59     0.34     3.17
gamma[3,9]      -2.55    0.06   4.37   -11.07    -5.45    -2.58     0.40
gamma[3,10]      1.42    0.05   3.95    -6.37    -1.28     1.48     4.05
gamma[3,11]      0.61    0.05   4.84    -8.76    -2.62     0.64     3.86
gamma[4,1]       0.84    0.05   4.88    -8.82    -2.54     0.88     4.15
gamma[4,2]      -0.71    0.07   4.91   -10.20    -4.00    -0.72     2.54
gamma[4,3]       0.54    0.06   4.95    -9.22    -2.64     0.59     3.79
gamma[4,4]      -0.06    0.06   4.13    -8.31    -2.74    -0.04     2.78
gamma[4,5]      -1.93    0.07   4.70   -10.89    -5.12    -1.99     1.19
gamma[4,6]       0.28    0.06   4.04    -7.85    -2.44     0.32     3.06
gamma[4,7]       0.38    0.06   4.37    -7.87    -2.65     0.34     3.37
gamma[4,8]       0.19    0.05   3.96    -7.59    -2.57     0.14     2.85
gamma[4,9]      -2.64    0.07   4.34   -10.75    -5.62    -2.73     0.22
gamma[4,10]      4.26    0.05   3.63    -2.75     1.84     4.17     6.66
gamma[4,11]      0.61    0.06   4.93    -8.99    -2.67     0.69     4.00
gamma[5,1]      -0.21    0.05   4.89    -9.85    -3.48    -0.28     3.13
gamma[5,2]       0.06    0.05   4.94    -9.62    -3.37     0.05     3.52
gamma[5,3]       1.66    0.06   5.03    -8.30    -1.66     1.67     5.00
gamma[5,4]       2.08    0.08   4.57    -6.95    -0.93     2.12     5.19
gamma[5,5]      -2.77    0.07   4.87   -12.35    -6.10    -2.72     0.53
gamma[5,6]      -0.14    0.05   4.24    -8.71    -2.93    -0.11     2.67
gamma[5,7]       0.84    0.05   4.63    -8.37    -2.26     0.91     3.96
gamma[5,8]      -2.23    0.05   4.21   -10.58    -5.03    -2.18     0.58
gamma[5,9]       0.30    0.05   4.27    -8.05    -2.53     0.25     3.03
gamma[5,10]     -1.70    0.05   3.78    -9.20    -4.23    -1.69     0.78
gamma[5,11]     -1.75    0.06   5.06   -11.72    -5.23    -1.70     1.64
gamma[6,1]      -1.13    0.06   4.87   -10.76    -4.37    -1.13     2.07
gamma[6,2]       1.92    0.09   5.28    -8.67    -1.60     1.96     5.50
gamma[6,3]      -0.52    0.05   4.86    -9.96    -3.78    -0.49     2.70
gamma[6,4]       2.94    0.05   4.56    -5.97    -0.12     3.03     6.08
gamma[6,5]      -0.72    0.06   4.80    -9.98    -4.06    -0.80     2.61
gamma[6,6]      -0.88    0.05   4.01    -8.68    -3.53    -0.88     1.75
gamma[6,7]      -1.81    0.07   4.75   -11.11    -4.95    -1.77     1.32
gamma[6,8]       2.03    0.05   4.20    -6.28    -0.77     1.95     4.81
gamma[6,9]      -0.06    0.05   4.28    -8.46    -2.89    -0.06     2.82
gamma[6,10]     -3.01    0.06   3.79   -10.50    -5.56    -2.91    -0.44
gamma[6,11]     -0.52    0.05   4.94   -10.31    -3.75    -0.49     2.72
gamma[7,1]       1.27    0.06   4.98    -8.58    -2.11     1.31     4.62
gamma[7,2]      -1.24    0.06   5.08   -11.04    -4.69    -1.15     2.18
gamma[7,3]      -0.70    0.05   4.97   -10.25    -4.16    -0.68     2.70
gamma[7,4]      -0.44    0.05   4.49    -9.18    -3.47    -0.43     2.57
gamma[7,5]       1.25    0.06   4.73    -8.05    -1.90     1.21     4.31
gamma[7,6]       0.52    0.06   4.28    -7.71    -2.45     0.51     3.48
gamma[7,7]       2.28    0.08   4.97    -7.62    -1.00     2.36     5.61
gamma[7,8]       1.91    0.05   4.35    -6.73    -0.97     1.94     4.78
gamma[7,9]      -1.50    0.06   4.37    -9.83    -4.47    -1.51     1.42
gamma[7,10]      0.01    0.05   3.98    -7.62    -2.71    -0.03     2.72
gamma[7,11]     -0.43    0.05   4.87    -9.97    -3.77    -0.43     3.01
gamma[8,1]      -1.87    0.05   4.92   -11.56    -5.13    -1.93     1.44
gamma[8,2]       2.37    0.09   5.05    -7.66    -1.08     2.43     5.74
gamma[8,3]       0.89    0.05   4.82    -8.54    -2.32     0.80     4.11
gamma[8,4]       1.35    0.06   4.63    -7.87    -1.71     1.31     4.41
gamma[8,5]      -2.65    0.07   4.90   -12.26    -5.95    -2.74     0.65
gamma[8,6]      -1.22    0.07   4.44    -9.90    -4.21    -1.23     1.76
gamma[8,7]      -3.11    0.10   5.08   -12.80    -6.62    -3.18     0.34
gamma[8,8]      -0.65    0.06   4.37    -9.17    -3.61    -0.67     2.22
gamma[8,9]       0.39    0.06   4.46    -8.46    -2.57     0.39     3.34
gamma[8,10]     -1.21    0.05   4.01    -9.13    -3.87    -1.20     1.49
gamma[8,11]      0.19    0.05   4.96    -9.59    -3.15     0.24     3.52
gamma[9,1]      -0.17    0.05   4.87    -9.69    -3.51    -0.14     3.07
gamma[9,2]      -0.82    0.05   5.00   -10.57    -4.21    -0.85     2.60
gamma[9,3]       0.54    0.06   5.00    -9.19    -2.76     0.60     3.83
gamma[9,4]       0.40    0.05   4.29    -7.99    -2.43     0.40     3.27
gamma[9,5]       0.25    0.05   4.70    -8.78    -2.96     0.13     3.42
gamma[9,6]       1.02    0.05   4.22    -7.29    -1.79     1.05     3.83
gamma[9,7]       0.00    0.05   4.76    -9.36    -3.19     0.00     3.16
gamma[9,8]       0.91    0.05   4.31    -7.72    -1.95     0.92     3.83
gamma[9,9]       2.02    0.06   4.22    -6.26    -0.76     1.98     4.80
gamma[9,10]     -0.01    0.05   3.89    -7.87    -2.48     0.04     2.61
gamma[9,11]     -0.40    0.05   4.96   -10.10    -3.75    -0.41     2.97
gamma[10,1]      0.26    0.05   4.99    -9.43    -3.06     0.23     3.63
gamma[10,2]     -1.12    0.07   5.07   -10.94    -4.56    -1.13     2.33
gamma[10,3]     -0.67    0.06   5.02   -10.53    -3.97    -0.65     2.62
gamma[10,4]     -0.64    0.06   4.71    -9.83    -3.93    -0.62     2.53
gamma[10,5]      1.01    0.06   4.77    -8.29    -2.19     1.03     4.25
gamma[10,6]      1.92    0.05   4.18    -6.38    -0.84     1.96     4.70
gamma[10,7]      0.53    0.05   4.85    -9.02    -2.85     0.60     3.83
gamma[10,8]     -2.61    0.07   4.45   -11.31    -5.60    -2.65     0.33
gamma[10,9]      1.90    0.05   4.47    -7.01    -1.05     1.94     4.91
gamma[10,10]    -1.81    0.05   4.01    -9.69    -4.52    -1.80     0.88
gamma[10,11]    -0.29    0.05   4.85    -9.81    -3.58    -0.33     3.05
gamma[11,1]      0.20    0.05   4.90    -9.20    -3.13     0.15     3.56
gamma[11,2]     -0.69    0.06   5.01   -10.62    -4.02    -0.68     2.64
gamma[11,3]     -1.35    0.05   4.85   -11.06    -4.56    -1.29     1.90
gamma[11,4]     -0.10    0.05   4.44    -8.73    -3.10    -0.20     2.95
gamma[11,5]      3.21    0.07   4.89    -6.54    -0.03     3.26     6.56
gamma[11,6]      2.67    0.05   4.12    -5.42    -0.12     2.63     5.36
gamma[11,7]      0.43    0.06   4.77    -8.98    -2.79     0.45     3.78
gamma[11,8]     -0.69    0.05   4.25    -8.99    -3.52    -0.72     2.16
gamma[11,9]     -0.14    0.05   4.37    -8.73    -3.07    -0.10     2.82
gamma[11,10]     0.30    0.05   3.95    -7.67    -2.31     0.36     3.03
gamma[11,11]     0.14    0.05   4.90    -9.56    -3.18     0.16     3.44
gamma[12,1]      0.60    0.05   4.87    -9.04    -2.71     0.67     3.91
gamma[12,2]     -0.82    0.07   4.91   -10.47    -4.12    -0.78     2.45
gamma[12,3]      1.44    0.06   4.70    -7.74    -1.64     1.48     4.60
gamma[12,4]      1.46    0.07   3.96    -6.61    -1.12     1.57     4.10
gamma[12,5]     -0.95    0.06   4.40    -9.65    -3.90    -0.87     2.06
gamma[12,6]     -2.24    0.06   3.61    -9.57    -4.57    -2.20     0.16
gamma[12,7]      0.80    0.08   4.47    -7.67    -2.25     0.74     3.73
gamma[12,8]      1.46    0.05   3.78    -5.98    -1.05     1.46     3.97
gamma[12,9]      0.02    0.06   4.22    -8.13    -2.78    -0.05     2.69
gamma[12,10]     0.29    0.08   3.41    -6.09    -2.03     0.10     2.42
gamma[12,11]    -0.31    0.05   4.80    -9.66    -3.54    -0.34     2.91
sig[1]         126.52    3.01  61.35    37.06    81.20   117.26   160.82
sig[2]         480.87   12.12 265.68   126.28   290.01   428.08   611.97
sig[3]         415.76    9.50 195.70   128.61   268.99   387.10   525.35
lp__         -2136.68    1.44  50.20 -2240.45 -2170.05 -2134.80 -2102.70
                97.5% n_eff Rhat
gamma[1,1]      10.47  9652 1.00
gamma[1,2]      11.05  4504 1.00
gamma[1,3]       6.74  5453 1.00
gamma[1,4]      11.53  7032 1.00
gamma[1,5]       9.74  3907 1.00
gamma[1,6]       9.79  9531 1.00
gamma[1,7]       6.18  5235 1.00
gamma[1,8]       6.86  6847 1.00
gamma[1,9]       4.69  5167 1.00
gamma[1,10]      6.56  5296 1.00
gamma[1,11]     18.94   853 1.00
gamma[2,1]       8.70  8478 1.00
gamma[2,2]      10.92  4423 1.00
gamma[2,3]       8.58  8860 1.00
gamma[2,4]      10.82  6494 1.00
gamma[2,5]       9.00  5804 1.00
gamma[2,6]       9.23  6707 1.00
gamma[2,7]       6.02  7327 1.00
gamma[2,8]       7.32  5247 1.00
gamma[2,9]       6.33  6110 1.00
gamma[2,10]      6.23  4795 1.00
gamma[2,11]     10.27  7028 1.00
gamma[3,1]       9.30  9693 1.00
gamma[3,2]      10.72  8973 1.00
gamma[3,3]       9.07  9291 1.00
gamma[3,4]       9.57  8231 1.00
gamma[3,5]       7.71  6680 1.00
gamma[3,6]       7.76  7328 1.00
gamma[3,7]       8.05  8084 1.00
gamma[3,8]       8.66  5491 1.00
gamma[3,9]       5.97  6124 1.00
gamma[3,10]      9.04  6708 1.00
gamma[3,11]     10.06  8709 1.00
gamma[4,1]      10.18  8041 1.00
gamma[4,2]       8.91  5434 1.00
gamma[4,3]      10.24  7563 1.00
gamma[4,4]       7.95  4570 1.00
gamma[4,5]       7.47  4745 1.00
gamma[4,6]       7.90  4248 1.00
gamma[4,7]       8.98  6293 1.00
gamma[4,8]       7.93  5920 1.00
gamma[4,9]       5.93  3930 1.00
gamma[4,10]     11.58  4724 1.00
gamma[4,11]     10.15  6780 1.00
gamma[5,1]       9.45 10096 1.00
gamma[5,2]       9.60 11595 1.00
gamma[5,3]      11.45  7995 1.00
gamma[5,4]      10.77  3533 1.00
gamma[5,5]       6.70  4555 1.00
gamma[5,6]       8.10  7358 1.00
gamma[5,7]       9.91  9424 1.00
gamma[5,8]       6.09  6230 1.00
gamma[5,9]       8.97  6544 1.00
gamma[5,10]      5.82  6034 1.00
gamma[5,11]      8.18  6625 1.00
gamma[6,1]       8.49  7575 1.00
gamma[6,2]      12.37  3150 1.00
gamma[6,3]       9.09  8551 1.00
gamma[6,4]      11.68  7374 1.00
gamma[6,5]       8.41  6684 1.00
gamma[6,6]       7.14  6247 1.00
gamma[6,7]       7.45  4933 1.00
gamma[6,8]      10.32  6577 1.00
gamma[6,9]       8.43  6124 1.00
gamma[6,10]      4.21  4232 1.00
gamma[6,11]      9.18  9193 1.00
gamma[7,1]      10.99  7439 1.00
gamma[7,2]       8.74  6586 1.00
gamma[7,3]       8.94  9319 1.00
gamma[7,4]       8.41  7398 1.00
gamma[7,5]      10.63  5846 1.00
gamma[7,6]       8.82  4536 1.00
gamma[7,7]      11.81  4309 1.00
gamma[7,8]      10.42  7098 1.00
gamma[7,9]       7.10  5635 1.00
gamma[7,10]      7.89  5543 1.00
gamma[7,11]      8.90  8429 1.00
gamma[8,1]       7.85  8798 1.00
gamma[8,2]      12.21  2977 1.00
gamma[8,3]      10.51  8659 1.00
gamma[8,4]      10.35  5261 1.00
gamma[8,5]       6.98  5185 1.00
gamma[8,6]       7.63  4268 1.00
gamma[8,7]       7.16  2517 1.00
gamma[8,8]       7.98  4904 1.00
gamma[8,9]       9.11  6278 1.00
gamma[8,10]      6.84  5778 1.00
gamma[8,11]      9.84  9113 1.00
gamma[9,1]       9.45  8868 1.00
gamma[9,2]       8.82  9948 1.00
gamma[9,3]      10.47  7561 1.00
gamma[9,4]       8.76  6283 1.00
gamma[9,5]       9.55  7799 1.00
gamma[9,6]       9.23  6188 1.00
gamma[9,7]       9.49  9385 1.00
gamma[9,8]       9.41  7147 1.00
gamma[9,9]      10.25  5315 1.00
gamma[9,10]      7.56  6445 1.00
gamma[9,11]      9.22  9283 1.00
gamma[10,1]     10.01  8275 1.00
gamma[10,2]      8.73  5135 1.00
gamma[10,3]      9.45  7750 1.00
gamma[10,4]      8.45  6275 1.00
gamma[10,5]     10.29  5796 1.00
gamma[10,6]     10.04  6006 1.00
gamma[10,7]      9.91  7890 1.00
gamma[10,8]      6.24  4551 1.00
gamma[10,9]     10.43  7114 1.00
gamma[10,10]     6.09  7047 1.00
gamma[10,11]     9.10  9927 1.00
gamma[11,1]      9.83  8479 1.00
gamma[11,2]      9.19  8083 1.00
gamma[11,3]      8.12  8564 1.00
gamma[11,4]      8.67  7851 1.00
gamma[11,5]     12.65  4523 1.00
gamma[11,6]     10.78  6014 1.00
gamma[11,7]      9.51  5388 1.00
gamma[11,8]      7.68  7045 1.00
gamma[11,9]      8.42  6423 1.00
gamma[11,10]     7.77  5500 1.00
gamma[11,11]     9.63  9803 1.00
gamma[12,1]     10.15  9448 1.00
gamma[12,2]      9.07  5392 1.00
gamma[12,3]     10.50  6802 1.00
gamma[12,4]      9.11  3560 1.00
gamma[12,5]      7.70  4699 1.00
gamma[12,6]      4.79  4030 1.00
gamma[12,7]      9.76  3256 1.00
gamma[12,8]      8.95  6288 1.00
gamma[12,9]      8.58  4298 1.00
gamma[12,10]     7.63  2007 1.00
gamma[12,11]     9.08  9651 1.00
sig[1]         273.60   415 1.01
sig[2]        1140.30   480 1.01
sig[3]         883.35   424 1.01
lp__         -2042.92  1219 1.00

Samples were drawn using NUTS(diag_e) at Sun Aug 22 05:19:22 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/rainy_spab_50_model_inter_full_WD_valley.rda"
[1] "MCMC done!!"
