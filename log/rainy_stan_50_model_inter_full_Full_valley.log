── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  rainy season"
[1] "Use full"
[1] "Habitat = valley"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: 1 + all the traits"
[1] "sp number in seedling data: 69"
[1] "sp number in trait data: 69"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.34 as a scaling parameter for the distance effect"
[1] "n_sp = J =69"
[1] "n_para = K = 11"
[1] "n_plot = S = 180"
[1] "n_census = T = 10"
[1] "n_tag = M = 2978"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.


COMPILING MODEL 'COMPILING MODEL 'model_intermodel_inter' NOW.
' NOW.


STARTING SAMPLER FOR MODEL 'STARTING SAMPLER FOR MODEL 'model_intermodel_inter' NOW.
' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.


SAMPLINGSAMPLING FOR MODEL ' FOR MODEL 'model_intermodel_inter' NOW (CHAIN ' NOW (CHAIN 24).
).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).
Chain 2: 
Chain 2: Gradient evaluation took 0.006816 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 68.16 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: 
Chain 1: Gradient evaluation took 0.007182 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 71.82 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.007488 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 74.88 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 3: 
Chain 3: Gradient evaluation took 0.0087 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 87 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 231954 seconds (Warm-up)
Chain 2:                15725.1 seconds (Sampling)
Chain 2:                247679 seconds (Total)
Chain 2: 
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 225465 seconds (Warm-up)
Chain 4:                69485.3 seconds (Sampling)
Chain 4:                294950 seconds (Total)
Chain 4: 
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 225256 seconds (Warm-up)
Chain 1:                103751 seconds (Sampling)
Chain 1:                329007 seconds (Total)
Chain 1: 
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 248489 seconds (Warm-up)
Chain 3:                87752.8 seconds (Sampling)
Chain 3:                336242 seconds (Total)
Chain 3: 
Warning messages:
1: There were 1939 divergent transitions after warmup. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
2: Examine the pairs() plot to diagnose sampling problems
 
3: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat 
4: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
5: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean     sd     2.5%      25%      50%      75%
gamma[1,1]       0.29    0.16   4.48    -8.59    -2.28    -0.20     3.10
gamma[1,2]       0.56    0.14   4.47    -8.28    -1.98     0.11     3.17
gamma[1,3]      -3.23    0.32   4.59   -11.94    -6.11    -3.86    -0.08
gamma[1,4]       3.05    0.31   4.10    -4.80     0.34     3.51     5.59
gamma[1,5]      -0.36    1.06   4.61    -8.32    -3.72    -1.02     2.80
gamma[1,6]       0.13    0.17   4.57    -8.96    -2.75     0.32     2.89
gamma[1,7]      -2.48    0.20   4.34   -11.49    -5.16    -2.28     0.18
gamma[1,8]      -1.20    0.53   3.85    -8.04    -3.99    -1.40     1.40
gamma[1,9]      -3.34    0.86   4.20   -12.13    -6.19    -2.75    -0.45
gamma[1,10]      0.08    0.09   3.22    -6.73    -1.75     0.31     1.92
gamma[1,11]      5.00    0.23   5.28    -4.85     1.79     4.38     7.98
gamma[2,1]      -1.55    0.65   4.69    -9.96    -4.93    -1.77     1.55
gamma[2,2]       0.54    0.47   4.78    -8.34    -2.44     0.12     3.69
gamma[2,3]       0.00    0.81   4.56    -9.55    -3.07     0.70     2.83
gamma[2,4]       2.20    0.50   4.11    -5.47    -0.60     1.77     4.90
gamma[2,5]       0.82    0.60   4.41    -8.31    -1.99     0.98     4.04
gamma[2,6]       1.90    0.76   3.89    -6.42    -0.65     2.37     4.58
gamma[2,7]      -3.32    1.22   4.52   -10.50    -6.65    -3.75    -0.08
gamma[2,8]      -1.46    1.53   4.38    -8.33    -5.31    -1.59     1.68
gamma[2,9]      -3.20    2.12   5.12   -11.95    -7.43    -2.89     0.63
gamma[2,10]     -0.41    0.72   3.53    -8.02    -2.73     0.04     2.04
gamma[2,11]      1.93    1.54   5.23    -8.85    -1.85     2.18     6.61
gamma[3,1]      -0.30    0.26   4.63    -9.49    -3.53     0.21     2.44
gamma[3,2]       0.64    0.50   4.82    -8.91    -2.81     1.20     4.12
gamma[3,3]      -0.07    0.29   4.53    -9.41    -2.87     0.28     3.00
gamma[3,4]      -0.76    0.58   4.87    -9.97    -4.08    -0.97     1.97
gamma[3,5]      -1.58    0.31   4.48   -10.96    -4.35    -1.18     1.16
gamma[3,6]      -0.11    0.22   4.07    -8.75    -2.54    -0.04     2.52
gamma[3,7]      -0.77    0.31   4.42    -9.40    -3.62    -0.98     1.93
gamma[3,8]      -1.22    0.58   4.25    -8.96    -4.15    -1.64     1.58
gamma[3,9]      -0.77    0.29   4.17    -9.71    -3.31    -0.23     1.82
gamma[3,10]      1.94    1.09   4.10    -6.42    -0.96     2.15     5.09
gamma[3,11]     -1.93    2.51   5.98   -12.27    -6.27    -1.60     2.49
gamma[4,1]      -0.80    0.43   4.50    -9.14    -3.74    -1.17     2.09
gamma[4,2]      -0.09    1.00   4.91    -8.75    -3.66    -0.51     3.19
gamma[4,3]      -1.09    0.49   4.51    -9.60    -3.92    -1.69     1.65
gamma[4,4]      -0.11    1.10   4.61    -8.14    -3.48    -0.26     3.01
gamma[4,5]      -1.18    0.21   4.40   -10.25    -3.83    -1.11     1.62
gamma[4,6]      -0.09    0.34   4.00    -8.60    -2.66     0.26     2.45
gamma[4,7]      -0.89    0.06   4.19    -9.60    -3.25    -0.90     1.54
gamma[4,8]      -0.25    1.01   4.40    -7.74    -3.46    -0.82     2.76
gamma[4,9]      -2.78    0.24   4.29   -11.03    -5.49    -3.12    -0.21
gamma[4,10]      0.69    0.15   3.89    -7.00    -1.70     0.42     3.06
gamma[4,11]      0.50    0.05   4.51    -8.81    -1.93     0.58     3.00
gamma[5,1]       1.81    1.27   5.18    -8.44    -1.65     2.08     5.34
gamma[5,2]      -0.61    0.12   4.50    -9.66    -3.26    -0.70     1.90
gamma[5,3]       0.54    0.07   4.48    -8.77    -1.97     0.39     3.17
gamma[5,4]      -1.17    1.03   4.34    -8.61    -4.46    -1.43     1.88
gamma[5,5]      -1.24    0.82   4.48   -10.43    -4.28    -0.88     2.16
gamma[5,6]       0.94    0.91   4.12    -7.48    -1.84     1.04     4.03
gamma[5,7]      -0.03    0.47   4.23    -7.73    -3.10    -0.32     2.59
gamma[5,8]       0.97    0.63   4.02    -7.25    -1.92     1.29     4.23
gamma[5,9]      -2.99    0.27   4.06   -10.73    -5.62    -3.50    -0.41
gamma[5,10]      4.15    0.36   3.61    -2.92     1.77     4.58     6.30
gamma[5,11]      1.02    0.59   4.58    -8.87    -1.75     1.69     3.69
gamma[6,1]      -0.25    0.11   4.54    -9.39    -2.95    -0.20     2.33
gamma[6,2]       2.55    2.89   6.32    -8.76    -2.00     1.99     7.04
gamma[6,3]       0.92    0.84   4.63    -7.59    -1.99     0.07     3.94
gamma[6,4]       2.19    0.30   4.24    -6.69    -0.46     2.64     4.82
gamma[6,5]      -0.73    2.35   5.68   -11.57    -4.91    -0.93     4.55
gamma[6,6]      -1.34    1.51   4.59    -8.57    -5.20    -1.68     1.87
gamma[6,7]       1.61    0.84   4.59    -7.98    -1.50     2.02     5.03
gamma[6,8]      -1.66    0.36   4.03    -9.99    -4.23    -1.34     0.81
gamma[6,9]       1.15    1.04   4.37    -7.80    -1.78     1.33     4.60
gamma[6,10]      0.34    2.46   5.14    -8.80    -3.43    -0.31     4.85
gamma[6,11]     -0.04    2.04   5.52   -11.02    -4.13    -0.04     4.85
gamma[7,1]       0.03    1.26   4.92   -10.23    -3.35     0.61     3.95
gamma[7,2]       1.26    0.31   4.74    -8.07    -1.73     0.98     4.12
gamma[7,3]      -1.89    1.94   5.11    -9.99    -6.45    -2.02     1.78
gamma[7,4]       4.24    1.84   5.13    -5.74     0.50     4.40     8.45
gamma[7,5]       0.11    0.55   4.56    -9.55    -2.80     0.58     3.15
gamma[7,6]      -0.95    0.07   3.65    -8.28    -3.02    -1.21     1.17
gamma[7,7]      -1.42    0.10   4.25   -10.52    -3.86    -1.08     0.89
gamma[7,8]       3.11    1.40   4.48    -5.93    -0.08     3.49     6.24
gamma[7,9]       0.03    0.23   4.17    -8.46    -2.80     0.50     2.68
gamma[7,10]     -2.73    0.30   3.62   -10.69    -4.93    -2.17    -0.47
gamma[7,11]     -0.01    0.35   4.72    -9.73    -3.08     0.35     3.01
gamma[8,1]       0.94    0.20   4.67    -8.35    -2.06     1.03     3.52
gamma[8,2]      -0.96    0.11   4.55   -10.35    -3.50    -1.08     1.79
gamma[8,3]      -0.13    0.33   4.58    -9.90    -3.08     0.05     3.19
gamma[8,4]       0.85    1.54   5.18    -9.33    -2.77     0.67     4.86
gamma[8,5]       0.39    1.00   4.61    -8.04    -2.75    -0.35     3.54
gamma[8,6]       0.21    0.07   4.06    -7.88    -2.28     0.09     2.47
gamma[8,7]       0.42    1.82   5.52    -9.01    -3.80     0.59     4.38
gamma[8,8]       3.75    2.07   5.53    -6.34    -0.14     3.29     7.34
gamma[8,9]      -1.68    0.10   4.07    -9.65    -3.93    -1.94     0.71
gamma[8,10]     -0.08    0.06   3.61    -7.22    -1.98    -0.27     2.00
gamma[8,11]      0.64    1.17   4.96    -9.71    -2.85     1.06     4.67
gamma[9,1]      -2.62    1.31   5.09   -12.13    -5.73    -3.11     0.79
gamma[9,2]       3.10    0.87   5.20    -7.58    -0.43     3.58     6.80
gamma[9,3]       0.88    0.09   4.43    -8.44    -1.80     0.99     3.39
gamma[9,4]       1.12    0.24   4.30    -7.49    -1.49     1.06     3.65
gamma[9,5]      -1.45    1.22   4.91   -11.28    -4.82    -1.07     1.47
gamma[9,6]      -0.79    0.12   4.02    -9.45    -3.20    -0.30     1.39
gamma[9,7]      -3.20    0.72   5.00   -11.85    -7.06    -3.27     0.39
gamma[9,8]      -2.41    2.00   5.38   -12.47    -6.22    -2.10     1.39
gamma[9,9]      -0.48    1.03   4.54    -7.90    -3.98    -0.64     2.65
gamma[9,10]     -1.40    0.26   3.81    -8.87    -3.83    -1.67     1.22
gamma[9,11]     -0.08    0.16   4.39    -8.77    -2.60    -0.46     2.53
gamma[10,1]      0.97    1.53   4.96    -9.47    -2.43     1.42     4.88
gamma[10,2]     -0.26    0.38   4.59    -9.72    -3.12    -0.38     3.09
gamma[10,3]      0.00    0.36   4.60    -8.53    -3.21    -0.11     2.80
gamma[10,4]     -0.46    0.66   4.17    -7.52    -3.88    -0.68     2.40
gamma[10,5]     -0.25    0.42   4.54    -8.87    -3.28    -0.80     2.91
gamma[10,6]      0.74    0.26   4.07    -7.47    -1.64     0.85     3.02
gamma[10,7]     -0.53    0.68   4.57    -9.00    -3.82    -1.02     2.44
gamma[10,8]     -0.98    2.26   5.27    -9.19    -5.54    -0.72     2.84
gamma[10,9]      2.84    1.00   4.41    -6.46    -0.21     3.38     5.86
gamma[10,10]     0.27    0.09   3.62    -7.40    -1.79     0.57     2.06
gamma[10,11]    -0.13    0.14   4.64    -9.79    -2.85     0.06     2.54
gamma[11,1]     -0.45    0.92   4.76    -8.86    -4.00    -0.84     2.73
gamma[11,2]     -0.01    1.31   4.96   -10.51    -3.40     0.61     3.85
gamma[11,3]      0.36    0.94   4.77    -9.69    -2.87     1.08     3.95
gamma[11,4]     -1.22    0.58   4.65   -10.81    -3.99    -1.60     1.73
gamma[11,5]      0.91    0.24   4.45    -8.24    -1.79     0.79     3.81
gamma[11,6]      0.37    1.60   4.93    -7.91    -3.70     0.50     3.85
gamma[11,7]      1.24    1.11   4.74    -8.53    -2.02     1.68     4.86
gamma[11,8]     -3.21    0.92   4.44   -10.81    -6.44    -3.43    -0.42
gamma[11,9]      0.15    2.00   5.19    -8.64    -4.25     0.25     3.94
gamma[11,10]    -1.89    0.16   3.74    -9.38    -4.01    -1.87     0.32
gamma[11,11]     0.11    0.22   4.44    -9.16    -2.58     0.34     2.91
gamma[12,1]     -0.20    0.40   4.66    -9.01    -3.39    -0.38     2.77
gamma[12,2]      0.60    1.57   5.07    -9.90    -3.00     1.02     4.42
gamma[12,3]     -0.91    0.29   4.56   -10.73    -3.75    -0.38     1.88
gamma[12,4]     -1.00    1.06   4.43    -8.58    -4.04    -1.55     2.01
gamma[12,5]      3.15    0.29   4.45    -5.74     0.58     2.94     6.00
gamma[12,6]      3.44    1.09   4.27    -5.43     0.43     3.94     6.77
gamma[12,7]      1.37    1.41   4.92    -9.07    -2.01     1.69     5.52
gamma[12,8]     -1.69    1.12   4.56    -9.82    -4.97    -2.09     1.45
gamma[12,9]      0.23    0.45   4.14    -8.45    -2.45     0.57     3.18
gamma[12,10]     0.22    0.19   3.67    -7.16    -2.19     0.16     2.52
gamma[12,11]     1.81    2.25   5.75    -9.75    -2.29     1.73     6.83
gamma[13,1]     -0.92    1.49   5.15    -9.03    -4.89    -1.43     2.76
gamma[13,2]      1.37    2.54   5.80    -9.60    -2.90     1.11     6.48
gamma[13,3]      1.25    0.17   4.31    -7.52    -1.17     0.99     4.02
gamma[13,4]      0.83    0.32   3.75    -6.74    -1.71     0.79     3.08
gamma[13,5]     -0.10    0.84   4.49    -9.49    -3.08     0.37     3.03
gamma[13,6]     -1.56    1.04   3.63    -9.14    -4.09    -1.16     1.23
gamma[13,7]      0.73    0.40   4.23    -7.34    -2.48     0.81     3.45
gamma[13,8]      0.12    1.46   4.22    -6.28    -3.33     0.00     3.11
gamma[13,9]     -0.35    0.49   4.37    -8.09    -3.70    -0.81     2.98
gamma[13,10]     1.68    1.17   3.69    -5.32    -1.01     1.77     4.32
gamma[13,11]    -0.48    0.09   4.33    -9.29    -2.88    -0.96     2.22
sig[1]         152.74   10.24  61.89    45.92   105.66   153.08   198.42
sig[2]         626.94  109.92 313.71   159.05   373.84   563.45   936.86
sig[3]         507.48   41.59 202.18   158.42   350.54   505.70   675.11
lp__         -2133.54    3.26  45.84 -2232.79 -2162.42 -2129.30 -2101.79
                97.5% n_eff Rhat
gamma[1,1]       9.62   781 1.01
gamma[1,2]      10.03  1094 1.01
gamma[1,3]       6.27   207 1.03
gamma[1,4]      11.04   180 1.02
gamma[1,5]       9.44    19 1.07
gamma[1,6]       9.33   742 1.00
gamma[1,7]       6.03   457 1.01
gamma[1,8]       6.81    53 1.07
gamma[1,9]       4.30    24 1.06
gamma[1,10]      6.64  1185 1.01
gamma[1,11]     16.80   512 1.01
gamma[2,1]       8.25    52 1.04
gamma[2,2]      10.27   102 1.03
gamma[2,3]       8.20    32 1.05
gamma[2,4]      10.85    69 1.04
gamma[2,5]       9.01    55 1.07
gamma[2,6]       8.83    26 1.05
gamma[2,7]       6.06    14 1.10
gamma[2,8]       7.37     8 1.20
gamma[2,9]       6.41     6 1.26
gamma[2,10]      5.98    24 1.06
gamma[2,11]     10.12    12 1.14
gamma[3,1]       8.82   306 1.02
gamma[3,2]       9.95    93 1.06
gamma[3,3]       8.69   238 1.02
gamma[3,4]       9.66    71 1.07
gamma[3,5]       6.82   212 1.02
gamma[3,6]       7.45   334 1.01
gamma[3,7]       8.42   209 1.03
gamma[3,8]       7.67    53 1.06
gamma[3,9]       7.23   203 1.02
gamma[3,10]      8.90    14 1.10
gamma[3,11]      9.08     6 1.35
gamma[4,1]       8.52   111 1.03
gamma[4,2]      10.13    24 1.06
gamma[4,3]       8.73    86 1.03
gamma[4,4]       9.34    18 1.10
gamma[4,5]       7.50   436 1.01
gamma[4,6]       7.35   135 1.02
gamma[4,7]       7.75  4435 1.00
gamma[4,8]       8.89    19 1.07
gamma[4,9]       6.29   317 1.01
gamma[4,10]      8.86   643 1.01
gamma[4,11]      9.75  6751 1.00
gamma[5,1]      11.73    17 1.13
gamma[5,2]       9.11  1483 1.00
gamma[5,3]       9.79  4021 1.00
gamma[5,4]       7.53    18 1.12
gamma[5,5]       6.79    30 1.05
gamma[5,6]       7.78    21 1.13
gamma[5,7]       8.97    80 1.03
gamma[5,8]       7.96    41 1.09
gamma[5,9]       5.62   224 1.03
gamma[5,10]     11.33   101 1.06
gamma[5,11]      9.89    61 1.03
gamma[6,1]       9.06  1753 1.00
gamma[6,2]      13.50     5 1.42
gamma[6,3]      11.02    31 1.05
gamma[6,4]      10.42   195 1.03
gamma[6,5]       8.55     6 1.24
gamma[6,6]       8.06     9 1.16
gamma[6,7]       9.48    30 1.07
gamma[6,8]       6.20   127 1.02
gamma[6,9]       8.35    18 1.11
gamma[6,10]      8.68     4 1.39
gamma[6,11]      8.68     7 1.22
gamma[7,1]       8.45    15 1.09
gamma[7,2]      11.42   231 1.03
gamma[7,3]       8.29     7 1.18
gamma[7,4]      12.53     8 1.19
gamma[7,5]       8.59    68 1.03
gamma[7,6]       6.75  2453 1.01
gamma[7,7]       7.39  1846 1.00
gamma[7,8]      10.61    10 1.13
gamma[7,9]       8.30   326 1.02
gamma[7,10]      3.76   146 1.03
gamma[7,11]      8.99   184 1.02
gamma[8,1]      10.53   526 1.02
gamma[8,2]       8.39  1741 1.00
gamma[8,3]       8.34   194 1.02
gamma[8,4]      10.73    11 1.23
gamma[8,5]      10.23    21 1.06
gamma[8,6]       8.89  3745 1.00
gamma[8,7]      11.06     9 1.24
gamma[8,8]      13.72     7 1.35
gamma[8,9]       6.93  1503 1.01
gamma[8,10]      7.52  4173 1.00
gamma[8,11]      8.70    18 1.10
gamma[9,1]       7.81    15 1.10
gamma[9,2]      12.20    36 1.06
gamma[9,3]       9.60  2198 1.00
gamma[9,4]       9.88   322 1.01
gamma[9,5]       7.52    16 1.10
gamma[9,6]       7.13  1071 1.01
gamma[9,7]       6.77    48 1.10
gamma[9,8]       7.77     7 1.31
gamma[9,9]       8.74    19 1.14
gamma[9,10]      6.35   214 1.03
gamma[9,11]      9.12   728 1.01
gamma[10,1]      9.06    11 1.11
gamma[10,2]      8.70   143 1.03
gamma[10,3]      9.60   160 1.02
gamma[10,4]      8.26    40 1.08
gamma[10,5]      8.92   116 1.03
gamma[10,6]      9.03   244 1.03
gamma[10,7]      9.15    45 1.05
gamma[10,8]      9.12     5 1.29
gamma[10,9]     10.62    19 1.08
gamma[10,10]     7.67  1662 1.01
gamma[10,11]     8.96  1039 1.01
gamma[11,1]      9.29    27 1.07
gamma[11,2]      8.15    14 1.09
gamma[11,3]      8.60    26 1.06
gamma[11,4]      8.50    64 1.04
gamma[11,5]      9.78   341 1.02
gamma[11,6]      9.97     9 1.21
gamma[11,7]      9.13    18 1.08
gamma[11,8]      6.15    23 1.07
gamma[11,9]     10.12     7 1.21
gamma[11,10]     5.82   541 1.01
gamma[11,11]     8.95   411 1.02
gamma[12,1]      9.39   137 1.02
gamma[12,2]      8.43    10 1.14
gamma[12,3]      7.88   256 1.02
gamma[12,4]      8.14    17 1.08
gamma[12,5]     11.88   229 1.03
gamma[12,6]     10.46    15 1.10
gamma[12,7]      9.01    12 1.12
gamma[12,8]      7.62    17 1.09
gamma[12,9]      7.86    84 1.03
gamma[12,10]     7.72   358 1.02
gamma[12,11]    10.21     7 1.23
gamma[13,1]      9.54    12 1.11
gamma[13,2]     11.58     5 1.27
gamma[13,3]     10.06   655 1.01
gamma[13,4]      8.44   138 1.04
gamma[13,5]      7.69    28 1.08
gamma[13,6]      4.27    12 1.12
gamma[13,7]      9.47   114 1.04
gamma[13,8]      8.55     8 1.20
gamma[13,9]      8.45    80 1.06
gamma[13,10]     8.90    10 1.14
gamma[13,11]     8.58  2395 1.00
sig[1]         273.23    37 1.07
sig[2]        1152.06     8 1.19
sig[3]         880.64    24 1.10
lp__         -2051.27   198 1.02

Samples were drawn using NUTS(diag_e) at Wed Aug 18 17:32:03 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/rainy_spab_50_model_inter_full_Full_valley.rda"
[1] "MCMC done!!"
