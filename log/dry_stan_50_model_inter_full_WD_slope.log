── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  dry season"
[1] "Use full"
[1] "Habitat = slope"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: except for WD"
[1] "sp number in seedling data: 69"
[1] "sp number in trait data: 69"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.4 as a scaling parameter for the distance effect"
[1] "n_sp = J =69"
[1] "n_para = K = 11"
[1] "n_plot = S = 93"
[1] "n_census = T = 10"
[1] "n_tag = M = 1993"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.00315 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 31.5 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.003508 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 35.08 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.004283 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 42.83 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.007241 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 72.41 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 9698.1 seconds (Warm-up)
Chain 3:                9507.66 seconds (Sampling)
Chain 3:                19205.8 seconds (Total)
Chain 3: 
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 9993.58 seconds (Warm-up)
Chain 2:                9550.48 seconds (Sampling)
Chain 2:                19544.1 seconds (Total)
Chain 2: 
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 10066.2 seconds (Warm-up)
Chain 1:                9488.94 seconds (Sampling)
Chain 1:                19555.2 seconds (Total)
Chain 1: 
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 15234.7 seconds (Warm-up)
Chain 4:                9063.22 seconds (Sampling)
Chain 4:                24298 seconds (Total)
Chain 4: 
Warning messages:
1: There were 2 chains where the estimated Bayesian Fraction of Missing Information was low. See
http://mc-stan.org/misc/warnings.html#bfmi-low 
2: Examine the pairs() plot to diagnose sampling problems
 
3: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
4: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean     sd     2.5%      25%      50%      75%
gamma[1,1]       6.04    0.06   1.71     2.83     4.87     5.97     7.14
gamma[1,2]      -2.11    0.04   2.41    -6.90    -3.66    -2.04    -0.46
gamma[1,3]      -0.08    0.02   1.04    -2.16    -0.76    -0.07     0.62
gamma[1,4]       1.36    0.02   0.52     0.48     1.00     1.32     1.67
gamma[1,5]      -0.02    0.01   0.44    -0.89    -0.31    -0.02     0.26
gamma[1,6]      -1.54    0.03   1.32    -4.17    -2.41    -1.54    -0.67
gamma[1,7]      -3.01    0.04   2.25    -7.44    -4.54    -3.01    -1.47
gamma[1,8]      -0.18    0.01   0.73    -1.69    -0.65    -0.16     0.31
gamma[1,9]       0.41    0.01   0.31    -0.18     0.20     0.41     0.61
gamma[1,10]     -0.34    0.00   0.28    -0.91    -0.53    -0.34    -0.15
gamma[1,11]      1.59    0.02   0.45     0.78     1.29     1.57     1.85
gamma[2,1]      -0.89    0.04   2.06    -4.93    -2.27    -0.90     0.49
gamma[2,2]      -1.80    0.05   3.19    -8.15    -3.97    -1.77     0.36
gamma[2,3]       0.93    0.03   1.42    -1.82    -0.02     0.93     1.84
gamma[2,4]      -1.70    0.02   0.71    -3.19    -2.13    -1.66    -1.22
gamma[2,5]       0.64    0.01   0.72    -0.80     0.18     0.64     1.09
gamma[2,6]      -0.21    0.03   1.69    -3.51    -1.34    -0.19     0.93
gamma[2,7]       1.54    0.05   2.95    -4.28    -0.45     1.58     3.57
gamma[2,8]      -0.76    0.02   1.02    -2.81    -1.43    -0.72    -0.07
gamma[2,9]      -0.90    0.01   0.46    -1.81    -1.20    -0.91    -0.60
gamma[2,10]      0.01    0.01   0.47    -0.92    -0.30     0.00     0.32
gamma[2,11]     -0.47    0.01   0.59    -1.65    -0.85    -0.46    -0.07
gamma[3,1]       0.85    0.03   1.69    -2.57    -0.23     0.87     1.96
gamma[3,2]       3.19    0.04   2.72    -2.22     1.39     3.22     5.08
gamma[3,3]      -0.55    0.02   1.07    -2.70    -1.24    -0.53     0.16
gamma[3,4]       0.88    0.01   0.63    -0.33     0.48     0.87     1.27
gamma[3,5]      -0.96    0.01   0.62    -2.20    -1.35    -0.95    -0.55
gamma[3,6]      -0.89    0.02   1.41    -3.63    -1.85    -0.89     0.06
gamma[3,7]      -0.56    0.04   2.61    -5.59    -2.32    -0.57     1.15
gamma[3,8]      -0.51    0.01   0.75    -2.02    -0.99    -0.51    -0.01
gamma[3,9]       0.47    0.01   0.41    -0.38     0.21     0.48     0.73
gamma[3,10]      0.16    0.01   0.37    -0.59    -0.09     0.16     0.40
gamma[3,11]      0.37    0.01   0.55    -0.69     0.01     0.37     0.72
gamma[4,1]      -1.19    0.03   1.89    -4.88    -2.47    -1.20     0.05
gamma[4,2]      -1.83    0.04   2.81    -7.42    -3.68    -1.85     0.02
gamma[4,3]      -0.82    0.03   1.35    -3.53    -1.71    -0.82     0.08
gamma[4,4]       0.38    0.01   0.65    -0.86    -0.04     0.36     0.78
gamma[4,5]      -0.30    0.01   0.62    -1.56    -0.70    -0.29     0.10
gamma[4,6]       0.19    0.03   1.54    -2.78    -0.83     0.20     1.21
gamma[4,7]       1.09    0.04   2.51    -3.84    -0.59     1.08     2.76
gamma[4,8]      -0.40    0.02   1.01    -2.43    -1.05    -0.37     0.26
gamma[4,9]       0.00    0.01   0.48    -1.00    -0.30     0.01     0.32
gamma[4,10]     -0.15    0.01   0.41    -0.97    -0.43    -0.15     0.12
gamma[4,11]      0.17    0.01   0.55    -0.92    -0.19     0.17     0.54
gamma[5,1]      -0.49    0.03   1.57    -3.54    -1.55    -0.50     0.55
gamma[5,2]      -2.97    0.04   2.46    -7.73    -4.67    -2.98    -1.33
gamma[5,3]       0.42    0.02   1.01    -1.57    -0.23     0.42     1.08
gamma[5,4]       0.16    0.01   0.45    -0.73    -0.12     0.15     0.43
gamma[5,5]      -0.78    0.01   0.53    -1.90    -1.09    -0.75    -0.43
gamma[5,6]      -1.25    0.03   1.35    -3.92    -2.17    -1.24    -0.34
gamma[5,7]      -2.29    0.04   2.30    -6.84    -3.83    -2.25    -0.75
gamma[5,8]      -0.20    0.01   0.74    -1.69    -0.69    -0.19     0.29
gamma[5,9]      -0.02    0.00   0.30    -0.61    -0.21    -0.02     0.17
gamma[5,10]     -0.07    0.00   0.32    -0.71    -0.28    -0.07     0.14
gamma[5,11]     -0.53    0.01   0.44    -1.44    -0.81    -0.53    -0.25
gamma[6,1]      -4.47    0.03   2.11    -8.54    -5.92    -4.46    -3.02
gamma[6,2]      -1.84    0.04   3.14    -7.96    -3.94    -1.90     0.26
gamma[6,3]      -2.55    0.02   1.33    -5.19    -3.41    -2.57    -1.69
gamma[6,4]       1.15    0.01   0.73    -0.32     0.67     1.16     1.62
gamma[6,5]       0.12    0.02   0.81    -1.33    -0.43     0.07     0.61
gamma[6,6]       3.25    0.03   1.75    -0.07     2.07     3.22     4.43
gamma[6,7]       4.45    0.04   2.86    -1.04     2.53     4.43     6.43
gamma[6,8]       0.29    0.02   0.98    -1.68    -0.35     0.28     0.95
gamma[6,9]       0.09    0.01   0.52    -0.97    -0.23     0.09     0.42
gamma[6,10]      0.49    0.01   0.48    -0.45     0.18     0.49     0.80
gamma[6,11]     -0.69    0.02   0.68    -2.10    -1.12    -0.67    -0.25
gamma[7,1]       0.90    0.05   2.99    -5.13    -1.09     0.95     2.94
gamma[7,2]      -0.87    0.04   3.64    -7.98    -3.31    -0.84     1.57
gamma[7,3]       1.35    0.04   2.38    -3.47    -0.21     1.40     2.94
gamma[7,4]      -1.76    0.02   1.40    -4.56    -2.68    -1.75    -0.86
gamma[7,5]       0.62    0.03   1.48    -2.52    -0.31     0.70     1.63
gamma[7,6]       1.90    0.04   2.40    -2.78     0.28     1.86     3.51
gamma[7,7]       1.30    0.05   3.48    -5.63    -0.99     1.37     3.64
gamma[7,8]       1.13    0.03   1.80    -2.38    -0.08     1.12     2.32
gamma[7,9]       0.32    0.02   1.05    -1.62    -0.37     0.26     0.96
gamma[7,10]     -0.89    0.02   0.95    -2.84    -1.52    -0.88    -0.26
gamma[7,11]     -0.10    0.02   1.22    -2.49    -0.89    -0.11     0.69
gamma[8,1]       2.26    0.04   2.61    -2.88     0.57     2.29     4.01
gamma[8,2]      -0.35    0.04   3.26    -6.63    -2.56    -0.33     1.89
gamma[8,3]       2.00    0.04   2.12    -2.30     0.61     2.03     3.42
gamma[8,4]      -2.14    0.03   1.46    -4.93    -3.12    -2.16    -1.22
gamma[8,5]       1.43    0.04   1.57    -1.90     0.44     1.52     2.50
gamma[8,6]       0.44    0.03   2.00    -3.44    -0.90     0.44     1.75
gamma[8,7]       1.14    0.04   3.03    -4.72    -0.92     1.13     3.21
gamma[8,8]       0.57    0.03   1.66    -2.67    -0.52     0.58     1.67
gamma[8,9]       0.32    0.02   1.07    -1.73    -0.38     0.28     1.00
gamma[8,10]     -0.67    0.02   0.99    -2.63    -1.32    -0.66    -0.03
gamma[8,11]     -0.09    0.02   1.33    -2.73    -0.94    -0.09     0.77
gamma[9,1]      -2.21    0.03   2.05    -6.17    -3.61    -2.20    -0.83
gamma[9,2]      -0.46    0.04   3.34    -6.98    -2.73    -0.50     1.81
gamma[9,3]      -0.71    0.02   1.17    -3.05    -1.47    -0.72     0.07
gamma[9,4]      -0.25    0.01   0.56    -1.35    -0.61    -0.26     0.11
gamma[9,5]      -0.25    0.01   0.63    -1.47    -0.66    -0.26     0.15
gamma[9,6]       1.04    0.03   1.76    -2.45    -0.14     1.05     2.22
gamma[9,7]       1.94    0.04   3.13    -4.30    -0.18     1.94     4.04
gamma[9,8]      -0.17    0.02   0.80    -1.82    -0.69    -0.15     0.37
gamma[9,9]      -0.51    0.01   0.42    -1.36    -0.78    -0.50    -0.23
gamma[9,10]     -0.57    0.01   0.42    -1.43    -0.83    -0.55    -0.28
gamma[9,11]      0.09    0.01   0.49    -0.86    -0.24     0.08     0.39
gamma[10,1]      0.19    0.03   1.48    -2.71    -0.81     0.20     1.19
gamma[10,2]     -0.98    0.03   2.22    -5.31    -2.45    -0.98     0.50
gamma[10,3]      0.01    0.02   0.97    -1.95    -0.61     0.04     0.66
gamma[10,4]     -0.76    0.01   0.53    -1.81    -1.10    -0.76    -0.40
gamma[10,5]      0.90    0.01   0.58    -0.22     0.52     0.90     1.28
gamma[10,6]      0.65    0.02   1.14    -1.58    -0.09     0.66     1.42
gamma[10,7]      3.03    0.03   2.01    -0.83     1.65     3.00     4.37
gamma[10,8]     -0.68    0.01   0.69    -2.06    -1.13    -0.67    -0.22
gamma[10,9]      0.33    0.01   0.38    -0.38     0.08     0.32     0.57
gamma[10,10]     0.22    0.01   0.37    -0.51    -0.03     0.22     0.47
gamma[10,11]    -0.43    0.01   0.45    -1.32    -0.72    -0.41    -0.13
gamma[11,1]      2.02    0.04   2.14    -2.17     0.56     2.00     3.43
gamma[11,2]     -1.43    0.05   3.21    -7.80    -3.53    -1.37     0.78
gamma[11,3]      1.35    0.03   1.50    -1.50     0.35     1.33     2.34
gamma[11,4]     -0.61    0.01   0.85    -2.27    -1.16    -0.63    -0.08
gamma[11,5]      0.99    0.02   0.81    -0.58     0.47     0.98     1.51
gamma[11,6]     -1.34    0.03   1.68    -4.61    -2.47    -1.33    -0.20
gamma[11,7]      0.04    0.05   2.95    -5.63    -1.94     0.02     2.00
gamma[11,8]     -0.75    0.02   1.17    -3.10    -1.50    -0.72     0.05
gamma[11,9]      0.06    0.01   0.62    -1.25    -0.32     0.08     0.47
gamma[11,10]    -0.04    0.01   0.52    -1.05    -0.40    -0.05     0.29
gamma[11,11]    -0.66    0.01   0.79    -2.22    -1.17    -0.66    -0.15
gamma[12,1]     -0.11    0.03   1.71    -3.47    -1.24    -0.11     1.05
gamma[12,2]     -1.22    0.04   2.71    -6.58    -3.01    -1.20     0.59
gamma[12,3]      0.33    0.02   1.18    -2.01    -0.44     0.33     1.10
gamma[12,4]     -1.06    0.01   0.62    -2.32    -1.46    -1.05    -0.65
gamma[12,5]      1.05    0.01   0.63    -0.09     0.64     1.02     1.42
gamma[12,6]      1.12    0.02   1.33    -1.48     0.23     1.12     2.01
gamma[12,7]      2.13    0.04   2.44    -2.65     0.48     2.12     3.77
gamma[12,8]      0.04    0.01   0.82    -1.62    -0.49     0.05     0.58
gamma[12,9]     -0.37    0.01   0.39    -1.17    -0.62    -0.37    -0.12
gamma[12,10]     0.19    0.01   0.39    -0.55    -0.07     0.18     0.44
gamma[12,11]    -0.79    0.01   0.57    -1.98    -1.14    -0.78    -0.41
sig[1]           0.76    0.02   0.23     0.42     0.61     0.73     0.87
sig[2]           1.00    0.03   0.43     0.48     0.72     0.91     1.16
sig[3]           2.34    0.07   0.65     1.52     1.95     2.23     2.58
lp__         -2930.51   12.48 128.35 -3145.38 -3015.57 -2942.94 -2858.97
                97.5% n_eff Rhat
gamma[1,1]       9.59   931 1.00
gamma[1,2]       2.48  3393 1.00
gamma[1,3]       1.97  2582 1.00
gamma[1,4]       2.52   732 1.00
gamma[1,5]       0.87  4655 1.00
gamma[1,6]       1.07  2732 1.00
gamma[1,7]       1.38  3971 1.00
gamma[1,8]       1.19  3071 1.00
gamma[1,9]       1.06  3613 1.00
gamma[1,10]      0.19  4684 1.00
gamma[1,11]      2.56   418 1.01
gamma[2,1]       3.11  3428 1.00
gamma[2,2]       4.38  4706 1.00
gamma[2,3]       3.77  3177 1.00
gamma[2,4]      -0.37  2089 1.00
gamma[2,5]       2.07  2660 1.00
gamma[2,6]       3.11  2919 1.00
gamma[2,7]       7.14  3749 1.00
gamma[2,8]       1.16  3632 1.00
gamma[2,9]       0.01  2384 1.00
gamma[2,10]      0.92  2958 1.00
gamma[2,11]      0.63  4058 1.00
gamma[3,1]       4.16  3314 1.00
gamma[3,2]       8.36  4048 1.00
gamma[3,3]       1.51  4099 1.00
gamma[3,4]       2.16  3927 1.00
gamma[3,5]       0.25  2927 1.00
gamma[3,6]       1.87  3527 1.00
gamma[3,7]       4.69  3596 1.00
gamma[3,8]       1.01  4872 1.00
gamma[3,9]       1.24  2644 1.00
gamma[3,10]      0.89  3349 1.00
gamma[3,11]      1.48  4511 1.00
gamma[4,1]       2.51  3032 1.00
gamma[4,2]       3.66  4299 1.00
gamma[4,3]       1.77  2645 1.00
gamma[4,4]       1.73  3221 1.00
gamma[4,5]       0.91  2987 1.00
gamma[4,6]       3.21  2389 1.00
gamma[4,7]       6.02  3902 1.00
gamma[4,8]       1.53  2341 1.00
gamma[4,9]       0.89  3768 1.00
gamma[4,10]      0.65  4409 1.00
gamma[4,11]      1.26  3519 1.00
gamma[5,1]       2.61  3233 1.00
gamma[5,2]       1.85  4058 1.00
gamma[5,3]       2.42  3350 1.00
gamma[5,4]       1.05  3723 1.00
gamma[5,5]       0.17  1311 1.00
gamma[5,6]       1.42  2174 1.00
gamma[5,7]       2.15  3013 1.00
gamma[5,8]       1.25  3306 1.00
gamma[5,9]       0.56  4140 1.00
gamma[5,10]      0.56  4956 1.00
gamma[5,11]      0.32  3104 1.00
gamma[6,1]      -0.36  3782 1.00
gamma[6,2]       4.27  5925 1.00
gamma[6,3]       0.06  3344 1.00
gamma[6,4]       2.60  3640 1.00
gamma[6,5]       1.86  2029 1.00
gamma[6,6]       6.77  4075 1.00
gamma[6,7]      10.10  4908 1.00
gamma[6,8]       2.23  3696 1.00
gamma[6,9]       1.09  3064 1.00
gamma[6,10]      1.42  3663 1.00
gamma[6,11]      0.61  1558 1.00
gamma[7,1]       6.67  3854 1.00
gamma[7,2]       6.32  6777 1.00
gamma[7,3]       5.97  3427 1.00
gamma[7,4]       1.14  3352 1.00
gamma[7,5]       3.30  1977 1.00
gamma[7,6]       6.69  3984 1.00
gamma[7,7]       7.93  5736 1.00
gamma[7,8]       4.74  3487 1.00
gamma[7,9]       2.51  2257 1.00
gamma[7,10]      0.92  2427 1.00
gamma[7,11]      2.34  3349 1.00
gamma[8,1]       7.22  3410 1.00
gamma[8,2]       5.98  5272 1.00
gamma[8,3]       6.08  3275 1.00
gamma[8,4]       0.83  3008 1.00
gamma[8,5]       4.30  1982 1.00
gamma[8,6]       4.40  3347 1.00
gamma[8,7]       7.04  4623 1.00
gamma[8,8]       3.85  3319 1.00
gamma[8,9]       2.54  2270 1.00
gamma[8,10]      1.27  2487 1.00
gamma[8,11]      2.58  2983 1.00
gamma[9,1]       1.82  4241 1.00
gamma[9,2]       6.14  5881 1.00
gamma[9,3]       1.58  3784 1.00
gamma[9,4]       0.86  5134 1.00
gamma[9,5]       1.00  4174 1.00
gamma[9,6]       4.46  4035 1.00
gamma[9,7]       8.07  4868 1.00
gamma[9,8]       1.32  2716 1.00
gamma[9,9]       0.30  3963 1.00
gamma[9,10]      0.24  3967 1.00
gamma[9,11]      1.06  5769 1.00
gamma[10,1]      3.06  3466 1.00
gamma[10,2]      3.32  4338 1.00
gamma[10,3]      1.82  3738 1.00
gamma[10,4]      0.26  4225 1.00
gamma[10,5]      2.05  2814 1.00
gamma[10,6]      2.87  3585 1.00
gamma[10,7]      7.05  4470 1.00
gamma[10,8]      0.65  3939 1.00
gamma[10,9]      1.08  2553 1.00
gamma[10,10]     0.95  2813 1.00
gamma[10,11]     0.43  4155 1.00
gamma[11,1]      6.30  2731 1.00
gamma[11,2]      4.65  4188 1.00
gamma[11,3]      4.34  3365 1.00
gamma[11,4]      1.11  3922 1.00
gamma[11,5]      2.63  2667 1.00
gamma[11,6]      1.94  2728 1.00
gamma[11,7]      5.89  3612 1.00
gamma[11,8]      1.44  2891 1.00
gamma[11,9]      1.23  3388 1.00
gamma[11,10]     1.00  3772 1.00
gamma[11,11]     0.87  3662 1.00
gamma[12,1]      3.25  3264 1.00
gamma[12,2]      4.09  4585 1.00
gamma[12,3]      2.71  3101 1.00
gamma[12,4]      0.14  1900 1.00
gamma[12,5]      2.44  3190 1.00
gamma[12,6]      3.78  3169 1.00
gamma[12,7]      6.93  3792 1.00
gamma[12,8]      1.64  3653 1.00
gamma[12,9]      0.39  3411 1.00
gamma[12,10]     1.00  2965 1.00
gamma[12,11]     0.27  3362 1.00
sig[1]           1.30   157 1.02
sig[2]           2.10   184 1.02
sig[3]           3.86    84 1.05
lp__         -2647.52   106 1.04

Samples were drawn using NUTS(diag_e) at Mon Aug 23 15:52:16 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/dry_spab_50_model_inter_full_WD_slope.rda"
[1] "MCMC done!!"
