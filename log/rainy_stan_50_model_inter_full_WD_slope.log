── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  rainy season"
[1] "Use full"
[1] "Habitat = slope"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: except for WD"
[1] "sp number in seedling data: 68"
[1] "sp number in trait data: 68"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.3 as a scaling parameter for the distance effect"
[1] "n_sp = J =68"
[1] "n_para = K = 11"
[1] "n_plot = S = 93"
[1] "n_census = T = 10"
[1] "n_tag = M = 1915"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 2: 
Chain 2: Gradient evaluation took 0.003369 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 33.69 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 1: 
Chain 1: Gradient evaluation took 0.007025 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 70.25 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 3: 
Chain 3: Gradient evaluation took 0.007101 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 71.01 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 4: 
Chain 4: Gradient evaluation took 0.007282 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 72.82 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 48238.9 seconds (Warm-up)
Chain 1:                19730 seconds (Sampling)
Chain 1:                67968.9 seconds (Total)
Chain 1: 
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 44199.4 seconds (Warm-up)
Chain 2:                36787.8 seconds (Sampling)
Chain 2:                80987.2 seconds (Total)
Chain 2: 
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 45265.4 seconds (Warm-up)
Chain 3:                36664.4 seconds (Sampling)
Chain 3:                81929.8 seconds (Total)
Chain 3: 
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 56748.7 seconds (Warm-up)
Chain 4:                32978 seconds (Sampling)
Chain 4:                89726.7 seconds (Total)
Chain 4: 
Warning messages:
1: There were 2 divergent transitions after warmup. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them. 
2: Examine the pairs() plot to diagnose sampling problems
 
3: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat 
4: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
5: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean     sd     2.5%      25%      50%      75%
gamma[1,1]       0.34    0.05   5.21    -9.88    -3.25     0.35     3.86
gamma[1,2]       0.11    0.04   4.90    -9.54    -3.20     0.10     3.43
gamma[1,3]      -1.94    0.06   5.05   -11.66    -5.40    -1.95     1.44
gamma[1,4]       3.59    0.05   4.59    -5.31     0.45     3.59     6.72
gamma[1,5]       2.08    0.05   5.03    -7.71    -1.29     2.04     5.52
gamma[1,6]       0.05    0.05   5.03    -9.87    -3.37     0.06     3.47
gamma[1,7]       0.33    0.05   4.93    -9.49    -2.92     0.34     3.63
gamma[1,8]      -2.35    0.05   4.41   -10.93    -5.32    -2.35     0.58
gamma[1,9]      -1.73    0.05   3.88    -9.34    -4.29    -1.73     0.88
gamma[1,10]      0.06    0.05   4.36    -8.63    -2.85     0.13     3.05
gamma[1,11]      3.32    0.07   5.12    -6.58    -0.24     3.38     6.87
gamma[2,1]      -0.23    0.04   4.92    -9.95    -3.51    -0.26     2.98
gamma[2,2]       0.36    0.05   4.89    -9.15    -2.91     0.34     3.63
gamma[2,3]      -0.84    0.05   4.97   -10.63    -4.15    -0.81     2.43
gamma[2,4]       2.71    0.05   4.81    -6.59    -0.60     2.80     5.97
gamma[2,5]       1.66    0.06   5.06    -8.37    -1.76     1.69     5.16
gamma[2,6]       0.06    0.06   4.55    -8.69    -3.05     0.08     3.15
gamma[2,7]      -1.27    0.06   4.86   -10.78    -4.58    -1.32     2.05
gamma[2,8]       0.31    0.04   4.36    -8.21    -2.65     0.29     3.28
gamma[2,9]       2.83    0.05   4.30    -5.54    -0.11     2.79     5.68
gamma[2,10]     -1.34    0.04   4.38   -10.02    -4.21    -1.35     1.59
gamma[2,11]      0.44    0.04   4.96    -9.38    -2.80     0.42     3.73
gamma[3,1]      -0.19    0.04   4.98    -9.77    -3.55    -0.16     3.20
gamma[3,2]       0.16    0.05   4.83    -9.31    -3.08     0.15     3.39
gamma[3,3]      -0.05    0.06   4.97    -9.95    -3.42    -0.10     3.33
gamma[3,4]       1.22    0.06   4.58    -7.70    -1.99     1.23     4.32
gamma[3,5]       0.30    0.05   4.84    -9.04    -3.02     0.23     3.53
gamma[3,6]       2.92    0.07   4.61    -6.24    -0.18     2.97     6.10
gamma[3,7]      -1.97    0.05   4.77   -11.20    -5.18    -2.00     1.20
gamma[3,8]      -0.63    0.04   4.26    -8.90    -3.52    -0.64     2.26
gamma[3,9]      -1.90    0.04   4.22   -10.13    -4.71    -1.85     0.90
gamma[3,10]     -0.83    0.08   4.62   -10.19    -3.85    -0.79     2.36
gamma[3,11]      0.17    0.05   4.93    -9.56    -3.13     0.11     3.49
gamma[4,1]       0.90    0.04   4.81    -8.62    -2.24     0.90     4.12
gamma[4,2]       0.00    0.05   4.92    -9.66    -3.29    -0.05     3.32
gamma[4,3]       0.12    0.05   4.94    -9.49    -3.24     0.14     3.52
gamma[4,4]       1.48    0.05   4.59    -7.55    -1.61     1.50     4.62
gamma[4,5]      -1.06    0.05   4.84   -10.35    -4.34    -1.03     2.18
gamma[4,6]      -0.84    0.05   4.39    -9.52    -3.74    -0.84     2.17
gamma[4,7]       0.48    0.05   4.80    -8.99    -2.79     0.46     3.82
gamma[4,8]      -3.26    0.05   4.28   -11.72    -6.09    -3.23    -0.46
gamma[4,9]      -1.32    0.05   4.11    -9.54    -4.06    -1.30     1.47
gamma[4,10]     -1.21    0.04   4.33    -9.88    -4.06    -1.19     1.69
gamma[4,11]     -0.21    0.08   5.05   -10.25    -3.61    -0.21     3.12
gamma[5,1]      -0.56    0.04   4.91   -10.13    -3.94    -0.60     2.73
gamma[5,2]      -0.26    0.04   4.87    -9.88    -3.50    -0.28     3.05
gamma[5,3]       0.95    0.06   4.85    -8.73    -2.29     0.92     4.18
gamma[5,4]      -1.74    0.05   4.57   -10.80    -4.78    -1.69     1.31
gamma[5,5]      -0.11    0.04   4.93    -9.82    -3.43    -0.08     3.25
gamma[5,6]       0.10    0.06   4.28    -8.20    -2.81     0.06     2.99
gamma[5,7]       0.24    0.05   4.77    -9.11    -2.96     0.27     3.45
gamma[5,8]      -0.49    0.04   4.04    -8.36    -3.20    -0.48     2.18
gamma[5,9]       0.26    0.05   3.82    -7.22    -2.28     0.23     2.77
gamma[5,10]     -0.58    0.05   4.40    -9.05    -3.60    -0.62     2.35
gamma[5,11]     -0.80    0.08   5.13   -10.67    -4.19    -0.82     2.52
gamma[6,1]      -1.21    0.04   4.95   -10.82    -4.54    -1.17     2.00
gamma[6,2]       0.29    0.06   4.99    -9.27    -3.07     0.27     3.66
gamma[6,3]       0.08    0.05   4.93    -9.81    -3.23     0.14     3.44
gamma[6,4]       0.46    0.06   4.81    -8.97    -2.78     0.41     3.71
gamma[6,5]       1.14    0.07   5.05    -8.73    -2.28     1.09     4.52
gamma[6,6]       1.12    0.05   4.52    -7.60    -1.94     1.05     4.22
gamma[6,7]      -1.56    0.05   4.90   -11.17    -4.97    -1.55     1.79
gamma[6,8]       1.96    0.06   4.46    -6.89    -1.07     1.98     4.91
gamma[6,9]       2.43    0.05   4.34    -5.94    -0.51     2.43     5.32
gamma[6,10]      0.51    0.05   4.54    -8.28    -2.56     0.45     3.62
gamma[6,11]     -0.88    0.04   4.91   -10.33    -4.23    -0.91     2.43
gamma[7,1]       0.29    0.05   5.01    -9.62    -3.10     0.30     3.69
gamma[7,2]      -0.13    0.04   4.89    -9.76    -3.47    -0.10     3.18
gamma[7,3]       0.06    0.05   4.85    -9.56    -3.18     0.05     3.35
gamma[7,4]      -1.54    0.05   4.72   -10.86    -4.69    -1.53     1.68
gamma[7,5]      -0.10    0.05   4.99    -9.96    -3.40    -0.10     3.14
gamma[7,6]       0.84    0.05   4.57    -8.17    -2.21     0.82     3.89
gamma[7,7]       0.32    0.05   4.76    -8.93    -2.97     0.34     3.55
gamma[7,8]      -2.09    0.04   4.36   -10.68    -4.98    -2.10     0.93
gamma[7,9]      -0.67    0.04   4.44    -9.19    -3.76    -0.69     2.32
gamma[7,10]      2.56    0.06   4.53    -6.07    -0.55     2.49     5.61
gamma[7,11]     -0.40    0.04   4.89    -9.98    -3.69    -0.41     2.97
gamma[8,1]      -0.93    0.04   4.87   -10.45    -4.26    -0.93     2.37
gamma[8,2]       0.17    0.05   4.56    -8.50    -2.96     0.14     3.28
gamma[8,3]       0.52    0.05   4.97    -9.31    -2.81     0.54     3.87
gamma[8,4]       2.05    0.09   4.75    -7.65    -1.05     2.09     5.21
gamma[8,5]       0.48    0.05   4.85    -8.92    -2.86     0.50     3.70
gamma[8,6]       0.36    0.05   4.36    -8.36    -2.52     0.39     3.32
gamma[8,7]      -1.86    0.06   4.54   -10.77    -4.88    -1.93     1.13
gamma[8,8]       3.22    0.05   4.24    -4.92     0.35     3.22     6.03
gamma[8,9]       0.84    0.05   4.07    -7.19    -1.89     0.86     3.58
gamma[8,10]     -2.81    0.06   4.20   -11.06    -5.68    -2.81     0.04
gamma[8,11]     -0.37    0.06   4.88    -9.99    -3.67    -0.29     2.97
gamma[9,1]      -0.43    0.10   5.03   -10.68    -3.75    -0.34     2.97
gamma[9,2]       0.11    0.04   4.94    -9.35    -3.29     0.09     3.49
gamma[9,3]       0.44    0.05   4.89    -9.30    -2.89     0.41     3.82
gamma[9,4]       0.37    0.04   4.81    -9.05    -2.92     0.38     3.63
gamma[9,5]       0.05    0.05   4.96    -9.62    -3.33     0.07     3.38
gamma[9,6]       2.54    0.07   4.74    -6.72    -0.60     2.56     5.74
gamma[9,7]      -0.87    0.04   4.92   -10.42    -4.27    -0.85     2.44
gamma[9,8]      -1.39    0.04   4.42   -10.12    -4.29    -1.42     1.47
gamma[9,9]       1.31    0.04   4.52    -7.63    -1.79     1.41     4.38
gamma[9,10]      0.38    0.04   4.67    -8.97    -2.69     0.47     3.58
gamma[9,11]      0.83    0.05   5.01    -8.84    -2.60     0.87     4.24
gamma[10,1]      0.37    0.04   5.00    -9.48    -3.02     0.44     3.86
gamma[10,2]     -0.19    0.06   4.94   -10.00    -3.49    -0.18     3.08
gamma[10,3]     -0.17    0.04   4.96   -10.06    -3.41    -0.16     3.08
gamma[10,4]     -1.06    0.04   4.57   -10.01    -4.15    -1.05     2.03
gamma[10,5]     -0.98    0.05   5.04   -10.93    -4.43    -0.94     2.43
gamma[10,6]     -2.58    0.04   4.41   -11.38    -5.54    -2.52     0.29
gamma[10,7]      2.29    0.05   4.79    -7.27    -0.89     2.38     5.45
gamma[10,8]      0.81    0.04   4.25    -7.60    -1.95     0.82     3.67
gamma[10,9]     -0.44    0.04   4.09    -8.63    -3.16    -0.40     2.35
gamma[10,10]     0.66    0.04   4.49    -8.23    -2.33     0.67     3.71
gamma[10,11]     0.84    0.05   4.89    -8.92    -2.43     0.86     4.17
gamma[11,1]     -0.20    0.05   4.94    -9.89    -3.55    -0.17     3.18
gamma[11,2]      0.24    0.04   4.83    -9.20    -2.96     0.25     3.42
gamma[11,3]     -0.01    0.05   4.91    -9.55    -3.35     0.01     3.32
gamma[11,4]     -2.06    0.07   4.73   -11.29    -5.33    -2.10     1.15
gamma[11,5]     -0.41    0.05   4.88    -9.95    -3.74    -0.37     2.92
gamma[11,6]      0.29    0.05   4.55    -8.77    -2.70     0.27     3.32
gamma[11,7]      0.48    0.05   4.81    -8.79    -2.80     0.42     3.80
gamma[11,8]      2.01    0.05   4.48    -7.07    -0.91     2.06     5.00
gamma[11,9]      1.98    0.04   4.35    -6.53    -1.00     2.01     4.96
gamma[11,10]     0.91    0.04   4.46    -7.84    -2.04     0.82     3.89
gamma[11,11]    -0.46    0.04   4.93    -9.96    -3.80    -0.53     2.94
gamma[12,1]      0.62    0.05   5.01    -9.19    -2.62     0.63     4.01
gamma[12,2]      0.02    0.04   5.02    -9.69    -3.38     0.01     3.47
gamma[12,3]      0.26    0.05   5.01    -9.55    -3.17     0.33     3.67
gamma[12,4]     -1.98    0.05   4.58   -10.71    -5.12    -2.01     1.07
gamma[12,5]     -1.15    0.05   4.95   -10.86    -4.54    -1.18     2.23
gamma[12,6]     -0.27    0.05   4.26    -8.62    -3.14    -0.29     2.51
gamma[12,7]      0.93    0.05   4.72    -8.46    -2.22     0.92     4.17
gamma[12,8]     -1.72    0.05   4.31   -10.09    -4.64    -1.63     1.19
gamma[12,9]     -3.02    0.05   3.98   -10.63    -5.71    -3.08    -0.34
gamma[12,10]     0.53    0.04   4.21    -7.74    -2.26     0.57     3.32
gamma[12,11]     0.03    0.04   4.93    -9.85    -3.20     0.01     3.30
sig[1]          88.49   15.98  81.84     0.25     4.55    85.98   141.43
sig[2]         768.34   12.02 355.75   316.34   525.79   693.03   921.98
sig[3]         679.33   10.64 256.44   328.30   499.26   628.77   804.32
lp__         -1536.37    0.82  39.59 -1615.82 -1562.44 -1535.55 -1509.31
                97.5% n_eff Rhat
gamma[1,1]      10.66 10305 1.00
gamma[1,2]       9.72 12781 1.00
gamma[1,3]       7.96  8346 1.00
gamma[1,4]      12.49  8195 1.00
gamma[1,5]      11.91  9039 1.00
gamma[1,6]       9.88 12451 1.00
gamma[1,7]       9.97  8692 1.00
gamma[1,8]       6.45  7358 1.00
gamma[1,9]       5.82  7121 1.00
gamma[1,10]      8.51  8386 1.00
gamma[1,11]     13.26  5328 1.00
gamma[2,1]       9.43 12873 1.00
gamma[2,2]       9.97 10946 1.00
gamma[2,3]       8.99  8874 1.00
gamma[2,4]      12.14  9942 1.00
gamma[2,5]      11.41  8134 1.00
gamma[2,6]       8.89  6658 1.00
gamma[2,7]       8.19  7722 1.00
gamma[2,8]       8.95 10006 1.00
gamma[2,9]      11.25  6656 1.00
gamma[2,10]      7.23 11123 1.00
gamma[2,11]     10.32 13963 1.00
gamma[3,1]       9.47 14692 1.00
gamma[3,2]       9.80 10759 1.00
gamma[3,3]       9.69  6827 1.00
gamma[3,4]      10.18  6884 1.00
gamma[3,5]       9.99 10174 1.00
gamma[3,6]      11.79  4133 1.00
gamma[3,7]       7.47  9131 1.00
gamma[3,8]       7.84 11148 1.00
gamma[3,9]       6.42 10179 1.00
gamma[3,10]      7.81  3440 1.00
gamma[3,11]      9.82 11793 1.00
gamma[4,1]      10.28 13406 1.00
gamma[4,2]       9.58  9816 1.00
gamma[4,3]       9.60  9429 1.00
gamma[4,4]      10.45 10033 1.00
gamma[4,5]       8.58 11082 1.00
gamma[4,6]       7.87  6998 1.00
gamma[4,7]       9.84  8512 1.00
gamma[4,8]       5.19  6955 1.00
gamma[4,9]       6.62  6873 1.00
gamma[4,10]      7.21  9856 1.00
gamma[4,11]      9.71  4302 1.00
gamma[5,1]       9.13 14635 1.00
gamma[5,2]       9.39 14595 1.00
gamma[5,3]      10.55  7308 1.00
gamma[5,4]       7.00  7599 1.00
gamma[5,5]       9.59 12289 1.00
gamma[5,6]       8.60  5917 1.00
gamma[5,7]       9.38 10002 1.00
gamma[5,8]       7.37  8982 1.00
gamma[5,9]       8.05  5987 1.00
gamma[5,10]      8.26  8641 1.00
gamma[5,11]      9.30  4358 1.00
gamma[6,1]       8.65 12893 1.00
gamma[6,2]       9.92  8063 1.00
gamma[6,3]       9.57 10690 1.00
gamma[6,4]      10.01  5693 1.00
gamma[6,5]      11.10  5864 1.00
gamma[6,6]       9.97  9223 1.00
gamma[6,7]       7.92  8963 1.00
gamma[6,8]      10.73  5429 1.00
gamma[6,9]      11.17  8016 1.00
gamma[6,10]      9.47  9020 1.00
gamma[6,11]      8.73 13465 1.00
gamma[7,1]       9.94 12292 1.00
gamma[7,2]       9.47 13394 1.00
gamma[7,3]       9.43  9358 1.00
gamma[7,4]       7.62  9272 1.00
gamma[7,5]       9.67 11713 1.00
gamma[7,6]       9.83 10242 1.00
gamma[7,7]       9.52 10909 1.00
gamma[7,8]       6.24 10136 1.00
gamma[7,9]       8.04 10534 1.00
gamma[7,10]     11.57  6200 1.00
gamma[7,11]      9.03 12339 1.00
gamma[8,1]       8.68 13764 1.00
gamma[8,2]       9.06  8719 1.00
gamma[8,3]      10.26 11633 1.00
gamma[8,4]      11.16  2540 1.00
gamma[8,5]       9.94  8627 1.00
gamma[8,6]       8.80  9103 1.00
gamma[8,7]       7.04  5393 1.00
gamma[8,8]      11.58  7988 1.00
gamma[8,9]       8.89  7015 1.00
gamma[8,10]      5.31  4850 1.00
gamma[8,11]      9.02  6323 1.00
gamma[9,1]       9.28  2679 1.00
gamma[9,2]       9.76 12736 1.00
gamma[9,3]       9.79 11602 1.00
gamma[9,4]       9.85 12569 1.00
gamma[9,5]       9.72 11917 1.00
gamma[9,6]      11.65  4066 1.00
gamma[9,7]       8.64 12617 1.00
gamma[9,8]       7.31 10740 1.00
gamma[9,9]      10.01 10564 1.00
gamma[9,10]      9.46 11304 1.00
gamma[9,11]     10.57  9658 1.00
gamma[10,1]      9.90 13977 1.00
gamma[10,2]      9.59  7498 1.00
gamma[10,3]      9.69 12436 1.00
gamma[10,4]      7.91 11882 1.00
gamma[10,5]      8.89  9403 1.00
gamma[10,6]      6.17 11116 1.00
gamma[10,7]     11.62 11068 1.00
gamma[10,8]      8.95  9919 1.00
gamma[10,9]      7.52 11169 1.00
gamma[10,10]     9.41 10804 1.00
gamma[10,11]    10.55 11310 1.00
gamma[11,1]      9.45 11073 1.00
gamma[11,2]      9.87 13455 1.00
gamma[11,3]      9.50  8654 1.00
gamma[11,4]      7.21  4851 1.00
gamma[11,5]      9.13  9179 1.00
gamma[11,6]      9.22  7734 1.00
gamma[11,7]     10.04  9498 1.00
gamma[11,8]     10.75  8325 1.00
gamma[11,9]     10.51 10878 1.00
gamma[11,10]     9.79 12098 1.00
gamma[11,11]     9.08 12364 1.00
gamma[12,1]     10.39 11987 1.00
gamma[12,2]      9.80 14422 1.00
gamma[12,3]      9.92  8709 1.00
gamma[12,4]      7.22  8125 1.00
gamma[12,5]      8.61  8484 1.00
gamma[12,6]      8.29  7493 1.00
gamma[12,7]     10.10  9920 1.00
gamma[12,8]      6.72  7238 1.00
gamma[12,9]      4.82  7632 1.00
gamma[12,10]     8.73  9754 1.00
gamma[12,11]     9.76 13775 1.00
sig[1]         271.16    26 1.20
sig[2]        1634.01   875 1.01
sig[3]        1311.82   581 1.01
lp__         -1459.46  2358 1.00

Samples were drawn using NUTS(diag_e) at Tue Aug 24 10:02:46 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/rainy_spab_50_model_inter_full_WD_slope.rda"
[1] "MCMC done!!"
