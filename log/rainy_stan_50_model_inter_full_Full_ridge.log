── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  rainy season"
[1] "Use full"
[1] "Habitat = ridge"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: 1 + all the traits"
[1] "sp number in seedling data: 66"
[1] "sp number in trait data: 66"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.15 as a scaling parameter for the distance effect"
[1] "n_sp = J =66"
[1] "n_para = K = 11"
[1] "n_plot = S = 111"
[1] "n_census = T = 10"
[1] "n_tag = M = 2966"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 1: 
Chain 1: Gradient evaluation took 0.006636 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 66.36 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.006409 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 64.09 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: 
Chain 4: Gradient evaluation took 0.0107 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 107 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 3: 
Chain 3: Gradient evaluation took 0.009119 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 91.19 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 119526 seconds (Warm-up)
Chain 4:                88398.4 seconds (Sampling)
Chain 4:                207925 seconds (Total)
Chain 4: 
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 111986 seconds (Warm-up)
Chain 2:                147398 seconds (Sampling)
Chain 2:                259384 seconds (Total)
Chain 2: 
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 119062 seconds (Warm-up)
Chain 3:                148384 seconds (Sampling)
Chain 3:                267445 seconds (Total)
Chain 3: 
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 121822 seconds (Warm-up)
Chain 1:                152584 seconds (Sampling)
Chain 1:                274406 seconds (Total)
Chain 1: 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean     sd     2.5%      25%      50%      75%
gamma[1,1]       3.67    0.02   1.18     1.42     2.88     3.66     4.46
gamma[1,2]      -0.80    0.03   1.92    -4.56    -2.11    -0.78     0.51
gamma[1,3]      -1.37    0.01   0.65    -2.74    -1.78    -1.34    -0.94
gamma[1,4]       0.20    0.00   0.23    -0.22     0.05     0.19     0.35
gamma[1,5]      -0.10    0.01   0.43    -0.94    -0.39    -0.10     0.18
gamma[1,6]       1.21    0.02   1.04    -0.77     0.52     1.21     1.92
gamma[1,7]       1.75    0.03   1.78    -1.71     0.53     1.75     2.96
gamma[1,8]       0.02    0.01   0.41    -0.78    -0.25     0.02     0.29
gamma[1,9]       0.19    0.00   0.18    -0.14     0.07     0.18     0.30
gamma[1,10]      0.27    0.01   0.34    -0.36     0.04     0.26     0.49
gamma[1,11]      1.45    0.01   0.27     0.94     1.26     1.44     1.63
gamma[2,1]      -1.81    0.03   1.79    -5.34    -3.01    -1.81    -0.59
gamma[2,2]       1.04    0.05   3.04    -4.94    -1.00     1.03     3.09
gamma[2,3]      -0.12    0.02   1.03    -2.05    -0.82    -0.15     0.56
gamma[2,4]      -0.36    0.01   0.37    -1.12    -0.59    -0.35    -0.11
gamma[2,5]       1.53    0.01   0.66     0.24     1.09     1.53     1.97
gamma[2,6]      -0.17    0.03   1.60    -3.28    -1.26    -0.19     0.90
gamma[2,7]      -0.64    0.05   2.89    -6.20    -2.59    -0.70     1.32
gamma[2,8]       0.24    0.01   0.63    -1.00    -0.18     0.25     0.66
gamma[2,9]       0.15    0.01   0.29    -0.42    -0.04     0.15     0.34
gamma[2,10]      0.35    0.01   0.57    -0.81    -0.03     0.35     0.72
gamma[2,11]     -0.08    0.01   0.46    -1.01    -0.38    -0.08     0.23
gamma[3,1]      -0.35    0.03   2.22    -4.58    -1.87    -0.37     1.18
gamma[3,2]      -1.55    0.05   3.42    -8.30    -3.83    -1.57     0.73
gamma[3,3]       1.85    0.02   1.42    -0.94     0.90     1.85     2.80
gamma[3,4]       0.14    0.01   0.79    -1.45    -0.38     0.15     0.67
gamma[3,5]      -0.26    0.02   1.38    -2.92    -1.21    -0.27     0.66
gamma[3,6]      -0.20    0.03   1.99    -4.10    -1.56    -0.20     1.14
gamma[3,7]      -1.03    0.05   3.33    -7.55    -3.30    -0.98     1.21
gamma[3,8]       0.38    0.02   0.96    -1.51    -0.27     0.38     1.02
gamma[3,9]       0.47    0.01   0.66    -0.80     0.05     0.47     0.91
gamma[3,10]     -1.07    0.02   1.11    -3.30    -1.80    -1.05    -0.33
gamma[3,11]     -1.09    0.01   0.87    -2.88    -1.65    -1.08    -0.53
gamma[4,1]      -0.23    0.03   2.00    -4.17    -1.60    -0.21     1.15
gamma[4,2]      -0.19    0.04   3.12    -6.40    -2.27    -0.20     1.92
gamma[4,3]      -1.72    0.02   1.39    -4.43    -2.64    -1.70    -0.77
gamma[4,4]      -0.02    0.01   0.70    -1.37    -0.49    -0.04     0.43
gamma[4,5]      -0.77    0.02   1.28    -3.36    -1.62    -0.76     0.08
gamma[4,6]       0.50    0.03   1.79    -2.95    -0.71     0.48     1.70
gamma[4,7]       0.73    0.04   3.03    -5.13    -1.32     0.71     2.78
gamma[4,8]       0.00    0.01   0.94    -1.88    -0.61     0.01     0.63
gamma[4,9]      -0.67    0.01   0.59    -1.83    -1.06    -0.66    -0.28
gamma[4,10]      0.68    0.02   1.06    -1.38    -0.02     0.68     1.40
gamma[4,11]      1.06    0.01   0.81    -0.55     0.53     1.04     1.57
gamma[5,1]       1.44    0.03   1.76    -1.96     0.23     1.43     2.61
gamma[5,2]      -0.78    0.04   2.83    -6.27    -2.67    -0.77     1.11
gamma[5,3]       0.39    0.02   0.87    -1.36    -0.17     0.40     0.96
gamma[5,4]       0.19    0.01   0.32    -0.42    -0.03     0.19     0.40
gamma[5,5]      -0.45    0.01   0.64    -1.68    -0.88    -0.45    -0.01
gamma[5,6]       1.10    0.03   1.52    -1.90     0.07     1.11     2.11
gamma[5,7]       1.05    0.04   2.62    -4.19    -0.67     1.05     2.78
gamma[5,8]       0.07    0.01   0.57    -1.01    -0.32     0.06     0.45
gamma[5,9]       0.32    0.00   0.27    -0.19     0.14     0.31     0.49
gamma[5,10]     -0.31    0.01   0.51    -1.35    -0.65    -0.30     0.03
gamma[5,11]      0.22    0.01   0.39    -0.52    -0.04     0.22     0.48
gamma[6,1]      -0.48    0.03   1.31    -3.03    -1.35    -0.48     0.39
gamma[6,2]      -0.29    0.04   2.31    -4.81    -1.85    -0.32     1.28
gamma[6,3]       0.51    0.01   0.65    -0.73     0.07     0.51     0.93
gamma[6,4]      -0.10    0.00   0.24    -0.58    -0.25    -0.10     0.05
gamma[6,5]      -0.13    0.01   0.47    -1.04    -0.44    -0.13     0.18
gamma[6,6]       1.01    0.02   1.12    -1.19     0.25     1.00     1.74
gamma[6,7]       0.82    0.03   2.06    -3.15    -0.58     0.81     2.20
gamma[6,8]       0.49    0.01   0.39    -0.26     0.23     0.49     0.76
gamma[6,9]       0.03    0.00   0.18    -0.31    -0.09     0.03     0.14
gamma[6,10]      0.06    0.01   0.38    -0.71    -0.19     0.06     0.32
gamma[6,11]      0.04    0.00   0.30    -0.56    -0.16     0.03     0.24
gamma[7,1]      -0.74    0.03   1.92    -4.44    -2.03    -0.75     0.58
gamma[7,2]      -0.25    0.04   3.28    -6.57    -2.47    -0.33     1.94
gamma[7,3]      -0.48    0.02   0.86    -2.20    -1.03    -0.47     0.08
gamma[7,4]       0.47    0.01   0.45    -0.41     0.19     0.46     0.76
gamma[7,5]       0.18    0.01   0.79    -1.39    -0.35     0.19     0.70
gamma[7,6]       0.65    0.03   1.78    -2.95    -0.50     0.67     1.82
gamma[7,7]      -0.64    0.04   3.16    -7.01    -2.71    -0.64     1.43
gamma[7,8]       0.05    0.01   0.54    -1.01    -0.31     0.06     0.41
gamma[7,9]       0.17    0.01   0.35    -0.49    -0.07     0.15     0.39
gamma[7,10]     -0.85    0.01   0.68    -2.22    -1.29    -0.85    -0.41
gamma[7,11]      0.75    0.01   0.57    -0.34     0.37     0.74     1.13
gamma[8,1]      -1.01    0.04   2.57    -5.99    -2.74    -1.00     0.70
gamma[8,2]      -2.51    0.05   3.59    -9.52    -4.94    -2.47    -0.16
gamma[8,3]      -0.21    0.03   1.73    -3.65    -1.37    -0.22     0.97
gamma[8,4]      -1.07    0.02   0.99    -3.13    -1.69    -1.03    -0.41
gamma[8,5]       1.77    0.02   1.51    -1.20     0.78     1.75     2.74
gamma[8,6]      -0.90    0.04   2.21    -5.16    -2.41    -0.89     0.60
gamma[8,7]       3.12    0.04   3.39    -3.53     0.85     3.10     5.38
gamma[8,8]      -0.68    0.02   1.20    -3.01    -1.50    -0.69     0.12
gamma[8,9]       0.39    0.02   0.79    -1.23    -0.12     0.41     0.92
gamma[8,10]      2.11    0.02   1.28    -0.37     1.27     2.08     2.94
gamma[8,11]     -1.52    0.02   0.92    -3.33    -2.13    -1.53    -0.91
gamma[9,1]       1.07    0.04   2.30    -3.46    -0.48     1.06     2.60
gamma[9,2]      -0.40    0.04   3.29    -6.87    -2.65    -0.40     1.83
gamma[9,3]      -0.49    0.04   1.79    -4.08    -1.67    -0.47     0.74
gamma[9,4]      -1.24    0.02   1.00    -3.26    -1.89    -1.21    -0.58
gamma[9,5]       1.12    0.03   1.65    -2.02     0.01     1.12     2.19
gamma[9,6]      -2.23    0.03   2.06    -6.20    -3.62    -2.25    -0.85
gamma[9,7]       0.55    0.04   3.10    -5.63    -1.51     0.53     2.58
gamma[9,8]      -0.64    0.02   1.16    -2.89    -1.42    -0.64     0.14
gamma[9,9]       0.28    0.02   0.81    -1.32    -0.25     0.30     0.83
gamma[9,10]      2.62    0.02   1.40    -0.03     1.68     2.58     3.54
gamma[9,11]     -1.41    0.02   1.03    -3.43    -2.08    -1.41    -0.75
gamma[10,1]     -2.12    0.03   1.71    -5.50    -3.27    -2.14    -0.97
gamma[10,2]     -1.88    0.04   3.17    -8.15    -4.03    -1.89     0.26
gamma[10,3]      0.33    0.01   0.67    -0.95    -0.12     0.31     0.75
gamma[10,4]      0.32    0.00   0.28    -0.23     0.14     0.31     0.50
gamma[10,5]      1.10    0.01   0.57     0.02     0.71     1.09     1.47
gamma[10,6]      0.82    0.02   1.52    -2.22    -0.20     0.83     1.83
gamma[10,7]      0.21    0.04   2.92    -5.68    -1.70     0.26     2.18
gamma[10,8]      0.24    0.01   0.40    -0.53    -0.03     0.23     0.50
gamma[10,9]     -0.32    0.00   0.22    -0.77    -0.47    -0.32    -0.18
gamma[10,10]     0.01    0.01   0.45    -0.88    -0.29     0.00     0.30
gamma[10,11]     0.66    0.00   0.33     0.02     0.43     0.66     0.88
gamma[11,1]      0.95    0.02   1.52    -2.12    -0.05     0.95     1.95
gamma[11,2]      2.43    0.04   2.69    -2.93     0.65     2.45     4.23
gamma[11,3]      0.08    0.01   0.73    -1.35    -0.40     0.08     0.56
gamma[11,4]     -0.46    0.01   0.34    -1.15    -0.69    -0.46    -0.23
gamma[11,5]      0.84    0.01   0.72    -0.56     0.35     0.83     1.31
gamma[11,6]     -2.09    0.02   1.36    -4.71    -3.00    -2.10    -1.17
gamma[11,7]     -2.30    0.04   2.42    -6.92    -3.95    -2.33    -0.69
gamma[11,8]     -0.42    0.01   0.48    -1.35    -0.74    -0.42    -0.09
gamma[11,9]      0.48    0.01   0.30    -0.08     0.28     0.48     0.68
gamma[11,10]     0.57    0.01   0.61    -0.62     0.16     0.56     0.98
gamma[11,11]    -1.07    0.01   0.43    -1.97    -1.34    -1.05    -0.77
gamma[12,1]     -0.84    0.03   2.21    -5.19    -2.30    -0.83     0.63
gamma[12,2]     -1.77    0.05   3.73    -9.08    -4.31    -1.80     0.73
gamma[12,3]      0.07    0.02   1.21    -2.41    -0.71     0.09     0.88
gamma[12,4]     -0.48    0.01   0.44    -1.34    -0.77    -0.48    -0.19
gamma[12,5]      0.06    0.01   0.89    -1.67    -0.51     0.06     0.65
gamma[12,6]     -0.54    0.03   1.99    -4.42    -1.90    -0.51     0.80
gamma[12,7]     -1.30    0.05   3.56    -8.32    -3.67    -1.24     1.09
gamma[12,8]      0.50    0.01   0.79    -1.06    -0.05     0.49     1.02
gamma[12,9]      0.11    0.01   0.35    -0.54    -0.12     0.10     0.34
gamma[12,10]     0.59    0.01   0.76    -0.90     0.08     0.58     1.08
gamma[12,11]     0.02    0.01   0.61    -1.21    -0.38     0.03     0.43
gamma[13,1]     -2.20    0.03   1.37    -4.85    -3.14    -2.20    -1.29
gamma[13,2]     -0.58    0.04   2.26    -5.07    -2.09    -0.59     0.94
gamma[13,3]     -0.22    0.01   0.84    -1.86    -0.78    -0.23     0.32
gamma[13,4]     -0.49    0.00   0.31    -1.16    -0.69    -0.48    -0.28
gamma[13,5]      1.02    0.01   0.52    -0.03     0.68     1.01     1.36
gamma[13,6]     -0.90    0.02   1.21    -3.24    -1.73    -0.89    -0.10
gamma[13,7]     -0.72    0.04   2.21    -5.04    -2.19    -0.71     0.76
gamma[13,8]      0.26    0.01   0.52    -0.75    -0.08     0.26     0.60
gamma[13,9]      0.24    0.00   0.23    -0.21     0.09     0.24     0.39
gamma[13,10]     0.40    0.01   0.45    -0.49     0.11     0.40     0.70
gamma[13,11]    -0.58    0.01   0.33    -1.27    -0.79    -0.57    -0.36
sig[1]           0.82    0.00   0.14     0.57     0.72     0.80     0.90
sig[2]           0.80    0.01   0.28     0.42     0.60     0.74     0.94
sig[3]           1.99    0.01   0.27     1.50     1.80     1.98     2.15
lp__         -4156.99    5.46 112.49 -4376.08 -4233.16 -4157.61 -4083.04
                97.5% n_eff Rhat
gamma[1,1]       6.01  2765 1.00
gamma[1,2]       2.88  3376 1.00
gamma[1,3]      -0.20  2845 1.00
gamma[1,4]       0.69  3465 1.00
gamma[1,5]       0.73  4226 1.00
gamma[1,6]       3.28  3000 1.00
gamma[1,7]       5.23  3371 1.00
gamma[1,8]       0.82  4246 1.00
gamma[1,9]       0.55  5080 1.00
gamma[1,10]      0.99  3988 1.00
gamma[1,11]      2.01  2626 1.00
gamma[2,1]       1.72  3442 1.00
gamma[2,2]       6.87  4370 1.00
gamma[2,3]       1.96  3259 1.00
gamma[2,4]       0.34  3270 1.00
gamma[2,5]       2.86  3517 1.00
gamma[2,6]       3.04  3167 1.00
gamma[2,7]       4.99  3788 1.00
gamma[2,8]       1.48  4734 1.00
gamma[2,9]       0.74  3157 1.00
gamma[2,10]      1.48  3798 1.00
gamma[2,11]      0.80  3754 1.00
gamma[3,1]       3.92  4346 1.00
gamma[3,2]       5.31  5670 1.00
gamma[3,3]       4.57  3845 1.00
gamma[3,4]       1.67  4073 1.00
gamma[3,5]       2.49  3591 1.00
gamma[3,6]       3.69  3599 1.00
gamma[3,7]       5.41  5010 1.00
gamma[3,8]       2.29  4089 1.00
gamma[3,9]       1.79  3844 1.00
gamma[3,10]      1.03  4826 1.00
gamma[3,11]      0.60  4500 1.00
gamma[4,1]       3.66  4406 1.00
gamma[4,2]       5.91  5417 1.00
gamma[4,3]       0.92  3854 1.00
gamma[4,4]       1.38  4037 1.00
gamma[4,5]       1.68  4087 1.00
gamma[4,6]       3.99  4040 1.00
gamma[4,7]       6.70  4869 1.00
gamma[4,8]       1.88  4050 1.00
gamma[4,9]       0.46  4007 1.00
gamma[4,10]      2.74  4648 1.00
gamma[4,11]      2.69  4472 1.00
gamma[5,1]       4.90  2949 1.00
gamma[5,2]       4.86  4159 1.00
gamma[5,3]       2.08  2186 1.00
gamma[5,4]       0.84  3932 1.00
gamma[5,5]       0.80  4392 1.00
gamma[5,6]       4.06  2935 1.00
gamma[5,7]       6.20  3611 1.00
gamma[5,8]       1.20  3710 1.00
gamma[5,9]       0.87  4163 1.00
gamma[5,10]      0.70  4856 1.00
gamma[5,11]      1.00  4456 1.00
gamma[6,1]       2.16  2594 1.00
gamma[6,2]       4.19  3276 1.00
gamma[6,3]       1.82  3476 1.00
gamma[6,4]       0.37  4551 1.00
gamma[6,5]       0.81  4334 1.00
gamma[6,6]       3.19  3470 1.00
gamma[6,7]       4.90  3673 1.00
gamma[6,8]       1.27  4723 1.00
gamma[6,9]       0.38  5021 1.00
gamma[6,10]      0.80  4898 1.00
gamma[6,11]      0.63  3888 1.00
gamma[7,1]       3.03  4535 1.00
gamma[7,2]       6.26  5419 1.00
gamma[7,3]       1.21  3039 1.00
gamma[7,4]       1.37  2967 1.00
gamma[7,5]       1.71  3620 1.00
gamma[7,6]       4.13  4767 1.00
gamma[7,7]       5.58  5789 1.00
gamma[7,8]       1.11  4088 1.00
gamma[7,9]       0.88  2773 1.00
gamma[7,10]      0.47  3669 1.00
gamma[7,11]      1.88  2986 1.00
gamma[8,1]       4.02  4264 1.00
gamma[8,2]       4.54  6323 1.00
gamma[8,3]       3.15  2807 1.00
gamma[8,4]       0.79  3190 1.00
gamma[8,5]       4.78  3663 1.00
gamma[8,6]       3.46  3685 1.00
gamma[8,7]       9.66  5920 1.00
gamma[8,8]       1.68  3431 1.00
gamma[8,9]       1.91  2374 1.00
gamma[8,10]      4.73  3640 1.00
gamma[8,11]      0.29  2655 1.00
gamma[9,1]       5.54  3621 1.00
gamma[9,2]       6.07  5490 1.00
gamma[9,3]       3.00  2476 1.00
gamma[9,4]       0.69  2947 1.00
gamma[9,5]       4.41  3318 1.00
gamma[9,6]       1.91  3469 1.00
gamma[9,7]       6.63  5097 1.00
gamma[9,8]       1.68  3286 1.00
gamma[9,9]       1.84  2331 1.00
gamma[9,10]      5.48  3226 1.00
gamma[9,11]      0.62  2607 1.00
gamma[10,1]      1.26  4400 1.00
gamma[10,2]      4.41  5108 1.00
gamma[10,3]      1.73  4618 1.00
gamma[10,4]      0.88  5355 1.00
gamma[10,5]      2.28  4706 1.00
gamma[10,6]      3.80  4833 1.00
gamma[10,7]      5.90  4994 1.00
gamma[10,8]      1.05  6070 1.00
gamma[10,9]      0.11  5472 1.00
gamma[10,10]     0.91  6164 1.00
gamma[10,11]     1.33  5404 1.00
gamma[11,1]      3.96  4306 1.00
gamma[11,2]      7.74  5098 1.00
gamma[11,3]      1.51  4552 1.00
gamma[11,4]      0.19  3640 1.00
gamma[11,5]      2.30  3843 1.00
gamma[11,6]      0.59  3732 1.00
gamma[11,7]      2.47  4612 1.00
gamma[11,8]      0.53  3632 1.00
gamma[11,9]      1.08  2792 1.00
gamma[11,10]     1.79  3671 1.00
gamma[11,11]    -0.27  3730 1.00
gamma[12,1]      3.48  4158 1.00
gamma[12,2]      5.51  6346 1.00
gamma[12,3]      2.41  3127 1.00
gamma[12,4]      0.40  4215 1.00
gamma[12,5]      1.77  4108 1.00
gamma[12,6]      3.36  4829 1.00
gamma[12,7]      5.73  5469 1.00
gamma[12,8]      2.07  4839 1.00
gamma[12,9]      0.82  4074 1.00
gamma[12,10]     2.11  3855 1.00
gamma[12,11]     1.21  3620 1.00
gamma[13,1]      0.45  2924 1.00
gamma[13,2]      3.84  3768 1.00
gamma[13,3]      1.45  3599 1.00
gamma[13,4]      0.10  3983 1.00
gamma[13,5]      2.02  3420 1.00
gamma[13,6]      1.50  3190 1.00
gamma[13,7]      3.65  3695 1.00
gamma[13,8]      1.31  4380 1.00
gamma[13,9]      0.71  4723 1.00
gamma[13,10]     1.30  4506 1.00
gamma[13,11]     0.03  3944 1.00
sig[1]           1.12  1352 1.00
sig[2]           1.51  1350 1.00
sig[3]           2.57   429 1.00
lp__         -3931.16   425 1.01

Samples were drawn using NUTS(diag_e) at Wed Aug 18 00:19:58 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/rainy_spab_50_model_inter_full_Full_ridge.rda"
[1] "MCMC done!!"
