── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  rainy season"
[1] "Use full"
[1] "Habitat = ridge"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: except for WD"
[1] "sp number in seedling data: 66"
[1] "sp number in trait data: 66"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.15 as a scaling parameter for the distance effect"
[1] "n_sp = J =66"
[1] "n_para = K = 11"
[1] "n_plot = S = 111"
[1] "n_census = T = 10"
[1] "n_tag = M = 2966"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 2: 
Chain 2: Gradient evaluation took 0.00469 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 46.9 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 3: 
Chain 3: Gradient evaluation took 0.0094 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 94 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 1: 
Chain 1: Gradient evaluation took 0.008948 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 89.48 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 4: 
Chain 4: Gradient evaluation took 0.009347 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 93.47 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain Chain 13: : Iteration:    1 / 4000 [  0%]  (Warmup)Iteration:    1 / 4000 [  0%]  (Warmup)

Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 51187 seconds (Warm-up)
Chain 2:                38083.7 seconds (Sampling)
Chain 2:                89270.7 seconds (Total)
Chain 2: 
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 68901.5 seconds (Warm-up)
Chain 4:                29437.3 seconds (Sampling)
Chain 4:                98338.8 seconds (Total)
Chain 4: 
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 69853.5 seconds (Warm-up)
Chain 3:                29348.2 seconds (Sampling)
Chain 3:                99201.7 seconds (Total)
Chain 3: 
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 71616.9 seconds (Warm-up)
Chain 1:                27866.3 seconds (Sampling)
Chain 1:                99483.2 seconds (Total)
Chain 1: 
Warning message:
Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                 mean se_mean     sd     2.5%      25%      50%      75%
gamma[1,1]       3.48    0.03   1.14     1.27     2.71     3.47     4.22
gamma[1,2]      -0.92    0.04   1.85    -4.58    -2.16    -0.92     0.34
gamma[1,3]      -1.17    0.01   0.60    -2.44    -1.55    -1.15    -0.75
gamma[1,4]       0.17    0.00   0.21    -0.21     0.03     0.16     0.31
gamma[1,5]      -0.15    0.01   0.38    -0.91    -0.41    -0.15     0.10
gamma[1,6]       1.21    0.02   0.99    -0.70     0.54     1.21     1.88
gamma[1,7]       1.60    0.04   1.68    -1.69     0.49     1.60     2.73
gamma[1,8]       0.14    0.01   0.36    -0.55    -0.10     0.14     0.38
gamma[1,9]       0.20    0.00   0.17    -0.11     0.09     0.19     0.31
gamma[1,10]      0.19    0.01   0.32    -0.40    -0.02     0.19     0.39
gamma[1,11]      1.32    0.01   0.25     0.85     1.15     1.32     1.49
gamma[2,1]      -1.88    0.04   1.75    -5.30    -3.06    -1.92    -0.75
gamma[2,2]       0.84    0.06   2.96    -4.96    -1.14     0.78     2.82
gamma[2,3]      -0.33    0.02   0.94    -2.11    -0.96    -0.35     0.27
gamma[2,4]      -0.34    0.01   0.34    -1.04    -0.55    -0.33    -0.12
gamma[2,5]       1.48    0.01   0.63     0.26     1.06     1.48     1.90
gamma[2,6]      -0.26    0.04   1.53    -3.31    -1.27    -0.25     0.77
gamma[2,7]      -0.63    0.06   2.77    -6.20    -2.45    -0.61     1.23
gamma[2,8]       0.11    0.01   0.56    -1.02    -0.26     0.11     0.49
gamma[2,9]       0.17    0.01   0.27    -0.38     0.00     0.17     0.35
gamma[2,10]      0.40    0.01   0.53    -0.64     0.05     0.40     0.75
gamma[2,11]     -0.13    0.01   0.43    -1.01    -0.41    -0.12     0.17
gamma[3,1]      -0.29    0.03   1.29    -2.86    -1.14    -0.27     0.59
gamma[3,2]      -1.15    0.04   2.31    -5.70    -2.68    -1.14     0.43
gamma[3,3]      -0.15    0.02   0.74    -1.69    -0.63    -0.13     0.35
gamma[3,4]       0.09    0.00   0.26    -0.40    -0.08     0.08     0.25
gamma[3,5]      -0.96    0.01   0.47    -1.93    -1.26    -0.95    -0.64
gamma[3,6]       0.37    0.02   1.11    -1.79    -0.39     0.37     1.13
gamma[3,7]      -0.01    0.04   2.11    -4.13    -1.44    -0.02     1.40
gamma[3,8]       0.28    0.01   0.45    -0.62    -0.02     0.28     0.57
gamma[3,9]      -0.28    0.00   0.20    -0.69    -0.41    -0.28    -0.15
gamma[3,10]     -0.23    0.01   0.42    -1.08    -0.50    -0.22     0.05
gamma[3,11]      0.12    0.00   0.31    -0.49    -0.09     0.12     0.33
gamma[4,1]       1.26    0.04   1.66    -1.93     0.12     1.25     2.37
gamma[4,2]      -0.91    0.06   2.77    -6.35    -2.81    -0.91     0.99
gamma[4,3]       0.46    0.02   0.78    -1.13    -0.04     0.48     0.99
gamma[4,4]       0.14    0.01   0.31    -0.44    -0.07     0.13     0.34
gamma[4,5]      -0.44    0.01   0.61    -1.64    -0.85    -0.44    -0.03
gamma[4,6]       1.11    0.03   1.44    -1.77     0.15     1.13     2.09
gamma[4,7]       1.04    0.05   2.54    -4.13    -0.64     1.13     2.76
gamma[4,8]       0.17    0.01   0.51    -0.83    -0.17     0.17     0.51
gamma[4,9]       0.28    0.01   0.25    -0.18     0.11     0.27     0.44
gamma[4,10]     -0.24    0.01   0.48    -1.21    -0.55    -0.23     0.07
gamma[4,11]      0.14    0.01   0.36    -0.58    -0.10     0.14     0.37
gamma[5,1]      -0.55    0.03   1.24    -2.94    -1.38    -0.56     0.30
gamma[5,2]      -0.37    0.05   2.22    -4.66    -1.85    -0.36     1.15
gamma[5,3]       0.36    0.01   0.61    -0.84    -0.04     0.36     0.76
gamma[5,4]      -0.11    0.00   0.22    -0.54    -0.25    -0.10     0.04
gamma[5,5]      -0.15    0.01   0.43    -0.98    -0.43    -0.15     0.13
gamma[5,6]       0.94    0.02   1.06    -1.13     0.23     0.95     1.64
gamma[5,7]       0.75    0.04   1.93    -3.04    -0.54     0.75     2.03
gamma[5,8]       0.47    0.01   0.38    -0.26     0.22     0.47     0.72
gamma[5,9]       0.01    0.00   0.17    -0.31    -0.10     0.01     0.12
gamma[5,10]      0.16    0.01   0.36    -0.57    -0.07     0.17     0.39
gamma[5,11]      0.01    0.01   0.29    -0.55    -0.18     0.01     0.20
gamma[6,1]      -0.80    0.04   1.82    -4.34    -2.00    -0.85     0.41
gamma[6,2]      -0.52    0.06   3.12    -6.64    -2.61    -0.53     1.55
gamma[6,3]      -0.54    0.02   0.83    -2.17    -1.09    -0.54     0.01
gamma[6,4]       0.43    0.01   0.39    -0.33     0.18     0.42     0.68
gamma[6,5]       0.29    0.02   0.77    -1.18    -0.23     0.27     0.79
gamma[6,6]       0.43    0.04   1.75    -2.94    -0.77     0.41     1.65
gamma[6,7]      -0.80    0.06   3.15    -6.91    -2.97    -0.81     1.37
gamma[6,8]       0.00    0.01   0.51    -1.02    -0.34     0.00     0.34
gamma[6,9]       0.08    0.01   0.29    -0.49    -0.12     0.07     0.27
gamma[6,10]     -0.68    0.02   0.64    -1.92    -1.11    -0.69    -0.26
gamma[6,11]      0.88    0.01   0.54    -0.17     0.52     0.89     1.24
gamma[7,1]      -1.18    0.05   2.51    -6.13    -2.81    -1.15     0.46
gamma[7,2]      -2.49    0.06   3.55    -9.52    -4.85    -2.47    -0.10
gamma[7,3]       0.03    0.04   1.67    -3.29    -1.06     0.07     1.14
gamma[7,4]      -0.99    0.02   0.85    -2.79    -1.52    -0.95    -0.45
gamma[7,5]       1.42    0.04   1.40    -1.35     0.49     1.40     2.35
gamma[7,6]      -0.67    0.05   2.14    -4.89    -2.11    -0.64     0.76
gamma[7,7]       3.05    0.06   3.35    -3.51     0.78     3.05     5.30
gamma[7,8]      -0.52    0.03   1.12    -2.71    -1.28    -0.52     0.23
gamma[7,9]       0.62    0.01   0.64    -0.69     0.21     0.62     1.04
gamma[7,10]      1.59    0.03   1.19    -0.70     0.79     1.57     2.37
gamma[7,11]     -1.86    0.02   0.85    -3.50    -2.44    -1.85    -1.29
gamma[8,1]       0.88    0.05   2.22    -3.59    -0.57     0.90     2.37
gamma[8,2]      -0.63    0.06   3.10    -6.67    -2.71    -0.60     1.49
gamma[8,3]       0.09    0.04   1.68    -3.34    -1.01     0.11     1.21
gamma[8,4]      -1.13    0.02   0.81    -2.83    -1.63    -1.10    -0.61
gamma[8,5]       0.71    0.04   1.50    -2.27    -0.30     0.73     1.71
gamma[8,6]      -1.93    0.05   1.90    -5.73    -3.19    -1.91    -0.63
gamma[8,7]       0.35    0.06   2.96    -5.46    -1.62     0.36     2.36
gamma[8,8]      -0.31    0.02   1.03    -2.32    -0.99    -0.33     0.37
gamma[8,9]       0.57    0.02   0.61    -0.68     0.17     0.57     0.98
gamma[8,10]      1.97    0.03   1.27    -0.45     1.12     1.95     2.80
gamma[8,11]     -1.87    0.02   0.91    -3.63    -2.47    -1.86    -1.26
gamma[9,1]      -2.05    0.03   1.68    -5.36    -3.18    -2.03    -0.92
gamma[9,2]      -1.70    0.06   3.13    -7.85    -3.81    -1.66     0.39
gamma[9,3]       0.22    0.01   0.63    -0.95    -0.21     0.19     0.62
gamma[9,4]       0.28    0.00   0.26    -0.25     0.11     0.29     0.46
gamma[9,5]       1.10    0.01   0.51     0.14     0.76     1.09     1.44
gamma[9,6]       0.76    0.03   1.49    -2.16    -0.25     0.76     1.78
gamma[9,7]       0.38    0.05   2.85    -5.26    -1.54     0.39     2.29
gamma[9,8]       0.15    0.01   0.37    -0.58    -0.10     0.14     0.39
gamma[9,9]      -0.32    0.00   0.21    -0.73    -0.46    -0.32    -0.18
gamma[9,10]      0.00    0.01   0.42    -0.81    -0.28     0.01     0.28
gamma[9,11]      0.59    0.01   0.31    -0.01     0.38     0.59     0.80
gamma[10,1]      0.88    0.03   1.45    -1.96    -0.09     0.86     1.86
gamma[10,2]      2.48    0.05   2.59    -2.61     0.72     2.48     4.22
gamma[10,3]     -0.24    0.01   0.65    -1.52    -0.68    -0.24     0.19
gamma[10,4]     -0.47    0.01   0.33    -1.12    -0.68    -0.46    -0.25
gamma[10,5]      0.77    0.02   0.66    -0.51     0.32     0.78     1.21
gamma[10,6]     -1.89    0.03   1.28    -4.39    -2.76    -1.89    -1.02
gamma[10,7]     -2.05    0.05   2.33    -6.57    -3.65    -2.08    -0.47
gamma[10,8]     -0.45    0.01   0.42    -1.26    -0.73    -0.44    -0.16
gamma[10,9]      0.42    0.01   0.28    -0.10     0.23     0.41     0.60
gamma[10,10]     0.65    0.01   0.55    -0.43     0.29     0.65     1.01
gamma[10,11]    -0.90    0.01   0.39    -1.67    -1.15    -0.89    -0.64
gamma[11,1]     -0.64    0.04   2.07    -4.71    -2.03    -0.67     0.77
gamma[11,2]     -1.35    0.06   3.53    -8.20    -3.76    -1.35     1.00
gamma[11,3]     -0.08    0.02   1.13    -2.46    -0.80    -0.05     0.68
gamma[11,4]     -0.48    0.01   0.41    -1.30    -0.74    -0.48    -0.21
gamma[11,5]      0.00    0.02   0.83    -1.64    -0.56     0.00     0.55
gamma[11,6]     -0.47    0.04   1.90    -4.15    -1.76    -0.49     0.84
gamma[11,7]     -1.15    0.06   3.37    -7.63    -3.45    -1.20     1.15
gamma[11,8]      0.51    0.02   0.76    -0.99    -0.01     0.50     1.01
gamma[11,9]      0.06    0.01   0.32    -0.55    -0.15     0.05     0.28
gamma[11,10]     0.71    0.01   0.69    -0.68     0.26     0.72     1.17
gamma[11,11]     0.05    0.01   0.56    -1.05    -0.33     0.06     0.43
gamma[12,1]     -2.15    0.03   1.30    -4.68    -3.04    -2.15    -1.27
gamma[12,2]     -0.54    0.05   2.17    -4.81    -2.01    -0.56     0.90
gamma[12,3]     -0.49    0.02   0.77    -2.00    -0.98    -0.49     0.00
gamma[12,4]     -0.46    0.01   0.28    -1.05    -0.64    -0.45    -0.27
gamma[12,5]      0.99    0.01   0.48     0.05     0.68     0.99     1.32
gamma[12,6]     -0.99    0.03   1.14    -3.20    -1.75    -1.01    -0.22
gamma[12,7]     -0.76    0.04   2.11    -4.81    -2.17    -0.78     0.65
gamma[12,8]      0.10    0.01   0.47    -0.83    -0.21     0.10     0.40
gamma[12,9]      0.23    0.00   0.22    -0.19     0.08     0.22     0.36
gamma[12,10]     0.45    0.01   0.42    -0.40     0.19     0.46     0.73
gamma[12,11]    -0.50    0.01   0.30    -1.10    -0.69    -0.49    -0.29
sig[1]           0.79    0.00   0.13     0.55     0.69     0.78     0.87
sig[2]           0.74    0.01   0.26     0.38     0.56     0.69     0.87
sig[3]           1.85    0.01   0.26     1.39     1.67     1.84     2.02
lp__         -4207.37    5.60 113.09 -4423.68 -4283.99 -4208.43 -4133.01
                97.5% n_eff Rhat
gamma[1,1]       5.80  1367 1.00
gamma[1,2]       2.65  1740 1.00
gamma[1,3]      -0.07  1821 1.00
gamma[1,4]       0.62  2777 1.00
gamma[1,5]       0.60  2368 1.00
gamma[1,6]       3.14  1649 1.00
gamma[1,7]       4.88  1983 1.00
gamma[1,8]       0.86  2129 1.00
gamma[1,9]       0.54  2904 1.00
gamma[1,10]      0.84  2931 1.00
gamma[1,11]      1.82  2144 1.00
gamma[2,1]       1.61  2045 1.00
gamma[2,2]       6.62  2582 1.00
gamma[2,3]       1.57  1669 1.00
gamma[2,4]       0.29  2253 1.00
gamma[2,5]       2.74  1767 1.00
gamma[2,6]       2.74  1890 1.00
gamma[2,7]       4.73  2167 1.00
gamma[2,8]       1.22  2127 1.00
gamma[2,9]       0.71  2397 1.00
gamma[2,10]      1.44  2220 1.00
gamma[2,11]      0.68  2327 1.00
gamma[3,1]       2.17  2360 1.00
gamma[3,2]       3.34  2650 1.00
gamma[3,3]       1.25  1907 1.00
gamma[3,4]       0.63  3043 1.00
gamma[3,5]      -0.08  3096 1.00
gamma[3,6]       2.57  2065 1.00
gamma[3,7]       4.16  2219 1.00
gamma[3,8]       1.15  3054 1.00
gamma[3,9]       0.12  3175 1.00
gamma[3,10]      0.56  3409 1.00
gamma[3,11]      0.73  3884 1.00
gamma[4,1]       4.55  1938 1.00
gamma[4,2]       4.46  2313 1.00
gamma[4,3]       1.93  1961 1.00
gamma[4,4]       0.76  2380 1.00
gamma[4,5]       0.74  1928 1.00
gamma[4,6]       3.90  1798 1.00
gamma[4,7]       5.98  2214 1.00
gamma[4,8]       1.18  1752 1.00
gamma[4,9]       0.78  2396 1.00
gamma[4,10]      0.69  2626 1.00
gamma[4,11]      0.83  2987 1.00
gamma[5,1]       1.90  1719 1.00
gamma[5,2]       3.95  1967 1.00
gamma[5,3]       1.55  2593 1.00
gamma[5,4]       0.32  2953 1.00
gamma[5,5]       0.71  2110 1.00
gamma[5,6]       3.03  1959 1.00
gamma[5,7]       4.53  2414 1.00
gamma[5,8]       1.22  2310 1.00
gamma[5,9]       0.34  3199 1.00
gamma[5,10]      0.84  2927 1.00
gamma[5,11]      0.58  2897 1.00
gamma[6,1]       2.85  2426 1.00
gamma[6,2]       5.72  2834 1.00
gamma[6,3]       1.09  2392 1.00
gamma[6,4]       1.21  1765 1.00
gamma[6,5]       1.82  1399 1.00
gamma[6,6]       3.82  2389 1.00
gamma[6,7]       5.29  2942 1.00
gamma[6,8]       1.00  2216 1.00
gamma[6,9]       0.69  2154 1.00
gamma[6,10]      0.58  1781 1.00
gamma[6,11]      1.96  1937 1.00
gamma[7,1]       3.66  2478 1.00
gamma[7,2]       4.41  4027 1.00
gamma[7,3]       3.27  2077 1.00
gamma[7,4]       0.56  1657 1.00
gamma[7,5]       4.13  1331 1.00
gamma[7,6]       3.50  2068 1.00
gamma[7,7]       9.63  3557 1.00
gamma[7,8]       1.70  1933 1.00
gamma[7,9]       1.86  1847 1.00
gamma[7,10]      3.98  1682 1.00
gamma[7,11]     -0.16  1704 1.00
gamma[8,1]       5.20  2004 1.00
gamma[8,2]       5.42  3011 1.00
gamma[8,3]       3.26  2012 1.00
gamma[8,4]       0.43  1568 1.00
gamma[8,5]       3.60  1174 1.00
gamma[8,6]       1.75  1616 1.00
gamma[8,7]       6.14  2657 1.00
gamma[8,8]       1.73  1767 1.00
gamma[8,9]       1.77  1673 1.00
gamma[8,10]      4.52  1504 1.00
gamma[8,11]     -0.11  1567 1.00
gamma[9,1]       1.23  2598 1.00
gamma[9,2]       4.45  2738 1.00
gamma[9,3]       1.51  2901 1.00
gamma[9,4]       0.80  3846 1.00
gamma[9,5]       2.18  3275 1.00
gamma[9,6]       3.64  2581 1.00
gamma[9,7]       5.88  2721 1.00
gamma[9,8]       0.88  4446 1.00
gamma[9,9]       0.07  3605 1.00
gamma[9,10]      0.83  3785 1.00
gamma[9,11]      1.20  3551 1.00
gamma[10,1]      3.77  2119 1.00
gamma[10,2]      7.64  2477 1.00
gamma[10,3]      1.06  2833 1.00
gamma[10,4]      0.17  2040 1.00
gamma[10,5]      2.04  1598 1.00
gamma[10,6]      0.67  1961 1.00
gamma[10,7]      2.55  2236 1.00
gamma[10,8]      0.36  2504 1.00
gamma[10,9]      1.00  2036 1.00
gamma[10,10]     1.73  1737 1.00
gamma[10,11]    -0.17  2290 1.00
gamma[11,1]      3.43  2583 1.00
gamma[11,2]      5.54  3343 1.00
gamma[11,3]      2.08  2065 1.00
gamma[11,4]      0.33  2515 1.00
gamma[11,5]      1.61  1832 1.00
gamma[11,6]      3.15  2505 1.00
gamma[11,7]      5.35  2846 1.00
gamma[11,8]      2.04  2101 1.00
gamma[11,9]      0.73  2777 1.00
gamma[11,10]     2.06  2256 1.00
gamma[11,11]     1.12  2393 1.00
gamma[12,1]      0.40  1590 1.00
gamma[12,2]      3.79  1939 1.00
gamma[12,3]      1.03  2137 1.00
gamma[12,4]      0.07  2548 1.00
gamma[12,5]      1.94  2455 1.00
gamma[12,6]      1.27  2072 1.00
gamma[12,7]      3.44  2295 1.00
gamma[12,8]      1.02  2440 1.00
gamma[12,9]      0.67  2996 1.00
gamma[12,10]     1.28  2879 1.00
gamma[12,11]     0.07  2529 1.00
sig[1]           1.07  1304 1.00
sig[2]           1.38  1321 1.00
sig[3]           2.41   382 1.01
lp__         -3982.29   408 1.00

Samples were drawn using NUTS(diag_e) at Mon Aug 23 09:02:22 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/rainy_spab_50_model_inter_full_WD_ridge.rda"
[1] "MCMC done!!"
