── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.1.2     ✔ dplyr   1.0.6
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

This is bayesplot version 1.8.0
- Online documentation and vignettes at mc-stan.org/bayesplot
- bayesplot theme set to bayesplot::theme_default()
   * Does _not_ affect other ggplot2 plots
   * See ?bayesplot_theme_set for details on theme setting
[1] "Model  model_inter"
[1] "Model for  dry season"
[1] "Use full"
[1] "Habitat = Full"
[1] "n_iter = 4000"
[1] "n_warm = 2000"
[1] "n_thin = 1"
[1] "n_chains = 4"
[1] "adapt_delta = 0.95"
[1] "minimum sp abund = 50"

── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  gx = col_double(),
  gy = col_double(),
  plot = col_double(),
  tag = col_character(),
  quadrat = col_character(),
  SPcode = col_character(),
  height = col_double(),
  date = col_character(),
  census = col_character(),
  year = col_double(),
  season = col_character(),
  survive = col_double(),
  CONS = col_double(),
  CONA = col_double(),
  HETA = col_double(),
  HETS = col_double(),
  Rainfall = col_double(),
  habitat = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  qua = col_double(),
  habit3 = col_character(),
  seedtrap = col_double(),
  habit5 = col_character()
)


── Column specification ────────────────────────────────────────────────────────
cols(
  SPcode = col_character(),
  LDMC = col_double(),
  WD = col_double(),
  SDMC = col_double(),
  LA = col_double(),
  SLA = col_double(),
  Chl = col_double(),
  LT = col_double(),
  C13 = col_double(),
  C = col_double(),
  N = col_double(),
  CN = col_double(),
  tlp = col_double()
)

No. of species
76
[1] "Sp-level: 1 + all the traits"
[1] "sp number in seedling data: 76"
[1] "sp number in trait data: 76"
data{
  int<lower=0> N; // number of sample
  int<lower=1> J; // number of sp
  int<lower=1> K; // number of tree-level preditor (i.e, CONS, HETS,...)
  int<lower=1> L; // number of sp-level predictor (i.e., interecept and WP)
  int<lower=1> M; // number of seedling individuals (tag)
  int<lower=1> S; // number of site
  int<lower=1> T; // number of census
  matrix[N, K] x; // tree-level predictor
  matrix[J, L] u; // sp-level predictor
  int<lower=0,upper=1> suv[N]; // 1 or 0
  int<lower=1,upper=J> sp[N]; // integer
  int<lower=1,upper=S> plot[N]; // integer
  int<lower=1,upper=T> census[N]; // integer
  int<lower=1> tag[N]; // integer
}

parameters{
  matrix[K, J] z;
  vector[S] phi_raw;
  vector[T] xi_raw;
  vector[M] psi_raw;
  matrix[L, K] gamma;
  cholesky_factor_corr[K] L_Omega;
  vector<lower=0,upper=pi()/2>[K] tau_unif;
  vector<lower=0,upper=pi()/2>[3] sig_unif;
}

transformed parameters{
  matrix[J, K] beta;
  vector<lower=0>[K] tau;
  vector<lower=0>[3] sig;
  vector[S] phi;
  vector[T] xi;
  vector[M] psi;
  for (k in 1:K) tau[k] = 2.5 * tan(tau_unif[k]); // implies tau ~ cauchy(0, 2.5)
  for (i in 1:3) sig[i] = 2.5 * tan(sig_unif[i]); // implies sig ~ cauchy(0, 2.5)
  beta = u * gamma + (diag_pre_multiply(tau,L_Omega) * z)';
  phi = phi_raw * sig[1];
  xi = xi_raw * sig[2];
  psi = psi_raw * sig[3];
}

model {
  // Hyper-priors
  to_vector(z) ~ std_normal();
  to_vector(phi_raw) ~ std_normal();
  to_vector(xi_raw) ~ std_normal();
  to_vector(psi_raw) ~ std_normal();
  L_Omega ~ lkj_corr_cholesky(2); // uniform of L_Omega * L_Omega'
  // Priors
  to_vector(gamma) ~ normal(0, 5);
  // Likelihood
  suv ~ bernoulli_logit(rows_dot_product(beta[sp] , x) + phi[plot] + xi[census] + psi[tag]);
}

generated quantities {
  vector[N] log_lik;
  corr_matrix[K] Omega;
  Omega = multiply_lower_tri_self_transpose(L_Omega);
  for (n in 1:N) {
    log_lik[n] = bernoulli_logit_lpmf(suv[n] | dot_product(beta[sp[n],] , x[n,]) + phi[plot[n]] + xi[census[n]] + psi[tag[n]]);
  }
}
[1] "use c = 0.27 as a scaling parameter for the distance effect"
[1] "n_sp = J =76"
[1] "n_para = K = 11"
[1] "n_plot = S = 384"
[1] "n_census = T = 10"
[1] "n_tag = M = 9153"

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

CHECKING DATA AND PREPROCESSING FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

COMPILING MODEL 'model_inter' NOW.

STARTING SAMPLER FOR MODEL 'model_inter' NOW.

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 1).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 2).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 3).

SAMPLING FOR MODEL 'model_inter' NOW (CHAIN 4).
Chain 3: 
Chain 3: Gradient evaluation took 0.037257 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 372.57 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 1: 
Chain 1: Gradient evaluation took 0.043039 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 430.39 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.048459 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 484.59 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 4: 
Chain 4: Gradient evaluation took 0.047623 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 476.23 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 3: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 1: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 4: Iteration:    1 / 4000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 3: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 2: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 4: Iteration:  200 / 4000 [  5%]  (Warmup)
Chain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 4000 [ 10%]  (Warmup)
Chain 3: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 1: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 2: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 4: Iteration:  600 / 4000 [ 15%]  (Warmup)
Chain 3: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 2: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 4: Iteration:  800 / 4000 [ 20%]  (Warmup)
Chain 3: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 1: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 2: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 4: Iteration: 1000 / 4000 [ 25%]  (Warmup)
Chain 3: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 4: Iteration: 1200 / 4000 [ 30%]  (Warmup)
Chain 3: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 1: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 2: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 4: Iteration: 1400 / 4000 [ 35%]  (Warmup)
Chain 3: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 2: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 3: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1600 / 4000 [ 40%]  (Warmup)
Chain 1: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 2: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 4: Iteration: 1800 / 4000 [ 45%]  (Warmup)
Chain 3: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 3: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 2: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 2: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 4: Iteration: 2000 / 4000 [ 50%]  (Warmup)
Chain 4: Iteration: 2001 / 4000 [ 50%]  (Sampling)
Chain 1: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 2: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 3: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 4: Iteration: 2200 / 4000 [ 55%]  (Sampling)
Chain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 2: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 1: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 2: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 3: Iteration: 2400 / 4000 [ 60%]  (Sampling)
Chain 4: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 2: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 4: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 2: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 3: Iteration: 2600 / 4000 [ 65%]  (Sampling)
Chain 4: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 2: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 4: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 1: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 2: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 4: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 2800 / 4000 [ 70%]  (Sampling)
Chain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 2: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 4: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 1: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 2: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 4: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 3000 / 4000 [ 75%]  (Sampling)
Chain 1: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 125641 seconds (Warm-up)
Chain 1:                41260.5 seconds (Sampling)
Chain 1:                166901 seconds (Total)
Chain 1: 
Chain 2: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 126570 seconds (Warm-up)
Chain 2:                40899.3 seconds (Sampling)
Chain 2:                167469 seconds (Total)
Chain 2: 
Chain 4: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 128983 seconds (Warm-up)
Chain 4:                40323.1 seconds (Sampling)
Chain 4:                169306 seconds (Total)
Chain 4: 
Chain 3: Iteration: 3200 / 4000 [ 80%]  (Sampling)
Chain 3: Iteration: 3400 / 4000 [ 85%]  (Sampling)
Chain 3: Iteration: 3600 / 4000 [ 90%]  (Sampling)
Chain 3: Iteration: 3800 / 4000 [ 95%]  (Sampling)
Chain 3: Iteration: 4000 / 4000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 124755 seconds (Warm-up)
Chain 3:                70057.9 seconds (Sampling)
Chain 3:                194813 seconds (Total)
Chain 3: 
Inference for Stan model: model_inter.
4 chains, each with iter=4000; warmup=2000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=8000.

                  mean se_mean     sd      2.5%       25%       50%       75%
gamma[1,1]        3.10    0.01   0.53      2.09      2.74      3.09      3.45
gamma[1,2]       -1.89    0.02   0.87     -3.69     -2.47     -1.86     -1.29
gamma[1,3]        0.05    0.00   0.29     -0.53     -0.14      0.06      0.25
gamma[1,4]        0.22    0.00   0.10      0.05      0.15      0.21      0.28
gamma[1,5]       -0.11    0.00   0.11     -0.31     -0.18     -0.11     -0.03
gamma[1,6]       -0.43    0.01   0.32     -1.05     -0.64     -0.43     -0.21
gamma[1,7]       -0.52    0.01   0.50     -1.49     -0.85     -0.52     -0.18
gamma[1,8]       -0.21    0.00   0.16     -0.53     -0.32     -0.21     -0.10
gamma[1,9]       -0.03    0.00   0.07     -0.17     -0.08     -0.03      0.01
gamma[1,10]      -0.02    0.00   0.07     -0.16     -0.07     -0.02      0.03
gamma[1,11]       1.13    0.00   0.11      0.92      1.06      1.13      1.21
gamma[2,1]       -1.41    0.02   0.75     -2.87     -1.92     -1.40     -0.90
gamma[2,2]       -1.87    0.03   1.27     -4.40     -2.73     -1.87     -1.01
gamma[2,3]       -0.23    0.01   0.45     -1.12     -0.52     -0.22      0.08
gamma[2,4]       -0.09    0.00   0.15     -0.40     -0.19     -0.09      0.00
gamma[2,5]        0.12    0.00   0.16     -0.19      0.01      0.12      0.23
gamma[2,6]       -1.07    0.01   0.50     -2.08     -1.41     -1.07     -0.73
gamma[2,7]       -1.75    0.02   0.87     -3.45     -2.34     -1.76     -1.16
gamma[2,8]        0.01    0.00   0.27     -0.52     -0.16      0.01      0.18
gamma[2,9]       -0.13    0.00   0.11     -0.35     -0.21     -0.13     -0.06
gamma[2,10]       0.04    0.00   0.12     -0.20     -0.05      0.04      0.12
gamma[2,11]      -0.05    0.00   0.19     -0.45     -0.18     -0.05      0.08
gamma[3,1]        0.13    0.02   0.91     -1.60     -0.47      0.13      0.74
gamma[3,2]       -0.49    0.03   1.54     -3.54     -1.54     -0.50      0.55
gamma[3,3]        0.63    0.01   0.56     -0.48      0.26      0.62      1.00
gamma[3,4]       -0.09    0.00   0.24     -0.57     -0.24     -0.08      0.08
gamma[3,5]        0.26    0.00   0.30     -0.31      0.06      0.26      0.46
gamma[3,6]        0.03    0.01   0.64     -1.22     -0.41      0.02      0.47
gamma[3,7]       -0.61    0.02   1.09     -2.77     -1.35     -0.61      0.13
gamma[3,8]        0.32    0.01   0.36     -0.38      0.06      0.31      0.56
gamma[3,9]       -0.14    0.00   0.19     -0.51     -0.27     -0.14     -0.01
gamma[3,10]       0.14    0.00   0.22     -0.28      0.00      0.14      0.29
gamma[3,11]      -0.55    0.00   0.30     -1.15     -0.74     -0.54     -0.34
gamma[4,1]        0.88    0.02   0.99     -1.06      0.21      0.90      1.54
gamma[4,2]        2.57    0.03   1.86     -1.11      1.33      2.60      3.81
gamma[4,3]       -0.67    0.01   0.56     -1.77     -1.05     -0.66     -0.29
gamma[4,4]        0.04    0.00   0.24     -0.44     -0.12      0.04      0.20
gamma[4,5]       -0.35    0.00   0.30     -0.95     -0.54     -0.35     -0.14
gamma[4,6]        0.47    0.01   0.73     -0.95     -0.04      0.47      0.98
gamma[4,7]        1.59    0.03   1.36     -1.02      0.68      1.58      2.52
gamma[4,8]       -0.39    0.00   0.36     -1.10     -0.63     -0.39     -0.14
gamma[4,9]        0.20    0.00   0.19     -0.19      0.07      0.20      0.33
gamma[4,10]      -0.07    0.00   0.22     -0.51     -0.22     -0.06      0.08
gamma[4,11]       0.59    0.00   0.31     -0.01      0.38      0.59      0.79
gamma[5,1]       -0.24    0.01   0.61     -1.43     -0.64     -0.24      0.16
gamma[5,2]       -2.06    0.02   0.95     -4.01     -2.67     -2.04     -1.43
gamma[5,3]        0.40    0.01   0.42     -0.41      0.12      0.40      0.68
gamma[5,4]        0.05    0.00   0.13     -0.18     -0.03      0.05      0.13
gamma[5,5]       -0.19    0.00   0.16     -0.51     -0.30     -0.19     -0.08
gamma[5,6]       -0.06    0.01   0.47     -0.98     -0.38     -0.06      0.26
gamma[5,7]        0.48    0.02   0.90     -1.29     -0.13      0.48      1.09
gamma[5,8]       -0.24    0.00   0.23     -0.68     -0.39     -0.24     -0.09
gamma[5,9]       -0.15    0.00   0.10     -0.35     -0.21     -0.15     -0.08
gamma[5,10]       0.14    0.00   0.11     -0.09      0.06      0.14      0.22
gamma[5,11]       0.16    0.00   0.16     -0.16      0.05      0.16      0.27
gamma[6,1]       -0.84    0.01   0.46     -1.75     -1.14     -0.83     -0.53
gamma[6,2]       -1.40    0.01   0.78     -2.96     -1.91     -1.42     -0.89
gamma[6,3]       -0.01    0.00   0.26     -0.53     -0.19     -0.01      0.16
gamma[6,4]        0.02    0.00   0.10     -0.17     -0.04      0.02      0.08
gamma[6,5]       -0.12    0.00   0.12     -0.36     -0.20     -0.12     -0.04
gamma[6,6]       -1.14    0.01   0.33     -1.78     -1.36     -1.13     -0.92
gamma[6,7]       -1.67    0.01   0.61     -2.87     -2.07     -1.67     -1.26
gamma[6,8]       -0.22    0.00   0.15     -0.52     -0.32     -0.22     -0.12
gamma[6,9]       -0.06    0.00   0.07     -0.21     -0.11     -0.06     -0.01
gamma[6,10]       0.07    0.00   0.09     -0.10      0.01      0.07      0.13
gamma[6,11]      -0.15    0.00   0.13     -0.41     -0.24     -0.15     -0.06
gamma[7,1]       -1.06    0.02   0.82     -2.63     -1.61     -1.08     -0.53
gamma[7,2]       -1.81    0.03   1.47     -4.70     -2.78     -1.79     -0.84
gamma[7,3]       -0.33    0.01   0.41     -1.14     -0.61     -0.34     -0.06
gamma[7,4]        0.23    0.00   0.17     -0.12      0.12      0.23      0.34
gamma[7,5]       -0.18    0.00   0.20     -0.57     -0.31     -0.18     -0.05
gamma[7,6]        0.24    0.01   0.62     -0.96     -0.18      0.24      0.66
gamma[7,7]        0.13    0.02   1.17     -2.12     -0.69      0.13      0.92
gamma[7,8]        0.02    0.00   0.23     -0.42     -0.13      0.02      0.18
gamma[7,9]        0.08    0.00   0.12     -0.15      0.00      0.08      0.16
gamma[7,10]       0.09    0.00   0.14     -0.18     -0.01      0.08      0.18
gamma[7,11]       0.11    0.00   0.21     -0.30     -0.02      0.12      0.25
gamma[8,1]        0.33    0.03   1.62     -3.00     -0.74      0.37      1.45
gamma[8,2]       -0.98    0.06   2.72     -6.31     -2.85     -0.95      0.85
gamma[8,3]        1.64    0.02   1.05     -0.48      0.95      1.69      2.35
gamma[8,4]       -0.29    0.01   0.34     -0.99     -0.50     -0.28     -0.07
gamma[8,5]        0.26    0.01   0.37     -0.46      0.01      0.25      0.51
gamma[8,6]        1.60    0.02   1.11     -0.58      0.85      1.59      2.34
gamma[8,7]        3.39    0.05   2.05     -0.54      1.97      3.40      4.78
gamma[8,8]        0.31    0.01   0.53     -0.71     -0.05      0.30      0.66
gamma[8,9]       -0.06    0.00   0.25     -0.55     -0.23     -0.07      0.10
gamma[8,10]      -0.03    0.00   0.26     -0.55     -0.20     -0.03      0.14
gamma[8,11]      -0.20    0.01   0.38     -0.95     -0.45     -0.20      0.05
gamma[9,1]        0.26    0.03   1.40     -2.60     -0.64      0.28      1.20
gamma[9,2]       -0.36    0.05   2.35     -4.94     -1.92     -0.34      1.21
gamma[9,3]        1.47    0.02   0.94     -0.45      0.85      1.49      2.11
gamma[9,4]       -0.32    0.01   0.36     -1.01     -0.55     -0.33     -0.10
gamma[9,5]        0.31    0.01   0.40     -0.49      0.03      0.30      0.57
gamma[9,6]        1.09    0.03   1.07     -0.99      0.35      1.09      1.81
gamma[9,7]        2.88    0.05   1.99     -1.01      1.52      2.91      4.23
gamma[9,8]        0.07    0.01   0.50     -0.90     -0.27      0.07      0.40
gamma[9,9]       -0.22    0.00   0.26     -0.73     -0.39     -0.22     -0.05
gamma[9,10]      -0.03    0.00   0.29     -0.60     -0.22     -0.02      0.17
gamma[9,11]      -0.14    0.01   0.41     -0.94     -0.41     -0.14      0.13
gamma[10,1]       0.02    0.01   0.62     -1.23     -0.40      0.03      0.44
gamma[10,2]      -0.26    0.02   1.10     -2.46     -1.01     -0.25      0.48
gamma[10,3]       0.28    0.00   0.29     -0.28      0.08      0.28      0.47
gamma[10,4]       0.04    0.00   0.11     -0.17     -0.03      0.04      0.11
gamma[10,5]      -0.12    0.00   0.14     -0.40     -0.21     -0.12     -0.02
gamma[10,6]       0.69    0.01   0.51     -0.29      0.34      0.68      1.03
gamma[10,7]       1.83    0.02   0.97     -0.05      1.16      1.81      2.48
gamma[10,8]      -0.13    0.00   0.17     -0.46     -0.25     -0.13     -0.02
gamma[10,9]      -0.10    0.00   0.09     -0.27     -0.16     -0.10     -0.03
gamma[10,10]      0.01    0.00   0.10     -0.19     -0.06      0.01      0.08
gamma[10,11]     -0.01    0.00   0.13     -0.28     -0.10     -0.01      0.08
gamma[11,1]      -0.66    0.01   0.56     -1.76     -1.03     -0.66     -0.29
gamma[11,2]      -1.40    0.02   1.03     -3.44     -2.10     -1.38     -0.69
gamma[11,3]       0.09    0.00   0.30     -0.54     -0.11      0.10      0.30
gamma[11,4]      -0.21    0.00   0.13     -0.47     -0.29     -0.20     -0.12
gamma[11,5]       0.25    0.00   0.16     -0.05      0.14      0.25      0.36
gamma[11,6]       0.22    0.01   0.45     -0.66     -0.08      0.23      0.53
gamma[11,7]       0.50    0.02   0.83     -1.14     -0.06      0.50      1.05
gamma[11,8]       0.06    0.00   0.19     -0.32     -0.07      0.06      0.19
gamma[11,9]      -0.16    0.00   0.10     -0.36     -0.22     -0.15     -0.09
gamma[11,10]      0.13    0.00   0.12     -0.10      0.05      0.13      0.21
gamma[11,11]     -0.23    0.00   0.15     -0.52     -0.33     -0.23     -0.13
gamma[12,1]       0.13    0.02   0.83     -1.47     -0.42      0.11      0.68
gamma[12,2]      -0.89    0.03   1.48     -3.75     -1.87     -0.87      0.09
gamma[12,3]       0.33    0.01   0.48     -0.62      0.01      0.33      0.64
gamma[12,4]      -0.14    0.00   0.17     -0.46     -0.25     -0.15     -0.03
gamma[12,5]      -0.15    0.00   0.20     -0.55     -0.28     -0.15     -0.01
gamma[12,6]      -0.50    0.01   0.64     -1.76     -0.93     -0.50     -0.08
gamma[12,7]      -0.05    0.03   1.25     -2.49     -0.89     -0.07      0.78
gamma[12,8]      -0.30    0.00   0.26     -0.82     -0.48     -0.30     -0.13
gamma[12,9]      -0.17    0.00   0.13     -0.44     -0.26     -0.17     -0.09
gamma[12,10]      0.11    0.00   0.14     -0.16      0.02      0.11      0.21
gamma[12,11]      0.02    0.00   0.23     -0.42     -0.12      0.02      0.17
gamma[13,1]      -0.37    0.01   0.53     -1.44     -0.73     -0.36     -0.01
gamma[13,2]      -0.77    0.02   0.97     -2.57     -1.43     -0.81     -0.12
gamma[13,3]      -0.15    0.01   0.39     -0.95     -0.41     -0.14      0.10
gamma[13,4]      -0.10    0.00   0.12     -0.33     -0.17     -0.10     -0.02
gamma[13,5]       0.08    0.00   0.13     -0.17     -0.01      0.08      0.16
gamma[13,6]      -0.49    0.01   0.31     -1.10     -0.69     -0.49     -0.28
gamma[13,7]      -1.15    0.01   0.52     -2.17     -1.50     -1.15     -0.80
gamma[13,8]       0.16    0.00   0.21     -0.24      0.03      0.17      0.30
gamma[13,9]       0.02    0.00   0.08     -0.15     -0.04      0.02      0.07
gamma[13,10]      0.02    0.00   0.09     -0.16     -0.04      0.02      0.08
gamma[13,11]     -0.05    0.00   0.16     -0.37     -0.15     -0.05      0.05
sig[1]            0.54    0.00   0.04      0.46      0.51      0.54      0.57
sig[2]            0.46    0.00   0.15      0.26      0.36      0.43      0.53
sig[3]            0.99    0.00   0.10      0.80      0.92      0.99      1.06
lp__         -13220.28    7.63 162.11 -13529.60 -13333.22 -13218.25 -13114.13
                 97.5% n_eff Rhat
gamma[1,1]        4.14  1554 1.00
gamma[1,2]       -0.23  1345 1.00
gamma[1,3]        0.63  3778 1.00
gamma[1,4]        0.44  2816 1.00
gamma[1,5]        0.10  6534 1.00
gamma[1,6]        0.21  3567 1.00
gamma[1,7]        0.50  4209 1.00
gamma[1,8]        0.10  4290 1.00
gamma[1,9]        0.10  7692 1.00
gamma[1,10]       0.12  8326 1.00
gamma[1,11]       1.35  5625 1.00
gamma[2,1]        0.03  2411 1.00
gamma[2,2]        0.59  2057 1.00
gamma[2,3]        0.61  3811 1.00
gamma[2,4]        0.18  3831 1.00
gamma[2,5]        0.45  4368 1.00
gamma[2,6]       -0.09  2137 1.00
gamma[2,7]       -0.03  2503 1.00
gamma[2,8]        0.55  3421 1.00
gamma[2,9]        0.09  4132 1.00
gamma[2,10]       0.27  4773 1.00
gamma[2,11]       0.31  4330 1.00
gamma[3,1]        1.88  2777 1.00
gamma[3,2]        2.56  3583 1.00
gamma[3,3]        1.73  3338 1.00
gamma[3,4]        0.38  4666 1.00
gamma[3,5]        0.86  4219 1.00
gamma[3,6]        1.30  2277 1.00
gamma[3,7]        1.51  2273 1.00
gamma[3,8]        1.03  4549 1.00
gamma[3,9]        0.24  5483 1.00
gamma[3,10]       0.58  4647 1.00
gamma[3,11]       0.05  5272 1.00
gamma[4,1]        2.80  3252 1.00
gamma[4,2]        6.27  3846 1.00
gamma[4,3]        0.45  3719 1.00
gamma[4,4]        0.52  4851 1.00
gamma[4,5]        0.23  4355 1.00
gamma[4,6]        1.89  2563 1.00
gamma[4,7]        4.32  2562 1.00
gamma[4,8]        0.31  5437 1.00
gamma[4,9]        0.57  5109 1.00
gamma[4,10]       0.36  4928 1.00
gamma[4,11]       1.20  4853 1.00
gamma[5,1]        0.97  2392 1.00
gamma[5,2]       -0.24  3014 1.00
gamma[5,3]        1.24  2712 1.00
gamma[5,4]        0.32  3699 1.00
gamma[5,5]        0.12  4666 1.00
gamma[5,6]        0.87  2418 1.00
gamma[5,7]        2.24  2426 1.00
gamma[5,8]        0.22  3895 1.00
gamma[5,9]        0.05  4972 1.00
gamma[5,10]       0.37  5585 1.00
gamma[5,11]       0.48  5440 1.00
gamma[6,1]        0.04  2656 1.00
gamma[6,2]        0.14  3490 1.00
gamma[6,3]        0.50  3644 1.00
gamma[6,4]        0.21  5492 1.00
gamma[6,5]        0.11  5060 1.00
gamma[6,6]       -0.51  2813 1.00
gamma[6,7]       -0.48  2792 1.00
gamma[6,8]        0.09  5475 1.00
gamma[6,9]        0.07  5359 1.00
gamma[6,10]       0.25  5939 1.00
gamma[6,11]       0.11  5988 1.00
gamma[7,1]        0.57  2850 1.00
gamma[7,2]        1.10  3326 1.00
gamma[7,3]        0.51  3383 1.00
gamma[7,4]        0.56  3476 1.00
gamma[7,5]        0.20  3567 1.00
gamma[7,6]        1.50  2296 1.00
gamma[7,7]        2.43  2704 1.00
gamma[7,8]        0.46  3618 1.00
gamma[7,9]        0.32  3525 1.00
gamma[7,10]       0.36  4207 1.00
gamma[7,11]       0.51  4106 1.00
gamma[8,1]        3.30  2196 1.00
gamma[8,2]        4.27  1854 1.00
gamma[8,3]        3.62  2326 1.00
gamma[8,4]        0.40  3351 1.00
gamma[8,5]        0.99  3084 1.00
gamma[8,6]        3.80  2005 1.00
gamma[8,7]        7.38  2045 1.00
gamma[8,8]        1.35  3685 1.00
gamma[8,9]        0.43  3242 1.00
gamma[8,10]       0.46  3602 1.00
gamma[8,11]       0.54  3683 1.00
gamma[9,1]        2.95  2379 1.00
gamma[9,2]        4.23  2655 1.00
gamma[9,3]        3.29  2392 1.00
gamma[9,4]        0.41  3131 1.00
gamma[9,5]        1.12  2977 1.00
gamma[9,6]        3.22  1620 1.00
gamma[9,7]        6.77  1751 1.00
gamma[9,8]        1.04  3168 1.00
gamma[9,9]        0.28  2880 1.00
gamma[9,10]       0.52  3357 1.00
gamma[9,11]       0.67  3564 1.00
gamma[10,1]       1.22  2696 1.00
gamma[10,2]       1.87  3382 1.00
gamma[10,3]       0.87  3465 1.00
gamma[10,4]       0.24  5848 1.00
gamma[10,5]       0.16  6195 1.00
gamma[10,6]       1.69  2883 1.00
gamma[10,7]       3.76  3090 1.00
gamma[10,8]       0.20  5757 1.00
gamma[10,9]       0.08  8107 1.00
gamma[10,10]      0.22  8236 1.00
gamma[10,11]      0.25  6987 1.00
gamma[11,1]       0.45  2818 1.00
gamma[11,2]       0.59  3041 1.00
gamma[11,3]       0.67  3715 1.00
gamma[11,4]       0.04  4896 1.00
gamma[11,5]       0.57  4256 1.00
gamma[11,6]       1.12  2186 1.00
gamma[11,7]       2.15  2309 1.00
gamma[11,8]       0.44  3898 1.00
gamma[11,9]       0.04  3691 1.00
gamma[11,10]      0.36  4610 1.00
gamma[11,11]      0.06  5536 1.00
gamma[12,1]       1.81  2256 1.00
gamma[12,2]       2.07  2552 1.00
gamma[12,3]       1.29  3239 1.00
gamma[12,4]       0.23  3631 1.00
gamma[12,5]       0.25  4366 1.00
gamma[12,6]       0.78  2192 1.00
gamma[12,7]       2.47  2247 1.00
gamma[12,8]       0.21  4151 1.00
gamma[12,9]       0.08  4149 1.00
gamma[12,10]      0.39  4760 1.00
gamma[12,11]      0.48  4844 1.00
gamma[13,1]       0.67  2407 1.00
gamma[13,2]       1.21  2103 1.00
gamma[13,3]       0.57  3325 1.00
gamma[13,4]       0.15  4828 1.00
gamma[13,5]       0.34  4429 1.00
gamma[13,6]       0.12  3104 1.00
gamma[13,7]      -0.12  3825 1.00
gamma[13,8]       0.57  4096 1.00
gamma[13,9]       0.18  5336 1.00
gamma[13,10]      0.20  5082 1.00
gamma[13,11]      0.26  5309 1.00
sig[1]            0.63  2436 1.00
sig[2]            0.85  2718 1.00
sig[3]            1.19   454 1.01
lp__         -12894.72   451 1.01

Samples were drawn using NUTS(diag_e) at Sat Aug 14 19:41:41 2021.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
[1] "./rda/dry_spab_50_model_inter_full_Full_Full.rda"
[1] "MCMC done!!"
